{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3af55ade-5ad9-4b61-a827-09adec9b08eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Extração e Classificação de BI-RADS em Laudos de Mamografia do Pardini\n",
    "\n",
    "## Introdução Técnica\n",
    "\n",
    "Este notebook implementa um sistema automatizado para extração, classificação e processamento de informações de BI-RADS (Breast Imaging Reporting and Data System) a partir de laudos de mamografia do laboratório Pardini. O sistema identifica pacientes com exames em atraso e gera listas de ativação para envio de notificações, com base na classificação BI-RADS e no tempo recomendado para realização de novo exame.\n",
    "\n",
    "### Objetivo Principal\n",
    "\n",
    "O objetivo principal deste notebook é extrair automaticamente as classificações BI-RADS (0-6) de laudos de mamografia do Pardini, calcular datas de retorno recomendadas com base nessa classificação, e identificar pacientes elegíveis para notificações de acompanhamento. Pacientes com BI-RADS 1, 2 e 3 são incluídos em uma lista de ativação para monitoramento e notificações de retorno.\n",
    "\n",
    "### Tecnologias Utilizadas\n",
    "\n",
    "- **Framework de Processamento**: Apache Spark (PySpark)\n",
    "- **Armazenamento de Dados**: Delta Lake\n",
    "- **Manipulação de Dados**: PySpark SQL e DataFrame API\n",
    "- **Processamento de Texto**: Expressões Regulares (RegEx)\n",
    "- **Monitoramento e Alertas**: Octoops (Sentinel)\n",
    "- **Logging**: Biblioteca padrão Python (logging)\n",
    "\n",
    "### Fluxo de Trabalho/Etapas Principais\n",
    "\n",
    "1. **Configuração do Ambiente**: Instalação de dependências e configuração de logging\n",
    "2. **Definição de Parâmetros**: Configuração das tabelas e filtros para consultas SQL\n",
    "3. **Extração de BI-RADS**: Consulta SQL complexa que:\n",
    "   - Utiliza RegEx para extrair classificações BI-RADS dos laudos\n",
    "   - Transforma códigos romanos em valores numéricos (I→1, II→2, etc.)\n",
    "   - Calcula valores mínimo, máximo e atual de BI-RADS\n",
    "4. **Processamento de Datas de Retorno**: Cálculo das datas previstas de retorno com base na classificação BI-RADS\n",
    "5. **Filtragem e Transformação**: Aplicação de transformações nos dados e filtragem de pacientes elegíveis\n",
    "6. **Persistência de Dados**: Salvamento dos dados processados em tabelas Delta\n",
    "\n",
    "### Dados Envolvidos\n",
    "\n",
    "- **Fonte de Dados**: \n",
    "  - Tabela: `refined.saude_preventiva.pardini_laudos`\n",
    "  - Filtro principal: `linha_cuidado = 'mama'`\n",
    "  - Tipos de exame: 'MAMO', 'MAMODI'\n",
    "\n",
    "- **Tabelas de Destino**:\n",
    "  - `refined.saude_preventiva.pardini_laudos_mama_birads`: Dados gerais dos laudos com classificação BI-RADS\n",
    "  - `refined.saude_preventiva.pardini_laudos_mama_birads_ativacao`: Subconjunto com pacientes elegíveis para notificações\n",
    "\n",
    "- **Colunas Principais**:\n",
    "  - `ficha`, `sigla_exame`, `id_marca`, `sequencial`: Identificadores do exame\n",
    "  - `MIN_BIRADS`, `MAX_BIRADS`, `BIRADS`: Valores de classificação BI-RADS\n",
    "  - `dth_pedido`: Data do pedido do exame\n",
    "  - `dth_previsao_retorno`: Data calculada para retorno do paciente\n",
    "  - `retorno_cliente`: Tempo recomendado para retorno em meses (12 para BI-RADS 1-2, 6 para BI-RADS 3)\n",
    "\n",
    "### Resultados/Saídas Esperadas\n",
    "\n",
    "1. **Tabela Estruturada de BI-RADS**: Tabela Delta contendo todos os exames de mamografia do Pardini com classificação BI-RADS extraída\n",
    "2. **Lista de Ativação**: Tabela Delta contendo apenas pacientes:\n",
    "   - Com BI-RADS 1, 2 ou 3\n",
    "   - Sem retorno registrado\n",
    "   - Do sexo feminino entre 40-75 anos\n",
    "\n",
    "### Pré-requisitos\n",
    "\n",
    "- **Ambiente Databricks**: Com acesso ao Delta Lake e capacidade de execução de PySpark\n",
    "- **Dependência Principal**: Pacote `octoops` para monitoramento e alertas\n",
    "- **Acesso a Dados**: Permissões de leitura na tabela `refined.saude_preventiva.pardini_laudos`\n",
    "- **Permissões de Escrita**: Para criar ou atualizar as tabelas de destino\n",
    "\n",
    "### Considerações Importantes\n",
    "\n",
    "- **Extração via RegEx**: A extração de BI-RADS usa expressões regulares sofisticadas para identificar padrões como \"BIRADS X\", \"CATEGORIA X\" ou notações em algarismos romanos\n",
    "- **Recomendações de Retorno**: Seguem o protocolo médico padrão:\n",
    "  - BI-RADS 1-2: Retorno em 12 meses (360 dias)\n",
    "  - BI-RADS 3: Retorno em 6 meses (180 dias)\n",
    "  - BI-RADS 4-6: Sem recomendação automática de retorno (requer avaliação médica)\n",
    "- **Deduplicação**: A tabela de ativação remove duplicatas por `ficha` para evitar múltiplas notificações para o mesmo paciente\n",
    "- **Tratamento de Erros**: Sistema robusto com captura de exceções e notificações via Sentinel em caso de falhas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61e99ce5-bcfe-41be-b891-b377065f38aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Instalação da Biblioteca Octoops\n",
    "\n",
    "Esta célula realiza a instalação da biblioteca `octoops` utilizando o comando `pip` com a sintaxe especial de magic command do Jupyter (`%pip`). \n",
    "\n",
    "A biblioteca Octoops é uma ferramenta interna utilizada para monitoramento e alertas em pipelines de dados. Ela fornece funcionalidades como:\n",
    "\n",
    "- Envio de notificações em caso de falha no processamento\n",
    "- Monitoramento de execuções de pipelines\n",
    "- Registro centralizado de eventos e métricas\n",
    "\n",
    "A instalação ocorre no ambiente atual de execução e é essencial para o funcionamento do sistema de alertas implementado nas células posteriores deste notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "007f5e58-8350-47b4-93d9-6063b27918bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install octoops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e30d795c-5001-409f-b4bf-22e1a465a68f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Reinicialização do Ambiente Python\n",
    "\n",
    "Esta célula reinicia o interpretador Python no ambiente Databricks através da função `dbutils.library.restartPython()`. Esta etapa é necessária após a instalação de pacotes via pip para garantir que as novas bibliotecas sejam carregadas corretamente.\n",
    "\n",
    "A reinicialização:\n",
    "- Finaliza a sessão Python atual\n",
    "- Inicia uma nova sessão com todas as bibliotecas recém-instaladas disponíveis\n",
    "- Mantém o contexto do notebook, permitindo que células anteriores permaneçam executadas\n",
    "- Garante que a biblioteca Octoops estará disponível para uso nas células seguintes\n",
    "\n",
    "Esta operação é equivalente a reiniciar o kernel no Jupyter Notebook tradicional, mas de forma mais controlada no ambiente Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c19f6bf6-deb6-4a15-adbf-3da839c5aab3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb348b89-a878-4d0e-ac40-91ee9f8c7377",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Importação de Bibliotecas e Módulos\n",
    "\n",
    "Esta célula importa todas as bibliotecas e módulos necessários para o processamento dos dados, manipulação de DataFrames, operações com datas, logging e envio de alertas.\n",
    "\n",
    "### Bibliotecas importadas:\n",
    "\n",
    "#### PySpark SQL e Funções\n",
    "- **Funções de coluna e manipulação de dados**: `col`, `year`, `month`, `dayofmonth`, `when`, `lit`, `expr`, `to_timestamp`\n",
    "- **Tipos de dados**: `DateType`\n",
    "- **DataFrame API**: `DataFrame` (para tipagem)\n",
    "- **Funções de data**: `datediff`, `to_date`\n",
    "\n",
    "#### Bibliotecas Padrão Python\n",
    "- **Manipulação de datas**: `datetime`\n",
    "- **Logging**: `logging`\n",
    "- **Interação com sistema**: `sys`\n",
    "- **Tratamento de erros**: `traceback`\n",
    "\n",
    "#### Monitoramento (Octoops)\n",
    "- `OctoOps`: Framework principal para monitoramento\n",
    "- `Sentinel`: Componente específico para envio de alertas\n",
    "\n",
    "Estas importações fornecem as ferramentas necessárias para executar o pipeline completo de extração, processamento e persistência de dados de BI-RADS dos laudos do Pardini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc6265e9-2336-42d2-abc4-83c6faf51bb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, year, month, dayofmonth, when, lit, expr, to_timestamp\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import datediff, to_date\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import sys\n",
    "import traceback\n",
    "from octoops import OctoOps\n",
    "from octoops import Sentinel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "537790d2-34cd-4712-a869-c7998ee5be92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuração do Logger\n",
    "\n",
    "Esta célula inicializa um objeto logger para registrar eventos e mensagens durante a execução do notebook. O logger é configurado utilizando a biblioteca padrão de logging do Python.\n",
    "\n",
    "A configuração `logging.getLogger(__name__)` cria um logger associado ao nome do módulo atual (neste caso, o notebook em execução). Esta é uma prática recomendada pois permite:\n",
    "\n",
    "1. Identificação clara da origem das mensagens de log\n",
    "2. Configuração hierárquica de loggers quando necessário\n",
    "3. Controle granular dos níveis de logging\n",
    "\n",
    "Este logger será utilizado posteriormente no código para registrar informações importantes, como avisos sobre DataFrames vazios ou detalhes sobre operações de persistência de dados, auxiliando na depuração e monitoramento do pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "780f24dd-4200-405f-8f6a-8d63b139a060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c92da758-a3c9-483d-b5e5-c50f206d3428",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuração de Tabelas e Filtros\n",
    "\n",
    "Esta célula define as tabelas de destino para os dados processados e configura os filtros SQL que serão aplicados nas consultas. A configuração é crucial para o processamento incremental e para a definição dos critérios de elegibilidade para notificações.\n",
    "\n",
    "### Tabelas Definidas\n",
    "\n",
    "- **`table_birads`**: Tabela principal para armazenar todos os laudos processados com classificação BI-RADS\n",
    "  ```python\n",
    "  table_birads = \"refined.saude_preventiva.pardini_laudos_mama_birads\"\n",
    "  ```\n",
    "\n",
    "- **`table_birads_ativacao`**: Tabela para armazenar apenas os pacientes elegíveis para notificações\n",
    "  ```python\n",
    "  table_birads_ativacao = \"refined.saude_preventiva.pardini_laudos_mama_birads_ativacao\"\n",
    "  ```\n",
    "\n",
    "### Filtros Configurados\n",
    "\n",
    "1. **`where_clause`**: Filtro para processamento incremental\n",
    "   - Inicialmente vazio\n",
    "   - Se a tabela principal já existir, configura-se para processar apenas registros mais recentes que o último processamento\n",
    "   - Utiliza subconsulta para determinar o valor de `_datestamp` mais recente na tabela existente\n",
    "\n",
    "2. **`filtro_ativacao`**: Filtro para selecionar pacientes elegíveis para notificações\n",
    "   - Pacientes sem retorno registrado (`eleg.ficha IS NULL`)\n",
    "   - Com BI-RADS 1, 2 ou 3 (`brd.BIRADS IN (1, 2, 3)`)\n",
    "   - Que realizaram exames específicos (`flr.sigla_exame IN ('MAMODI','MAMO')`)\n",
    "   - Do sexo feminino (`UPPER(flr.sexo_cliente) = 'F'`)\n",
    "   - Na faixa etária apropriada (40-75 anos)\n",
    "\n",
    "### Impacto\n",
    "\n",
    "Estas configurações otimizam o processamento, evitando reprocessar dados antigos e focando nas notificações apenas para os pacientes que realmente precisam ser acompanhados de acordo com os protocolos médicos para cada classificação BI-RADS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7c0c70e-635c-49a5-8ad6-f5e8a17d97f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_birads = \"refined.saude_preventiva.pardini_laudos_mama_birads\"\n",
    "table_birads_ativacao = \"refined.saude_preventiva.pardini_laudos_mama_birads_ativacao\"\n",
    "where_clause = \"\"\n",
    " \n",
    "if spark.catalog.tableExists(table_birads):\n",
    "    where_clause = f\"\"\"\n",
    "    WHERE        \n",
    "        flr._datestamp > (\n",
    "            SELECT MAX(brd._datestamp)\n",
    "            FROM {table_birads} brd\n",
    "        )\n",
    "        \n",
    "    \"\"\"\n",
    " \n",
    "filtro_ativacao = \"\"\"\n",
    "    WHERE\n",
    "        eleg.ficha IS NULL\n",
    "        AND brd.BIRADS IN (1, 2, 3)\n",
    "        AND flr.sigla_exame IN ('MAMODI','MAMO')\n",
    "        AND UPPER(flr.sexo_cliente) = 'F'\n",
    "        AND (\n",
    "            idade_cliente >= 40 AND idade_cliente < 76\n",
    "        )\n",
    "        AND flr.marca = 'HERMES PARDINI (MG)'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32ac3aee-1e2d-4ac6-9b82-d90000cde177",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Consulta SQL para Extração de BI-RADS\n",
    "\n",
    "Esta célula implementa uma consulta SQL complexa para extrair classificações BI-RADS de laudos de mamografia do Pardini e criar dois DataFrames: um com todos os laudos processados e outro apenas com pacientes elegíveis para notificações.\n",
    "\n",
    "### Objetivo da Célula\n",
    "\n",
    "A célula realiza o processamento principal do notebook:\n",
    "1. Define uma consulta SQL que extrai e processa informações de BI-RADS\n",
    "2. Cria dois DataFrames a partir desta consulta com diferentes filtros\n",
    "\n",
    "### Estrutura da Consulta\n",
    "\n",
    "A consulta utiliza Common Table Expressions (CTEs) para organizar o processamento em etapas lógicas:\n",
    "\n",
    "1. **CTE `base`**: \n",
    "   - Extrai os identificadores básicos dos exames (`ficha`, `sigla_exame`, `id_marca`, `sequencial`)\n",
    "   - Captura o texto do laudo\n",
    "   - Usa expressões regulares sofisticadas para extrair menções a BI-RADS\n",
    "   - Transforma algarismos romanos (I, II, III, etc.) em valores numéricos\n",
    "   - Filtra laudos da linha de cuidado 'mama' e tipos específicos de exame\n",
    "\n",
    "2. **CTE `dados_birads`**: \n",
    "   - Calcula métricas agregadas de BI-RADS:\n",
    "     - `MIN_BIRADS`: Menor valor de BI-RADS encontrado no laudo\n",
    "     - `MAX_BIRADS`: Maior valor de BI-RADS encontrado no laudo\n",
    "     - `BIRADS`: Valor mais recente/final (último elemento do array)\n",
    "\n",
    "3. **CTE `dados_laudos`**: \n",
    "   - Coleta dados demográficos e administrativos do paciente\n",
    "   - Calcula a idade do cliente a partir da data de nascimento\n",
    "   - Aplica o filtro incremental (`where_clause`)\n",
    "\n",
    "4. **Consulta Final**: \n",
    "   - Combina os dados das CTEs através de JOINs\n",
    "   - Une com a tabela de retornos elegíveis (`pardini_retorno_elegivel_ficha`)\n",
    "   - Seleciona todas as colunas relevantes para análise\n",
    "   - Aplica o filtro de ativação quando apropriado\n",
    "\n",
    "### Expressões Regulares\n",
    "\n",
    "A extração do BI-RADS utiliza expressões regulares complexas:\n",
    "\n",
    "```python\n",
    "REGEXP_EXTRACT_ALL(\n",
    "    REGEXP_EXTRACT(\n",
    "        REGEXP_REPLACE(UPPER(flr.laudo_tratado), r'[-:®]|\\xa0', ''),\n",
    "        r'(?mi)(AVALIA[CÇ][AÃ]O|CONCLUS[AÃ]O|IMPRESS[AÃ]O|OPINI[AÃ]O)?(.*)', 2\n",
    "    ),\n",
    "    r\"(?mi)(BIRADS|CATEGORI[AO]|CATEGORA|CATEGORIA R|CAT\\W)\\s*(\\d+\\w*|VI|V|IV|III|II|I)\\W*\\w?(BIRADS|CATEGORI[AO]|CATEGORA|CATEGORIA R)?(\\W|$)\", 2\n",
    ")\n",
    "```\n",
    "\n",
    "Esta expressão:\n",
    "1. Normaliza o texto (maiúsculas, remoção de caracteres especiais)\n",
    "2. Extrai a seção relevante do laudo (após conclusão/impressão diagnóstica)\n",
    "3. Localiza menções a BI-RADS em diversos formatos\n",
    "4. Captura o valor numérico ou romano da classificação\n",
    "\n",
    "### DataFrames Criados\n",
    "\n",
    "A célula cria dois DataFrames importantes:\n",
    "\n",
    "1. **`df_spk`**: DataFrame principal contendo todos os laudos processados\n",
    "   ```python\n",
    "   df_spk = spark.sql(query.format(where_clause = where_clause, filtro_ativacao = \"\"))\n",
    "   ```\n",
    "\n",
    "2. **`df_spk_ativacao`**: DataFrame filtrado apenas com pacientes elegíveis para notificações\n",
    "   ```python\n",
    "   df_spk_ativacao = spark.sql(query.format(where_clause = \"\", filtro_ativacao = filtro_ativacao))\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aadefc42-e4db-4c80-95c0-7a3a248212bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH base AS (\n",
    "    SELECT\n",
    "        flr.ficha,\n",
    "        flr.sigla_exame, \n",
    "        flr.id_marca,\n",
    "        flr.sequencial,\n",
    "        flr.laudo_tratado,\n",
    "        REGEXP_EXTRACT_ALL(\n",
    "            REGEXP_EXTRACT(\n",
    "                REGEXP_REPLACE(UPPER(flr.laudo_tratado), r'[-:®]|\\xa0', ''),\n",
    "                r'(?mi)(AVALIA[CÇ][AÃ]O|CONCLUS[AÃ]O|IMPRESS[AÃ]O|OPINI[AÃ]O)?(.*)', 2\n",
    "            ),\n",
    "            r\"(?mi)(BIRADS|CATEGORI[AO]|CATEGORA|CATEGORIA R|CAT\\W)\\s*(\\d+\\w*|VI|V|IV|III|II|I)\\W*\\w?(BIRADS|CATEGORI[AO]|CATEGORA|CATEGORIA R)?(\\W|$)\", 2\n",
    "        )        \n",
    "        AS RAW_BIRADS,\n",
    "        FILTER(\n",
    "            TRANSFORM(RAW_BIRADS, x ->\n",
    "                CASE\n",
    "                    WHEN x = \"I\" THEN 1\n",
    "                    WHEN x = \"II\" THEN 2\n",
    "                    WHEN x = \"III\" THEN 3\n",
    "                    WHEN x = \"IV\" THEN 4\n",
    "                    WHEN x = \"V\" THEN 5\n",
    "                    WHEN x = \"VI\" THEN 6\n",
    "                    WHEN TRY_CAST(x AS INT) > 6 THEN NULL\n",
    "                    ELSE REGEXP_REPLACE(x, r'[^0-9]', '')\n",
    "                END\n",
    "            ), x -> x IS NOT NULL\n",
    "        ) AS CAT_BIRADS\n",
    "    FROM refined.saude_preventiva.pardini_laudos flr\n",
    "    WHERE\n",
    "        flr.linha_cuidado = 'mama'\n",
    "        AND UPPER(flr.sexo_cliente) = 'F' \n",
    "        AND flr.sigla_exame IN ('MAMO','MAMODI')\n",
    "),\n",
    " \n",
    "dados_birads AS (\n",
    "    SELECT\n",
    "        *,\n",
    "        ARRAY_MIN(CAT_BIRADS) AS MIN_BIRADS,\n",
    "        ARRAY_MAX(CAT_BIRADS) AS MAX_BIRADS,\n",
    "        TRY_ELEMENT_AT(CAST(CAT_BIRADS AS ARRAY<INT>), -1) AS BIRADS\n",
    "    FROM base\n",
    "),\n",
    " \n",
    "dados_laudos AS (\n",
    "    SELECT\n",
    "        flr.linha_cuidado,\n",
    "        flr.id_unidade,\n",
    "        flr.id_ficha,\n",
    "        flr.id_exame, \n",
    "        flr.id_marca,\n",
    "        flr.sequencial,\n",
    "        flr.ficha,\n",
    "        flr.id_cliente,\n",
    "        flr.pefi_cliente,\n",
    "        flr.sigla_exame,\n",
    "        flr.marca,\n",
    "        flr.laudo_tratado,\n",
    "        (\n",
    "          TIMESTAMPDIFF(DAY, flr.dth_nascimento_cliente, CURDATE()) / 365.25\n",
    "        ) AS idade_cliente,\n",
    "        flr.sexo_cliente,\n",
    "        flr.dth_pedido,\n",
    "        flr._datestamp\n",
    "    FROM refined.saude_preventiva.pardini_laudos flr\n",
    "    {where_clause}\n",
    ")\n",
    " \n",
    "SELECT\n",
    "    flr.* except(idade_cliente),\n",
    "    brd.MIN_BIRADS,\n",
    "    brd.MAX_BIRADS,\n",
    "    brd.BIRADS,\n",
    " \n",
    "    eleg.dth_pedido        AS dth_pedido_retorno_elegivel,\n",
    "    eleg.ficha             AS ficha_retorno_elegivel,\n",
    "    eleg.siglas_ficha      AS siglas_ficha_retorno_elegivel,\n",
    "    eleg.marca             AS marca_retorno_elegivel,\n",
    "    eleg.unidade           AS unidade_retorno_elegivel,\n",
    "    eleg.convenio          AS convenio_retorno_elegivel,\n",
    "    eleg.valores_exame     AS valores_exame_retorno_elegivel,\n",
    "    eleg.valores_ficha     AS valores_ficha_retorno_elegivel,\n",
    "    eleg.qtd_exame         AS qtd_exame_retorno_elegivel,\n",
    "    eleg.secao             AS secao_retorno_elegivel,\n",
    "    eleg.dias_entre_ficha  AS dias_entre_ficha_elegivel\n",
    "\n",
    "FROM dados_laudos flr\n",
    "INNER JOIN dados_birads brd\n",
    "    ON flr.ficha = brd.ficha\n",
    "    AND flr.sigla_exame = brd.sigla_exame    \n",
    "    AND flr.id_marca = brd.id_marca\n",
    "    AND flr.sequencial = brd.sequencial    \n",
    "LEFT JOIN refined.saude_preventiva.pardini_retorno_elegivel_ficha eleg\n",
    "    ON eleg.ficha_origem = flr.ficha\n",
    "    AND eleg.id_cliente = flr.id_cliente\n",
    "    AND eleg.linha_cuidado = flr.linha_cuidado\n",
    "{filtro_ativacao}\n",
    "\"\"\"\n",
    " \n",
    "df_spk = spark.sql(query.format(\n",
    "    where_clause = where_clause,\n",
    "    filtro_ativacao = \"\"\n",
    "    )\n",
    ")\n",
    " \n",
    "df_spk_ativacao = spark.sql(query.format(\n",
    "    where_clause = \"\",\n",
    "    filtro_ativacao = filtro_ativacao\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b090da24-2739-4ab8-a347-41a08bf377e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Função para Cálculo da Data Prevista de Retorno\n",
    "\n",
    "Esta célula define a função `calcular_data_prevista`, que adiciona uma coluna ao DataFrame com a data recomendada de retorno para cada paciente, com base na sua classificação BI-RADS.\n",
    "\n",
    "### Objetivo da Função\n",
    "\n",
    "Calcular e adicionar a coluna `dth_previsao_retorno` ao DataFrame, definindo uma data futura recomendada para o próximo exame de mamografia, seguindo os protocolos médicos para acompanhamento de cada classificação BI-RADS.\n",
    "\n",
    "### Parâmetros e Retorno\n",
    "\n",
    "**Parâmetros:**\n",
    "- `df_spk` (DataFrame): DataFrame Spark contendo a coluna `BIRADS` e `dth_pedido`\n",
    "\n",
    "**Retorno:**\n",
    "- DataFrame com a coluna adicional `dth_previsao_retorno`\n",
    "\n",
    "### Lógica de Cálculo\n",
    "\n",
    "A função utiliza expressões condicionais baseadas no valor de BI-RADS:\n",
    "\n",
    "1. **BI-RADS 1 ou 2** (achados negativos ou benignos):\n",
    "   - Adiciona 360 dias (aproximadamente 1 ano) à data do pedido\n",
    "   - `when(col('BIRADS').isin([1, 2]), expr(\"date_add(dth_pedido, 360)\"))`\n",
    "\n",
    "2. **BI-RADS 3** (achados provavelmente benignos):\n",
    "   - Adiciona 180 dias (aproximadamente 6 meses) à data do pedido\n",
    "   - `when(col('BIRADS') == 3, expr(\"date_add(dth_pedido, 180)\"))`\n",
    "\n",
    "3. **Outros valores de BI-RADS** (4, 5, 6):\n",
    "   - Define como None (null), pois requerem outros tipos de acompanhamento\n",
    "   - `.otherwise(None)`\n",
    "\n",
    "### Implementação Técnica\n",
    "\n",
    "A implementação utiliza a API DataFrame do PySpark:\n",
    "- `withColumn()` para adicionar uma nova coluna ao DataFrame\n",
    "- `when()...otherwise()` para expressões condicionais\n",
    "- `expr()` para aplicar a função SQL `date_add()` que realiza os cálculos de data\n",
    "\n",
    "Esta abordagem segue os protocolos médicos padrão para acompanhamento de exames de mamografia baseado na classificação BI-RADS, onde categorias 1-2 são de baixo risco e categoria 3 requer acompanhamento mais frequente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f56c03bc-c3bc-47bc-a77a-f4059f5f9a9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def calcular_data_prevista(df_spk: DataFrame):\n",
    "    \"\"\"\n",
    "    Adiciona uma coluna 'dth_previsao_retorno' ao DataFrame com base na coluna 'BIRADS'.\n",
    "\n",
    "    - Para BIRADS 1 ou 2, adiciona 360 dias à data da coluna 'dth_pedido'.\n",
    "    - Para BIRADS 3, adiciona 180 dias à data da coluna 'dth_pedido'.\n",
    "    - Para outros valores de BIRADS, define 'dth_previsao_retorno' como None.\n",
    "\n",
    "    Parâmetros:\n",
    "    df_spk (DataFrame): O DataFrame Spark contendo os dados de entrada.\n",
    "\n",
    "    Retorna:\n",
    "    DataFrame: O DataFrame atualizado com a nova coluna 'dth_previsao_retorno'.\n",
    "    \"\"\"\n",
    "    df_spk = df_spk.withColumn(\n",
    "        'dth_previsao_retorno',\n",
    "        when(\n",
    "            col('BIRADS').isin([1, 2]),\n",
    "            expr(\"date_add(dth_pedido, 360)\")\n",
    "        ).when(\n",
    "            col('BIRADS') == 3,\n",
    "            expr(\"date_add(dth_pedido, 180)\")  \n",
    "        ).otherwise(None)\n",
    "    )\n",
    "    return df_spk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06652332-b503-4cc9-8845-534da7f56882",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Função de Transformação de Campos\n",
    "\n",
    "Esta célula define a função `transform_fields`, responsável por aplicar várias transformações ao DataFrame para enriquecê-lo com informações calculadas e preparar os dados para análise posterior.\n",
    "\n",
    "### Objetivo da Função\n",
    "\n",
    "Aplicar um conjunto de transformações aos DataFrames gerados pela consulta SQL, adicionando novas colunas com informações derivadas e convertendo formatos de dados para tipos apropriados.\n",
    "\n",
    "### Parâmetros e Retorno\n",
    "\n",
    "**Parâmetros:**\n",
    "- `df_spk` (DataFrame): DataFrame Spark a ser transformado\n",
    "\n",
    "**Retorno:**\n",
    "- DataFrame enriquecido com novas colunas e transformações\n",
    "\n",
    "### Lógica Detalhada\n",
    "\n",
    "A função executa as seguintes transformações, em sequência:\n",
    "\n",
    "1. **Verificação de DataFrame Vazio**:\n",
    "   - Verifica se o DataFrame está vazio usando `.isEmpty()`\n",
    "   - Se vazio, registra um aviso no log e retorna o DataFrame sem alterações\n",
    "   \n",
    "2. **Adição da Coluna de Recomendação de Retorno**:\n",
    "   - Cria a coluna `retorno_cliente` que indica o tempo recomendado para retorno em meses\n",
    "   - BI-RADS 1-2: 12 meses\n",
    "   - BI-RADS 3: 6 meses\n",
    "   - Outros valores: 0 (sem recomendação automática)\n",
    "\n",
    "3. **Cálculo da Data Prevista de Retorno**:\n",
    "   - Chama a função auxiliar `calcular_data_prevista()` definida anteriormente\n",
    "   \n",
    "4. **Conversão de Formatos de Data**:\n",
    "   - Converte `dth_pedido_retorno_elegivel` para formato timestamp\n",
    "   - Converte `dth_previsao_retorno` para formato timestamp\n",
    "   \n",
    "5. **Cálculo do Intervalo entre Datas**:\n",
    "   - Calcula a diferença em dias entre a data prevista de retorno e a data do pedido\n",
    "   - Armazena o resultado na coluna `dias_ate_retorno`\n",
    "\n",
    "### Variáveis Criadas/Modificadas\n",
    "\n",
    "- **`retorno_cliente`**: Tempo de retorno recomendado em meses (12, 6 ou 0)\n",
    "- **`dth_previsao_retorno`**: Data calculada para o próximo exame\n",
    "- **`dias_ate_retorno`**: Número de dias entre o exame atual e o próximo recomendado\n",
    "\n",
    "### Impacto\n",
    "\n",
    "Esta função enriquece os dados com informações cruciais para o sistema de notificações, permitindo identificar quando um paciente deve ser contatado para realizar um novo exame de acompanhamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fad58918-ae2d-4077-a4be-69cb72e518d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def transform_fields(df_spk: DataFrame) -> DataFrame:\n",
    "  \"\"\"\n",
    "  Transforma os campos do DataFrame fornecido.\n",
    "\n",
    "  - Verifica se o DataFrame está vazio. Se estiver, registra um aviso e retorna o DataFrame sem alterações.\n",
    "  - Adiciona uma coluna 'retorno_cliente' com valores baseados na coluna 'BIRADS':\n",
    "    - 12 meses para BIRADS 1 ou 2\n",
    "    - 6 meses para BIRADS 3\n",
    "    - 0 para outros valores\n",
    "  - Calcula a data prevista de retorno usando a função 'calcular_data_prevista'.\n",
    "  - Converte a coluna 'dth_pedido_retorno_elegivel' para o tipo timestamp.\n",
    "  - Converte a coluna 'dth_previsao_retorno' para o tipo timestamp.\n",
    "  - Calcula a diferença em dias entre 'dth_previsao_retorno' e 'dth_pedido', armazenando o resultado na coluna 'dias_ate_retorno'.\n",
    "\n",
    "  Parâmetros:\n",
    "  df_spk (DataFrame): O DataFrame a ser transformado.\n",
    "\n",
    "  Retorna:\n",
    "  DataFrame: O DataFrame transformado com as novas colunas.\n",
    "  \"\"\"\n",
    "\n",
    "  if df_spk.isEmpty():\n",
    "      logger.warning(\"No Data Found!\")\n",
    "      return df_spk\n",
    "\n",
    "  df_spk = df_spk.withColumn(\n",
    "      'retorno_cliente',\n",
    "      when(col('BIRADS').isin([1, 2]), 12).when(col('BIRADS') == 3, 6).otherwise(0)\n",
    "  )\n",
    "\n",
    "  df_spk = calcular_data_prevista(df_spk)\n",
    "  df_spk = df_spk.withColumn('dth_pedido_retorno_elegivel', to_timestamp(col('dth_pedido_retorno_elegivel')))\n",
    "  df_spk = df_spk.withColumn('dth_previsao_retorno', to_timestamp(col('dth_previsao_retorno')))\n",
    "  df_spk = df_spk.withColumn('dias_ate_retorno', datediff(to_date(col('dth_previsao_retorno')), to_date(col('dth_pedido'))))\n",
    "  return df_spk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54b6aad5-7d65-445d-babd-e6408da8ce23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuração de Webhook e Sistema de Tratamento de Erros\n",
    "\n",
    "Esta célula configura o sistema de alerta e define a função `error_message` para gerenciar erros de forma padronizada, garantindo que falhas durante o processamento sejam devidamente registradas e comunicadas.\n",
    "\n",
    "### Configurações Iniciais\n",
    "\n",
    "- **`WEBHOOK_DS_AI_BUSINESS_STG`**: Define o ambiente para o webhook do Sentinel\n",
    "  ```python\n",
    "  WEBHOOK_DS_AI_BUSINESS_STG = 'stg'  # Ambiente de homologação/staging\n",
    "  ```\n",
    "\n",
    "### Função `error_message`\n",
    "\n",
    "A função `error_message` é responsável pelo tratamento padronizado de erros durante a execução do pipeline, especialmente durante as operações de salvamento de dados.\n",
    "\n",
    "#### Parâmetros\n",
    "- `e` (Exception): A exceção capturada durante a execução\n",
    "\n",
    "#### Comportamento\n",
    "1. **Captura detalhada do erro**: \n",
    "   - Obtém o traceback completo usando `traceback.format_exc()`\n",
    "   - Formata uma mensagem informativa com esses detalhes\n",
    "\n",
    "2. **Envio de alerta**: \n",
    "   - Inicializa o objeto `Sentinel` com informações específicas do projeto:\n",
    "     - `project_name`: 'Monitor_Linhas_Cuidado_Mama'\n",
    "     - `env_type`: Ambiente definido em `WEBHOOK_DS_AI_BUSINESS_STG`\n",
    "     - `task_title`: 'Pardini Mama'\n",
    "   - Envia o alerta através de `alerta_sentinela()` com:\n",
    "     - Categoria: 'Alerta'\n",
    "     - Mensagem detalhada do erro\n",
    "     - ID descritivo: '1_pardini_mama_birads'\n",
    "\n",
    "3. **Registro do erro**:\n",
    "   - Exibe o traceback completo no console via `traceback.print_exc()`\n",
    "   - Relança a exceção para interromper a execução em caso de falha crítica\n",
    "\n",
    "### Importância\n",
    "\n",
    "Este sistema de tratamento de erros garante que:\n",
    "- Problemas durante o processamento sejam detectados rapidamente\n",
    "- A equipe responsável seja notificada automaticamente\n",
    "- Informações detalhadas sobre o erro estejam disponíveis para diagnóstico\n",
    "- O processo seja interrompido de forma apropriada em caso de falhas críticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc7df8df-ccb1-4301-9b27-ebd84d470cb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "WEBHOOK_DS_AI_BUSINESS_STG = 'stg'\n",
    "\n",
    "def error_message(e):\n",
    "    \"\"\"\n",
    "    Envia alerta para o Sentinel e exibe o traceback em caso de erro ao salvar dados.\n",
    "\n",
    "    Parâmetros:\n",
    "        e (Exception): Exceção capturada.\n",
    "\n",
    "    Comportamento:\n",
    "        - Formata o traceback do erro.\n",
    "        - Envia alerta para o Sentinel com detalhes do erro.\n",
    "        - Exibe o traceback no console.\n",
    "        - Relança a exceção.\n",
    "    \"\"\"\n",
    "    error_message = traceback.format_exc()\n",
    "    summary_message = f\"\"\"Erro ao salvar dados.\\n{error_message}\"\"\"\n",
    "    sentinela_ds_ai_business = Sentinel(\n",
    "        project_name='Monitor_Linhas_Cuidado_Mama',\n",
    "        env_type=WEBHOOK_DS_AI_BUSINESS_STG,\n",
    "        task_title='Pardini Mama'\n",
    "    )\n",
    "    sentinela_ds_ai_business.alerta_sentinela(\n",
    "        categoria='Alerta', \n",
    "        mensagem=summary_message,\n",
    "        job_id_descritivo='1_pardini_mama_birads'\n",
    "    )\n",
    "    traceback.print_exc()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "513c028c-a58a-4978-b8d8-4aabdfef36b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Funções de Persistência de Dados\n",
    "\n",
    "Esta célula define três funções essenciais para gerenciar a persistência dos dados processados em tabelas Delta Lake: `insert_data`, `merge_data` e `save_data`.\n",
    "\n",
    "### 1. Função `insert_data`\n",
    "\n",
    "**Objetivo:**\n",
    "Inserir dados em uma tabela Delta, realizando uma operação de sobrescrita completa (overwrite).\n",
    "\n",
    "**Parâmetros:**\n",
    "- `df_spk` (DataFrame): DataFrame Spark com os dados a serem inseridos\n",
    "- `table_name` (str): Nome da tabela Delta de destino\n",
    "\n",
    "**Comportamento:**\n",
    "- Registra a operação no log (`logger.info`)\n",
    "- Configura as opções de escrita para:\n",
    "  - Modo 'overwrite' para substituição completa\n",
    "  - Permitir alterações de schema (`mergeSchema=true`, `overwriteSchema=true`)\n",
    "  - Usar o formato Delta Lake\n",
    "  - Particionar os dados pela coluna `_datestamp` para otimizar consultas por data\n",
    "- Captura exceções e aciona o tratamento de erros via `error_message()`\n",
    "\n",
    "### 2. Função `merge_data`\n",
    "\n",
    "**Objetivo:**\n",
    "Realizar uma operação de merge (upsert) em tabelas Delta existentes, atualizando registros existentes e inserindo novos.\n",
    "\n",
    "**Parâmetros:**\n",
    "- `df_spk` (DataFrame): DataFrame Spark com os dados a serem mesclados\n",
    "- `table_name` (str): Nome da tabela Delta de destino\n",
    "\n",
    "**Comportamento:**\n",
    "- Registra a operação no log\n",
    "- Cria uma view temporária com os dados do DataFrame (`increment_birads`)\n",
    "- Executa um comando MERGE SQL que:\n",
    "  - Atualiza registros existentes (identificados pela chave composta `ficha`, `sequencial`, `sigla_exame`)\n",
    "  - Insere novos registros quando não existem correspondências\n",
    "- Captura exceções e aciona o tratamento de erros\n",
    "\n",
    "### 3. Função `save_data`\n",
    "\n",
    "**Objetivo:**\n",
    "Função de alto nível que decide entre inserção ou mesclagem com base no contexto.\n",
    "\n",
    "**Parâmetros:**\n",
    "- `df_spk` (DataFrame): DataFrame Spark com os dados a serem salvos\n",
    "- `table_name` (str): Nome da tabela Delta de destino\n",
    "\n",
    "**Lógica:**\n",
    "- Verifica se o DataFrame está vazio - se estiver, retorna sem ação\n",
    "- Verifica se a tabela já existe:\n",
    "  - Se existir, chama `merge_data()` para realizar upsert\n",
    "  - Se não existir, chama `insert_data()` para criar a tabela\n",
    "\n",
    "### Considerações Técnicas\n",
    "\n",
    "- A função `merge_data` utiliza o padrão MERGE INTO do SQL, otimizado para operações de upsert em Delta Lake\n",
    "- A partição por `_datestamp` melhora a performance para consultas filtradas por data\n",
    "- A opção `overwriteSchema=true` permite que o schema evolua com o tempo, adicionando novos campos quando necessário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2217e90c-c05d-4630-b4a9-5ee6cb2f312e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def insert_data(df_spk: DataFrame, table_name: str):\n",
    "    \"\"\"\n",
    "    Insere os dados do DataFrame na tabela Delta especificada, sobrescrevendo o conteúdo existente.\n",
    "    \n",
    "    Parâmetros:\n",
    "        df_spk (DataFrame): DataFrame a ser salvo.\n",
    "        table_name (str): Nome da tabela Delta de destino.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Inserting Data: {table_name}\")\n",
    "        (\n",
    "            df_spk.write\n",
    "                .mode('overwrite')\n",
    "                .option('mergeSchema','true')\n",
    "                .option('overwriteSchema','true')\n",
    "                .format('delta')\n",
    "                .partitionBy('_datestamp')\n",
    "                .saveAsTable(table_name)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        error_message(e)\n",
    " \n",
    "def merge_data(df_spk: DataFrame, table_name: str):\n",
    "    \"\"\"\n",
    "    Realiza merge dos dados do DataFrame na tabela Delta, atualizando registros existentes e inserindo novos.\n",
    "    \n",
    "    Parâmetros:\n",
    "        df_spk (DataFrame): DataFrame com os dados incrementais.\n",
    "        table_name (str): Nome da tabela Delta de destino.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Merging Data: {table_name}\")\n",
    "        df_spk.createOrReplaceTempView(f\"increment_birads\")\n",
    "        merge_query = f\"\"\"\n",
    "            MERGE INTO {table_name} AS target\n",
    "            USING increment_birads AS source\n",
    "                ON target.ficha = source.ficha\n",
    "                AND target.sequencial = source.sequencial\n",
    "                AND target.sigla_exame = source.sigla_exame\n",
    "            WHEN MATCHED THEN\n",
    "                UPDATE SET *\n",
    "            WHEN NOT MATCHED THEN\n",
    "                INSERT *\n",
    "        \"\"\"\n",
    "        spark.sql(merge_query)\n",
    "    except Exception as e:\n",
    "        error_message(e)\n",
    " \n",
    "def save_data(df_spk: DataFrame, table_name: str):\n",
    "    \"\"\"\n",
    "    Salva os dados do DataFrame na tabela Delta, utilizando merge se a tabela existir ou insert se não existir.\n",
    "    \n",
    "    Parâmetros:\n",
    "        df_spk (DataFrame): DataFrame a ser salvo.\n",
    "        table_name (str): Nome da tabela Delta de destino.\n",
    "    \"\"\"\n",
    "    if df_spk.isEmpty():\n",
    "        return None\n",
    " \n",
    "    if spark.catalog.tableExists(table_name):\n",
    "        merge_data(df_spk, table_name)\n",
    "    else:\n",
    "        insert_data(df_spk, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d88891e5-06f5-4f84-a7f1-f38e36a4d5b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Aplicação das Transformações nos DataFrames\n",
    "\n",
    "Esta célula aplica as transformações definidas na função `transform_fields` aos dois DataFrames gerados anteriormente: `df_spk` e `df_spk_ativacao`.\n",
    "\n",
    "### Objetivo da Célula\n",
    "\n",
    "Enriquecer os DataFrames com informações calculadas e transformar os dados para análise e persistência, aplicando as mesmas transformações tanto ao conjunto completo de dados quanto ao subconjunto de pacientes elegíveis para ativação.\n",
    "\n",
    "### Operações Realizadas\n",
    "\n",
    "1. **Para o DataFrame principal `df_spk`**:\n",
    "   - Adiciona a coluna `retorno_cliente` com o tempo recomendado em meses\n",
    "   - Calcula a data prevista de retorno em `dth_previsao_retorno`\n",
    "   - Converte campos de data para formato timestamp\n",
    "   - Calcula o intervalo em dias até o retorno em `dias_ate_retorno`\n",
    "\n",
    "2. **Para o DataFrame de ativação `df_spk_ativacao`**:\n",
    "   - Aplica as mesmas transformações que no DataFrame principal\n",
    "   - Os dados já estão pré-filtrados para incluir apenas pacientes elegíveis para notificações\n",
    "\n",
    "### Dependências\n",
    "\n",
    "- Função `transform_fields` definida anteriormente\n",
    "- DataFrames `df_spk` e `df_spk_ativacao` gerados pela consulta SQL\n",
    "\n",
    "### Impacto\n",
    "\n",
    "Após a execução desta célula, ambos os DataFrames estarão preparados com todos os campos calculados necessários para:\n",
    "- Análises sobre os prazos de retorno recomendados\n",
    "- Identificação de pacientes com exames em atraso\n",
    "- Geração de notificações com informações precisas sobre datas e prazos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5436fc4f-334a-4ac0-8ca4-52eb6d0405c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_spk = transform_fields(df_spk)\n",
    "df_spk_ativacao = transform_fields(df_spk_ativacao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d1886a6-63c4-4726-a24c-e6c715201042",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deduplicação de Registros para Notificações\n",
    "\n",
    "Esta célula realiza a deduplicação de registros no DataFrame de ativação (`df_spk_ativacao`), para garantir que cada paciente receba apenas uma notificação por ficha, independentemente da quantidade de exames realizados.\n",
    "\n",
    "### Objetivo da Célula\n",
    "\n",
    "Eliminar registros duplicados do DataFrame `df_spk_ativacao` para que cada ficha (que representa um atendimento) apareça apenas uma vez na lista de ativação, evitando o envio de múltiplas notificações para o mesmo paciente sobre o mesmo atendimento.\n",
    "\n",
    "### Lógica Implementada\n",
    "\n",
    "A célula utiliza o método `.dropDuplicates(['ficha'])` do PySpark DataFrame API para:\n",
    "\n",
    "1. Identificar registros com o mesmo valor na coluna `ficha`\n",
    "2. Manter apenas a primeira ocorrência de cada valor único de `ficha`\n",
    "3. Descartar todas as ocorrências duplicadas subsequentes\n",
    "\n",
    "### Contexto Importante\n",
    "\n",
    "Na estrutura de dados do Pardini:\n",
    "- Uma `ficha` corresponde a um atendimento ou visita do paciente\n",
    "- Um atendimento pode conter vários exames, representados por diferentes valores de `id_exame`, `sigla_exame`, etc.\n",
    "\n",
    "Sem esta deduplicação, o sistema poderia enviar múltiplas notificações para o mesmo paciente sobre o mesmo atendimento, apenas porque foram realizados vários procedimentos durante aquela visita. A deduplicação garante que a comunicação com o paciente seja apropriada e não excessiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "942a9487-2bca-455f-9675-67f7c0844b40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Excluir duplicados para considerar apenas a ficha na ativação e não os exames (itens). Assim vamos enviar apenas \n",
    "# 1 push por ficha\n",
    "df_spk_ativacao = df_spk_ativacao.dropDuplicates(['ficha'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8550136-f87a-41c1-b3c8-c3bc56e3dd8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Contagem de Laudos Processados\n",
    "\n",
    "Esta célula exibe a quantidade de registros em cada DataFrame após todas as transformações e filtragens. Este é um passo importante para monitorar o volume de dados processados e validar se os filtros estão funcionando como esperado.\n",
    "\n",
    "### Objetivo da Célula\n",
    "\n",
    "Contar e exibir o número de registros nos dois principais DataFrames do pipeline, oferecendo uma verificação visual do volume de dados que serão persistidos nas tabelas Delta.\n",
    "\n",
    "### Saídas Exibidas\n",
    "\n",
    "Duas métricas principais são apresentadas:\n",
    "\n",
    "1. **`quantidade de laudos salvos na tabela`**: Número total de laudos de mamografia do Pardini com classificação BI-RADS extraída e processada\n",
    "   - Este valor representa todos os laudos processados, independentemente da classificação BI-RADS\n",
    "\n",
    "2. **`quantidade de laudos salvos na tabela de ativação`**: Número de registros únicos (por ficha) que atendem aos critérios para notificação\n",
    "   - Este subconjunto inclui apenas pacientes com BI-RADS 1, 2 ou 3, do sexo feminino, entre 40-75 anos e sem retorno já registrado\n",
    "   - Os registros também foram deduplicados por ficha\n",
    "\n",
    "### Utilidade das Métricas\n",
    "\n",
    "Estas contagens são úteis para:\n",
    "- Verificar a eficácia dos filtros aplicados\n",
    "- Estimar o volume de notificações que serão enviadas\n",
    "- Detectar possíveis problemas de dados (valores muito baixos ou muito altos podem indicar anomalias)\n",
    "- Acompanhar tendências de volume ao longo do tempo, quando o notebook é executado periodicamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "408f1795-70e0-47ee-ac83-e710b3ec2d23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print('quantidade de laudos salvos na tabela',df_spk.count())\n",
    "print('quantidade de laudos salvos na tabela de ativação', df_spk_ativacao.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce7c6c38-cc0b-4de0-b07c-3186239ddfff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Salvamento dos Dados nas Tabelas Delta\n",
    "\n",
    "Esta célula final executa a persistência dos dados processados nas tabelas Delta Lake, concluindo o pipeline de processamento. Os dados serão utilizados para análises posteriores e para o sistema de notificação de pacientes.\n",
    "\n",
    "### Objetivo da Célula\n",
    "\n",
    "Salvar os dois DataFrames processados nas suas respectivas tabelas Delta:\n",
    "\n",
    "1. **Tabela Principal de BI-RADS**: \n",
    "   ```python\n",
    "   save_data(df_spk, table_birads)\n",
    "   ```\n",
    "   - Armazena todos os laudos com classificação BI-RADS extraída e dados enriquecidos\n",
    "   - Utiliza a função inteligente `save_data()` que decide entre operações de merge ou insert baseado na existência prévia da tabela\n",
    "\n",
    "2. **Tabela de Ativação**: \n",
    "   ```python\n",
    "   insert_data(df_spk_ativacao, table_birads_ativacao)\n",
    "   ```\n",
    "   - Armazena apenas os pacientes elegíveis para notificações\n",
    "   - Utiliza `insert_data()` para substituir completamente os dados, garantindo que apenas pacientes atualmente elegíveis estejam presentes\n",
    "\n",
    "### Tabelas de Destino\n",
    "\n",
    "- **`refined.saude_preventiva.pardini_laudos_mama_birads`**: Tabela histórica contendo todos os laudos processados\n",
    "- **`refined.saude_preventiva.pardini_laudos_mama_birads_ativacao`**: Tabela com apenas os pacientes elegíveis para notificação (BI-RADS 1-3)\n",
    "\n",
    "### Comportamento das Funções\n",
    "\n",
    "- **Para a tabela principal**: \n",
    "  - Se a tabela já existe: Realiza operação de merge (upsert)\n",
    "  - Se a tabela não existe: Cria a tabela com os dados do DataFrame\n",
    "\n",
    "- **Para a tabela de ativação**: \n",
    "  - Sempre realiza operação de substituição completa (overwrite)\n",
    "  - Particiona os dados pela coluna `_datestamp`\n",
    "\n",
    "### Impacto\n",
    "\n",
    "Após a execução desta célula, os dados estarão disponíveis para consumo por outros sistemas, como dashboards de BI ou sistemas de notificação automática de pacientes. A tabela de ativação, em particular, será utilizada para gerar alertas para pacientes que necessitam realizar exames de acompanhamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2ed5a97-36b4-4d8e-8441-55d4b71b7516",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "save_data(df_spk, table_birads)\n",
    "insert_data(df_spk_ativacao, table_birads_ativacao)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1_pardini_mama_birads",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
