{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "474c8cc2",
   "metadata": {},
   "source": [
    "# Documentação Técnica: Extração e Processamento de BI-RADS - Fleury\n",
    "\n",
    "## Objetivo Principal\n",
    "**Este notebook realiza a extração, classificação e persistência dos dados de laudos de mamografia do Fleury, com foco na classificação BI-RADS.** O objetivo é identificar e classificar exames de mama segundo o BI-RADS, além de gerar uma lista de ativação para notificação de pacientes com exames em atraso.\n",
    "\n",
    "## Tecnologias Utilizadas\n",
    "- **PySpark**: Manipulação de dados, consultas SQL, transformações e persistência em Delta Lake.\n",
    "- **Octoops**: Monitoramento, alertas e integração com sistemas de controle de execução.\n",
    "- **Delta Lake**: Persistência dos dados processados.\n",
    "- **Python (logging, traceback, etc.)**: Controle de fluxo, tratamento de erros e registro de eventos.\n",
    "\n",
    "## Fluxo de Trabalho/Etapas Principais\n",
    "1. Instalação de dependências (`octoops`).\n",
    "2. Configuração do ambiente (reinicialização do kernel e importação de bibliotecas).\n",
    "3. Definição de tabelas e filtros SQL para seleção incremental e ativação de pacientes.\n",
    "4. Consulta SQL principal para extração e enriquecimento dos dados dos laudos.\n",
    "5. Transformações nos DataFrames para cálculo de datas previstas de retorno, classificação e cálculo de diferenças de datas.\n",
    "6. Persistência dos dados nas tabelas Delta, com tratamento de erros e envio de alertas.\n",
    "7. Remoção de duplicados na lista de ativação.\n",
    "8. Impressão da quantidade de registros salvos.\n",
    "\n",
    "## Dados Envolvidos\n",
    "- **Fonte**: Tabela `refined.saude_preventiva.fleury_laudos`.\n",
    "- **Tabelas de destino**: `refined.saude_preventiva.fleury_laudos_mama_birads` e `refined.saude_preventiva.fleury_laudos_mama_birads_ativacao`.\n",
    "- **Colunas importantes**: `laudo_tratado`, `BIRADS`, `MIN_BIRADS`, `MAX_BIRADS`, `dth_pedido`, `dth_previsao_retorno`, `dias_ate_retorno`, entre outras.\n",
    "\n",
    "## Resultados/Saídas Esperadas\n",
    "- DataFrame completo com laudos processados e classificados segundo BI-RADS.\n",
    "- DataFrame filtrado para ativação de pacientes elegíveis para notificação.\n",
    "- Persistência dos dados nas tabelas Delta.\n",
    "- Alertas automáticos em caso de falhas ou ausência de dados.\n",
    "\n",
    "## Pré-requisitos\n",
    "- Ambiente Databricks ou Spark configurado.\n",
    "- Pacotes: `octoops`, permissões de escrita nas tabelas Delta.\n",
    "- Acesso às tabelas Delta e permissões de escrita.\n",
    "\n",
    "## Considerações Importantes\n",
    "- O notebook utiliza expressões regulares avançadas para correta identificação dos valores BI-RADS.\n",
    "- O merge/upsert garante atualização incremental dos dados sem duplicidade.\n",
    "- O monitoramento via Octoops/Sentinel facilita rastreabilidade e resposta rápida a falhas.\n",
    "- O filtro de idade e sexo garante que apenas pacientes do público-alvo sejam analisados.\n",
    "\n",
    "---\n",
    "Cada célula de código abaixo é precedida por uma explicação técnica detalhada sobre sua função e impacto no fluxo do notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99123c8f",
   "metadata": {},
   "source": [
    "### Instalação do pacote octoops\n",
    "**Objetivo da Célula:** Instalar o pacote octoops diretamente do repositório privado do GitLab.\n",
    "\n",
    "**Dependências:** Requer conectividade com o GitLab e credenciais de acesso (token) para acessar o repositório privado.\n",
    "\n",
    "**Variáveis/Objetos Criados/Modificados:** Nenhum explicitamente, mas instala o pacote `octoops` no ambiente Python.\n",
    "\n",
    "**Lógica Detalhada:** \n",
    "- Usa o comando pip com um parâmetro `--extra-index-url` que aponta para um repositório privado no GitLab.\n",
    "- Utiliza credenciais (token) codificadas na URL para autenticação.\n",
    "- O parâmetro `!` indica que o comando deve ser executado no sistema operacional, não no interpretador Python.\n",
    "\n",
    "**Saída/Impacto:** Após execução, o pacote `octoops` estará disponível para importação no ambiente. Os logs da instalação serão exibidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1762f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show octoops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8b91f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install octoops==0.21.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e298c85f",
   "metadata": {},
   "source": [
    "### Reinicialização do ambiente Python (Databricks)\n",
    "**Objetivo da Célula:** Reiniciar o kernel Python do Databricks para garantir que as novas instalações de pacotes sejam carregadas corretamente.\n",
    "\n",
    "**Dependências:** Requer o ambiente Databricks com a API `dbutils` disponível.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7c3f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090de1c7",
   "metadata": {},
   "source": [
    "### Importação de bibliotecas\n",
    "**Objetivo da Célula:** Importar todas as bibliotecas e funções necessárias para o processamento de dados, manipulação de datas e tratamento de erros.\n",
    "\n",
    "**Dependências:** Requer que os pacotes PySpark e octoops estejam instalados no ambiente.\n",
    "\n",
    "**Lógica Detalhada:**\n",
    "- Importa funções específicas do módulo `pyspark.sql.functions` para manipulação de colunas e expressões SQL.\n",
    "- Importa tipos de dados e classes para operações com DataFrames.\n",
    "- Importa funções para manipulação de datas e cálculos de diferenças.\n",
    "- Importa módulos Python padrão para logging e tratamento de exceções.\n",
    "- Importa as classes do pacote octoops para monitoramento e alertas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44ffb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, year, month, dayofmonth, when, lit, expr, to_timestamp\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import datediff, to_date\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import sys\n",
    "import traceback\n",
    "from octoops import OctoOps\n",
    "from octoops import Sentinel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e10a14e",
   "metadata": {},
   "source": [
    "### Configuração do logger\n",
    "**Objetivo da Célula:** Configurar um logger para registrar eventos, avisos e erros durante a execução do notebook.\n",
    "\n",
    "**Dependências:** Requer o módulo `logging` importado na célula anterior.\n",
    "\n",
    "**Variáveis/Objetos Criados/Modificados:** Cria a variável `logger` como uma instância do objeto Logger.\n",
    "\n",
    "**Lógica Detalhada:**\n",
    "- Utiliza a função `logging.getLogger(__name__)` para criar um logger associado ao nome do módulo atual.\n",
    "- O parâmetro `__name__` garante que o logger será específico para este notebook.\n",
    "- Este logger será usado posteriormente para registrar mensagens informativas, avisos e erros.\n",
    "\n",
    "**Saída/Impacto:** Cria e disponibiliza o objeto `logger` que será utilizado em outras funções do notebook para registrar informações de execução e erros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8558e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d86f4b",
   "metadata": {},
   "source": [
    "### Definição de tabelas e filtros\n",
    "**Objetivo da Célula:** Definir os nomes das tabelas de destino e construir os filtros SQL para processamento incremental e ativação.\n",
    "\n",
    "**Dependências:** Requer acesso ao catálogo Spark para verificação da existência de tabelas.\n",
    "\n",
    "**Variáveis/Objetos Criados/Modificados:** \n",
    "- `table_birads`: nome da tabela principal de destino\n",
    "- `table_birads_ativacao`: nome da tabela de destino para ativação\n",
    "- `where_clause`: filtro SQL para seleção incremental\n",
    "- `filtro_ativacao`: filtro SQL para regras de ativação\n",
    "\n",
    "**Lógica Detalhada:**\n",
    "1. Define os nomes das tabelas de destino no formato `schema.database.table`.\n",
    "2. Inicializa `where_clause` como string vazia.\n",
    "3. Verifica se a tabela principal já existe usando `spark.catalog.tableExists()`.\n",
    "4. Se existir, cria um filtro SQL que seleciona apenas registros mais recentes que o último datestamp da tabela.\n",
    "5. Define o filtro de ativação com regras de negócio específicas:\n",
    "   - Sem ficha já existente (`eleg.ficha IS NULL`)\n",
    "   - Apenas BI-RADS 1, 2 e 3\n",
    "   - Apenas exames específicos de mamografia\n",
    "   - Apenas pacientes do sexo feminino\n",
    "   - Apenas pacientes na faixa etária de 40 a 75 anos\n",
    "\n",
    "**Nomes de Tabelas/Colunas:**\n",
    "- `refined.saude_preventiva.fleury_laudos_mama_birads`\n",
    "- `refined.saude_preventiva.fleury_laudos_mama_birads_ativacao`\n",
    "- Colunas filtradas: `ficha`, `BIRADS`, `sigla_exame`, `sexo_cliente`, `idade_cliente`\n",
    "\n",
    "**Saída/Impacto:** Configura as variáveis com os nomes das tabelas e os filtros SQL que serão utilizados na consulta principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017de81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabela com todos os BIRADS extraídos dos laudos de mamografia\n",
    "table_birads = \"refined.saude_preventiva.fleury_laudos_mama_birads\"\n",
    "\n",
    "# Somente os laudos que estão dentro das regras de negócio para ativação (BIRADS 1, 2 e 3)\n",
    "table_birads_ativacao = \"refined.saude_preventiva.fleury_laudos_mama_birads_ativacao\"\n",
    "where_clause = \"\"\n",
    " \n",
    "# datestamp => data em que recebemos os dados na plataforma\n",
    "# Pegar da tabela de laudos\n",
    "if spark.catalog.tableExists(table_birads):\n",
    "    where_clause = f\"\"\"\n",
    "    WHERE\n",
    "        flr._datestamp >= (\n",
    "            SELECT MAX(brd._datestamp)\n",
    "            FROM {table_birads} brd\n",
    "        )\n",
    "    \"\"\"\n",
    "# Regra de negócio para ativação: BIRADS 1, 2 e 3, mulheres entre 40 e 75 anos, sem ficha ativa \n",
    "filtro_ativacao = \"\"\"\n",
    "    WHERE\n",
    "        eleg.ficha IS NULL\n",
    "        AND brd.BIRADS IN (1, 2, 3)\n",
    "        AND flr.sigla_exame IN ('MAMOG', 'MAMOGDIG', 'MAMOPROT', 'MAMOG3D')\n",
    "        AND UPPER(flr.sexo_cliente) = 'F'\n",
    "        AND (\n",
    "            idade_cliente >= 40 AND idade_cliente < 76\n",
    "        )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54d0f4e",
   "metadata": {},
   "source": [
    "### Consulta SQL principal para extração e processamento de BI-RADS\n",
    "**Objetivo da Célula:** Definir e executar a consulta SQL principal que extrai, limpa, classifica e enriquece os dados de laudos de mamografia.\n",
    "\n",
    "**Dependências:**\n",
    "- Variáveis `where_clause` e `filtro_ativacao` definidas anteriormente\n",
    "- Acesso às tabelas: \n",
    "  - `refined.saude_preventiva.fleury_laudos`\n",
    "  - `refined.saude_preventiva.fleury_retorno_elegivel_ficha`\n",
    "\n",
    "**Variáveis/Objetos Criados/Modificados:**\n",
    "- `query`: string da consulta SQL principal\n",
    "- `df_spk`: DataFrame com todos os resultados da consulta\n",
    "- `df_spk_ativacao`: DataFrame apenas com registros para ativação\n",
    "\n",
    "**Lógica Detalhada:**\n",
    "1. Define uma consulta SQL complexa com Common Table Expressions (CTEs):\n",
    "   - `base`: Extrai BI-RADS dos laudos usando expressões regulares avançadas e transformações\n",
    "   - `dados_birads`: Calcula valores mínimo, máximo e final do BI-RADS\n",
    "   - `dados_laudos`: Seleciona e transforma dados principais dos laudos\n",
    "2. A consulta principal:\n",
    "   - Combina as CTEs com JOIN\n",
    "   - Enriquece com dados de retorno elegível\n",
    "   - Aplica os filtros dinâmicos `where_clause` e `filtro_ativacao`\n",
    "3. Executa duas versões da consulta:\n",
    "   - Primeira com filtro incremental mas sem filtro de ativação\n",
    "   - Segunda sem filtro incremental mas com filtro de ativação\n",
    "\n",
    "**Transformações principais:**\n",
    "- Limpeza do texto com `REGEXP_REPLACE` removendo caracteres especiais\n",
    "- Extração dos valores BI-RADS com `REGEXP_EXTRACT_ALL` em duas etapas\n",
    "- Conversão de números romanos para decimais na função `TRANSFORM`\n",
    "- Cálculo da idade usando `TIMESTAMPDIFF`\n",
    "- Seleção dos valores mínimo/máximo de BI-RADS com funções de array\n",
    "\n",
    "**Saída/Impacto:** \n",
    "- Cria dois DataFrames Spark: \n",
    "  - `df_spk` com todos os laudos relevantes processados\n",
    "  - `df_spk_ativacao` apenas com os laudos que atendem critérios de ativação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de6adb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH base AS (\n",
    "    SELECT\n",
    "        flr.ficha,\n",
    "        flr.id_item,\n",
    "        flr.id_subitem,\n",
    "        REGEXP_EXTRACT_ALL(\n",
    "            REGEXP_EXTRACT(\n",
    "                REGEXP_REPLACE(UPPER(flr.laudo_tratado), r'[-:®]|\\xa0', ''),\n",
    "                r'(?mi)(AVALIA[CÇ][AÃ]O|CONCLUS[AÃ]O|IMPRESS[AÃ]O|OPINI[AÃ]O)?(.*)', 2\n",
    "            ),\n",
    "            r\"(?mi)(BIRADS|CATEGORIA|CAT\\W)\\s*(\\d+\\w*|VI|V|IV|III|II|I)\\W*\\w?(BIRADS|CATEGORIA)?(\\W|$)\", 2\n",
    "        ) AS RAW_BIRADS,\n",
    "        FILTER(\n",
    "            TRANSFORM(RAW_BIRADS, x ->\n",
    "                CASE\n",
    "                    WHEN x = \"I\" THEN 1\n",
    "                    WHEN x = \"II\" THEN 2\n",
    "                    WHEN x = \"III\" THEN 3\n",
    "                    WHEN x = \"IV\" THEN 4\n",
    "                    WHEN x = \"V\" THEN 5\n",
    "                    WHEN x = \"VI\" THEN 6\n",
    "                    WHEN TRY_CAST(x AS INT) > 6 THEN NULL\n",
    "                    ELSE REGEXP_REPLACE(x, r'[^0-9]', '')\n",
    "                END\n",
    "            ), x -> x IS NOT NULL\n",
    "        ) AS CAT_BIRADS\n",
    "    FROM refined.saude_preventiva.fleury_laudos flr\n",
    "    WHERE\n",
    "        flr.linha_cuidado = 'mama'\n",
    "        AND flr.sigla_exame IN ('MAMOG', 'MAMOGDIG', 'MAMOPROT', 'MAMOG3D')\n",
    "),\n",
    " \n",
    "dados_birads AS (\n",
    "    SELECT\n",
    "        *,\n",
    "        ARRAY_MIN(CAT_BIRADS) AS MIN_BIRADS,\n",
    "        ARRAY_MAX(CAT_BIRADS) AS MAX_BIRADS,\n",
    "        TRY_ELEMENT_AT(CAST(CAT_BIRADS AS ARRAY<INT>), -1) AS BIRADS\n",
    "    FROM base\n",
    "),\n",
    " \n",
    "dados_laudos AS (\n",
    "    SELECT\n",
    "        flr.linha_cuidado,\n",
    "        flr.id_unidade,\n",
    "        flr.id_ficha,\n",
    "        flr.id_item,\n",
    "        flr.id_subitem,\n",
    "        flr.ficha,\n",
    "        flr.id_exame,\n",
    "        flr.id_cliente,\n",
    "        flr.pefi_cliente,\n",
    "        flr.sigla_exame,\n",
    "        flr.id_marca,\n",
    "        flr.marca,\n",
    "        (\n",
    "          TIMESTAMPDIFF(DAY, flr.dth_nascimento_cliente, CURDATE()) / 365.25\n",
    "        ) AS idade_cliente,\n",
    "        flr.sexo_cliente,\n",
    "        flr.dth_pedido,\n",
    "        flr._datestamp\n",
    "    FROM refined.saude_preventiva.fleury_laudos flr\n",
    "    {where_clause}\n",
    ")\n",
    " \n",
    "SELECT\n",
    "    flr.* except(idade_cliente),\n",
    "    brd.MIN_BIRADS,\n",
    "    brd.MAX_BIRADS,\n",
    "    brd.BIRADS,\n",
    " \n",
    "    eleg.dth_pedido        AS dth_pedido_retorno_elegivel,\n",
    "    eleg.ficha             AS ficha_retorno_elegivel,\n",
    "    eleg.siglas_ficha      AS siglas_ficha_retorno_elegivel,\n",
    "    eleg.marca             AS marca_retorno_elegivel,\n",
    "    eleg.unidade           AS unidade_retorno_elegivel,\n",
    "    eleg.convenio          AS convenio_retorno_elegivel,\n",
    "    eleg.valores_exame     AS valores_exame_retorno_elegivel,\n",
    "    eleg.valores_ficha     AS valores_ficha_retorno_elegivel,\n",
    "    eleg.qtd_exame         AS qtd_exame_retorno_elegivel,\n",
    "    eleg.secao             AS secao_retorno_elegivel,\n",
    "    eleg.dias_entre_ficha  AS dias_entre_ficha_elegivel\n",
    "FROM dados_laudos flr\n",
    "INNER JOIN dados_birads brd\n",
    "    ON flr.ficha = brd.ficha\n",
    "    AND flr.id_item = brd.id_item\n",
    "    AND flr.id_subitem = brd.id_subitem\n",
    "LEFT JOIN refined.saude_preventiva.fleury_retorno_elegivel_ficha eleg\n",
    "    ON eleg.ficha_origem = flr.ficha\n",
    "    AND eleg.id_cliente = flr.id_cliente\n",
    "    AND eleg.linha_cuidado = flr.linha_cuidado\n",
    "{filtro_ativacao}\n",
    "\"\"\"\n",
    "\n",
    "# Executa a consulta SQL com o filtro de _datestamp e sem o filtro de ativação\n",
    "df_spk = spark.sql(query.format(\n",
    "    where_clause = where_clause,\n",
    "    filtro_ativacao = \"\"\n",
    "    )\n",
    ")\n",
    " \n",
    "# Executa a consulta SQL sem o filtro de _datestamp e com o filtro de ativação \n",
    "df_spk_ativacao = spark.sql(query.format(\n",
    "    where_clause = \"\",\n",
    "    filtro_ativacao = filtro_ativacao\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630fbaf5",
   "metadata": {},
   "source": [
    "### Função para calcular data prevista de retorno\n",
    "**Objetivo da Célula:** Definir uma função que calcula a data prevista de retorno com base na classificação BI-RADS.\n",
    "\n",
    "**Dependências:**\n",
    "- Funções PySpark: `col`, `when`, `expr`\n",
    "- Tipo de dados: `DataFrame`\n",
    "\n",
    "**Função Criada:**\n",
    "- `calcular_data_prevista(df_spk: DataFrame) -> DataFrame`\n",
    "\n",
    "**Lógica Detalhada:**\n",
    "1. A função recebe um DataFrame Spark como entrada.\n",
    "2. Utiliza a função `withColumn` para adicionar uma nova coluna `dth_previsao_retorno`.\n",
    "3. Aplica lógica condicional com `when` para diferentes valores de BI-RADS:\n",
    "   - Para BI-RADS 1 ou 2: adiciona 360 dias à data de pedido\n",
    "   - Para BI-RADS 3: adiciona 180 dias à data de pedido\n",
    "   - Para outros valores: define como None (nulo)\n",
    "4. Usa a função `expr(\"date_add(dth_pedido, N)\")` para realizar o cálculo de datas.\n",
    "5. Retorna o DataFrame com a nova coluna adicionada.\n",
    "\n",
    "**Parâmetros:**\n",
    "- `df_spk (DataFrame)`: O DataFrame Spark contendo os dados de entrada, que deve ter as colunas `BIRADS` e `dth_pedido`.\n",
    "\n",
    "**Retorno:**\n",
    "- `DataFrame`: O mesmo DataFrame de entrada, mas com a coluna adicional `dth_previsao_retorno`.\n",
    "\n",
    "**Impacto:** Esta função implementa uma regra de negócio crucial para determinar quando o paciente deve retornar para um novo exame com base na classificação BI-RADS atual, afetando diretamente o fluxo de acompanhamento clínico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcc73a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_data_prevista(df_spk: DataFrame):\n",
    "    \"\"\"\n",
    "    Adiciona uma coluna 'dth_previsao_retorno' ao DataFrame com base na coluna 'BIRADS'.\n",
    "\n",
    "    - Para BIRADS 1 ou 2, adiciona 360 dias à data da coluna 'dth_pedido'.\n",
    "    - Para BIRADS 3, adiciona 180 dias à data da coluna 'dth_pedido'.\n",
    "    - Para outros valores de BIRADS, define 'dth_previsao_retorno' como None.\n",
    "\n",
    "    Parâmetros:\n",
    "    df_spk (DataFrame): O DataFrame Spark contendo os dados de entrada.\n",
    "\n",
    "    Retorna:\n",
    "    DataFrame: O DataFrame atualizado com a nova coluna 'dth_previsao_retorno'.\n",
    "    \"\"\"\n",
    "    df_spk = df_spk.withColumn(\n",
    "        'dth_previsao_retorno',\n",
    "        when(\n",
    "            col('BIRADS').isin([1, 2]),\n",
    "            expr(\"date_add(dth_pedido, 360)\")\n",
    "        ).when(\n",
    "            col('BIRADS') == 3,\n",
    "            expr(\"date_add(dth_pedido, 180)\")  \n",
    "        ).otherwise(None)\n",
    "    )\n",
    "    return df_spk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d506c0c4",
   "metadata": {},
   "source": [
    "### Função para transformar campos do DataFrame\n",
    "**Objetivo da Célula:** Definir uma função que aplica múltiplas transformações aos DataFrames para enriquecimento dos dados com base na classificação BI-RADS.\n",
    "\n",
    "**Dependências:**\n",
    "- Função `calcular_data_prevista` definida anteriormente\n",
    "- Funções PySpark: `col`, `when`, `to_timestamp`, `datediff`, `to_date`\n",
    "- Objeto `logger` para registro de avisos\n",
    "\n",
    "**Função Criada:**\n",
    "- `transform_fields(df_spk: DataFrame) -> DataFrame`\n",
    "\n",
    "**Lógica Detalhada:**\n",
    "1. Verifica se o DataFrame está vazio usando `isEmpty()`:\n",
    "   - Se vazio, registra aviso e retorna o DataFrame sem alterações\n",
    "2. Adiciona coluna `retorno_cliente` com a periodicidade em meses:\n",
    "   - Para BI-RADS 1 ou 2: 12 meses\n",
    "   - Para BI-RADS 3: 6 meses\n",
    "   - Para outros valores: 0\n",
    "3. Chama a função `calcular_data_prevista` para adicionar a coluna de data prevista\n",
    "4. Converte duas colunas de data para o tipo timestamp:\n",
    "   - `dth_pedido_retorno_elegivel`\n",
    "   - `dth_previsao_retorno`\n",
    "5. Calcula a diferença em dias entre a data prevista e a data do pedido na coluna `dias_ate_retorno`\n",
    "\n",
    "**Parâmetros:**\n",
    "- `df_spk (DataFrame)`: O DataFrame a ser transformado\n",
    "\n",
    "**Retorno:**\n",
    "- `DataFrame`: O DataFrame com todas as transformações aplicadas\n",
    "\n",
    "**Colunas Adicionadas/Modificadas:**\n",
    "- `retorno_cliente`: número de meses para retorno\n",
    "- `dth_previsao_retorno`: data calculada para retorno\n",
    "- `dias_ate_retorno`: número de dias até a data de retorno\n",
    "- Conversões de tipo nas colunas de data\n",
    "\n",
    "**Impacto:** Esta função enriquece o DataFrame com informações cruciais para o agendamento e acompanhamento de pacientes, calculando prazos e convertendo dados para formatos apropriados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7e85ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_fields(df_spk: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transforma os campos do DataFrame fornecido.\n",
    "\n",
    "    - Verifica se o DataFrame está vazio. Se estiver, registra um aviso e retorna o DataFrame sem alterações.\n",
    "    - Adiciona uma coluna 'retorno_cliente' com valores baseados na coluna 'BIRADS':\n",
    "      - 12 meses para BIRADS 1 ou 2\n",
    "      - 6 meses para BIRADS 3\n",
    "      - 0 para outros valores\n",
    "    - Calcula a data prevista de retorno usando a função 'calcular_data_prevista'.\n",
    "    - Converte a coluna 'dth_pedido_retorno_elegivel' para o tipo timestamp.\n",
    "    - Converte a coluna 'dth_previsao_retorno' para o tipo timestamp.\n",
    "    - Calcula a diferença em dias entre 'dth_previsao_retorno' e 'dth_pedido', armazenando o resultado na coluna 'dias_ate_retorno'.\n",
    "\n",
    "    Parâmetros:\n",
    "    df_spk (DataFrame): O DataFrame a ser transformado.\n",
    "\n",
    "    Retorna:\n",
    "    DataFrame: O DataFrame transformado com as novas colunas.\n",
    "    \"\"\"\n",
    "    if df_spk.isEmpty():\n",
    "        logger.warning(\"No Data Found!\")\n",
    "        return df_spk\n",
    "    \n",
    "    df_spk = df_spk.withColumn(\n",
    "        'retorno_cliente',\n",
    "        when(col('BIRADS').isin([1, 2]), 12).when(col('BIRADS') == 3, 6).otherwise(0)\n",
    "    )\n",
    "\n",
    "    df_spk = calcular_data_prevista(df_spk)\n",
    "    df_spk = df_spk.withColumn('dth_pedido_retorno_elegivel', to_timestamp(col('dth_pedido_retorno_elegivel')))\n",
    "    df_spk = df_spk.withColumn('dth_previsao_retorno', to_timestamp(col('dth_previsao_retorno')))\n",
    "    df_spk = df_spk.withColumn('dias_ate_retorno', datediff(to_date(col('dth_previsao_retorno')), to_date(col('dth_pedido'))))\n",
    "    return df_spk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104f7198",
   "metadata": {},
   "source": [
    "### Configuração da função de tratamento de erros e alerta\n",
    "**Objetivo da Célula:** Definir uma variável de ambiente e uma função para tratamento de erros com envio de alertas ao sistema Sentinel.\n",
    "\n",
    "**Dependências:**\n",
    "- Módulos: `traceback`\n",
    "- Classes: `Sentinel` do pacote octoops\n",
    "\n",
    "**Variáveis/Funções Criadas:**\n",
    "- `WEBHOOK_DS_AI_BUSINESS_STG`: constante que define o ambiente ('prd')\n",
    "- `error_message(e)`: função para tratamento de exceções\n",
    "\n",
    "**Lógica Detalhada:**\n",
    "1. Define a constante `WEBHOOK_DS_AI_BUSINESS_STG` com valor 'prd' (produção)\n",
    "2. A função `error_message` recebe uma exceção `e` e:\n",
    "   - Formata o traceback completo da exceção usando `traceback.format_exc()`\n",
    "   - Constrói uma mensagem de erro formatada\n",
    "   - Cria uma instância da classe `Sentinel` configurada para o projeto 'Monitor_Linhas_Cuidado_Mama'\n",
    "   - Envia um alerta ao Sentinel usando `alerta_sentinela()`\n",
    "   - Imprime o traceback no console usando `traceback.print_exc()`\n",
    "   - Relança a exceção original para propagar o erro\n",
    "\n",
    "**Parâmetros da Função:**\n",
    "- `e (Exception)`: A exceção capturada que será processada e relatada\n",
    "\n",
    "**Impacto:** Esta configuração implementa um mecanismo robusto de tratamento de erros, garantindo que qualquer falha durante a persistência dos dados será:\n",
    "1. Registrada em log com detalhes completos\n",
    "2. Notificada ao sistema de monitoramento (Sentinel)\n",
    "3. Propagada adequadamente para tratamento em níveis superiores\n",
    "\n",
    "Isso é crucial para manter a qualidade e confiabilidade do pipeline de dados, facilitando o diagnóstico e resposta rápida a problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd7a9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEBHOOK_DS_AI_BUSINESS_STG = 'prd'\n",
    "\n",
    "def error_message(e):\n",
    "    \"\"\"\n",
    "    Envia alerta para o Sentinel e exibe o traceback em caso de erro ao salvar dados.\n",
    "\n",
    "    Parâmetros:\n",
    "        e (Exception): Exceção capturada.\n",
    "\n",
    "    Comportamento:\n",
    "        - Formata o traceback do erro.\n",
    "        - Envia alerta para o Sentinel com detalhes do erro.\n",
    "        - Exibe o traceback no console.\n",
    "        - Relança a exceção.\n",
    "    \"\"\"\n",
    "    error_message = traceback.format_exc()\n",
    "    summary_message = f\"\"\"Erro ao salvar dados.\\n{error_message}\"\"\"\n",
    "    sentinela_ds_ai_business = Sentinel(\n",
    "        project_name='Monitor_Linhas_Cuidado_Mama',\n",
    "        env_type=WEBHOOK_DS_AI_BUSINESS_STG,\n",
    "        task_title='Fleury Mamografia'\n",
    "    )\n",
    "    sentinela_ds_ai_business.alerta_sentinela(\n",
    "        categoria='Alerta', \n",
    "        mensagem=summary_message,\n",
    "        job_id_descritivo='1_fleury_mama_birads'\n",
    "    )\n",
    "    traceback.print_exc()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48310f0",
   "metadata": {},
   "source": [
    "### Funções de persistência de dados em tabelas Delta\n",
    "**Objetivo da Célula:** Definir três funções para persistência de dados em tabelas Delta, com suporte a inserção, merge e decisão inteligente entre essas operações.\n",
    "\n",
    "**Dependências:**\n",
    "- Função `error_message` definida anteriormente\n",
    "- Objeto `logger` para registros\n",
    "- API Spark SQL para execução de operações SQL\n",
    "\n",
    "**Funções Criadas:**\n",
    "1. `insert_data(df_spk, table_name, serializable=False)`: Insere dados em uma tabela Delta, sobrescrevendo dados existentes\n",
    "2. `merge_data(df_spk, table_name)`: Realiza um merge (upsert) em uma tabela Delta existente\n",
    "3. `save_data(df_spk, table_name, serializable=False)`: Função principal que decide entre insert ou merge\n",
    "\n",
    "**Lógica Detalhada:**\n",
    "\n",
    "**1. `insert_data`:**\n",
    "   - Registra o início da operação com `logger.info`\n",
    "   - Define o nível de isolamento da transação conforme parâmetro `serializable`\n",
    "   - Utiliza a API de escrita do Spark para salvar o DataFrame:\n",
    "     - Formato 'delta'\n",
    "     - Permite sobrescrita do schema\n",
    "     - Define o modo de isolamento da transação\n",
    "     - Usa modo 'overwrite' para substituir dados\n",
    "   - Captura exceções e chama `error_message` em caso de erro\n",
    "\n",
    "**2. `merge_data`:**\n",
    "   - Registra o início da operação com `logger.info`\n",
    "   - Cria uma view temporária com os dados do DataFrame\n",
    "   - Executa uma operação MERGE SQL:\n",
    "     - Combina dados com base nas chaves (ficha, id_item, id_subitem)\n",
    "     - Atualiza registros existentes\n",
    "     - Insere novos registros\n",
    "   - Captura exceções e chama `error_message` em caso de erro\n",
    "\n",
    "**3. `save_data`:**\n",
    "   - Verifica se o DataFrame está vazio - se estiver, retorna sem fazer nada\n",
    "   - Verifica se a tabela já existe no catálogo Spark\n",
    "   - Se existir, chama `merge_data` para fazer upsert\n",
    "   - Se não existir, chama `insert_data` para criar uma nova tabela\n",
    "\n",
    "**Impacto:** Este conjunto de funções implementa uma estratégia robusta de persistência de dados que:\n",
    "1. Evita processamento desnecessário para DataFrames vazios\n",
    "2. Decide automaticamente entre criação de tabela ou atualização incremental\n",
    "3. Gerencia corretamente o isolamento das transações\n",
    "4. Fornece tratamento de exceções e alertas em caso de falha\n",
    "5. Mantém registros de log para rastreabilidade das operações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdea2dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data(df_spk: DataFrame, table_name: str, serializable: bool = False):\n",
    "    \"\"\"\n",
    "    Insere os dados do DataFrame em uma tabela Delta no Databricks.\n",
    "    \n",
    "    Parâmetros:\n",
    "        df_spk (DataFrame): DataFrame Spark a ser salvo.\n",
    "        table_name (str): Nome da tabela destino.\n",
    "        serializable (bool): Define o nível de isolamento da transação Delta. \n",
    "            Se True, usa 'Serializable', caso contrário 'WriteSerializable'.\n",
    "    \n",
    "    Comportamento:\n",
    "        - Escreve o DataFrame na tabela Delta especificada, sobrescrevendo dados existentes.\n",
    "        - Atualiza o schema da tabela conforme necessário.\n",
    "        - Define o nível de isolamento da transação Delta.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Inserting Data: {table_name}\")\n",
    "        isolation_level = 'Serializable' if serializable else 'WriteSerializable'\n",
    "        (\n",
    "            df_spk.write\n",
    "                .format('delta')\n",
    "                .option('overwriteSchema', 'true')\n",
    "                .option('delta.isolationLevel', isolation_level)\n",
    "                .mode('overwrite')\n",
    "                .saveAsTable(table_name)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        error_message(e)\n",
    " \n",
    "def merge_data(df_spk: DataFrame, table_name: str):\n",
    "    \"\"\"\n",
    "    Realiza um merge (upsert) dos dados do DataFrame em uma tabela Delta existente.\n",
    "    \n",
    "    Parâmetros:\n",
    "        df_spk (DataFrame): DataFrame Spark a ser mesclado.\n",
    "        table_name (str): Nome da tabela destino.\n",
    "    \n",
    "    Comportamento:\n",
    "        - Cria uma view temporária com os dados do DataFrame.\n",
    "        - Executa um comando MERGE SQL para atualizar registros existentes (com base em ficha, id_item e id_subitem)\n",
    "          ou inserir novos registros na tabela Delta.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Merging Data: {table_name}\")\n",
    "        df_spk.createOrReplaceTempView(f\"increment_birads\")\n",
    "        merge_query = f\"\"\"\n",
    "            MERGE INTO {table_name} AS target\n",
    "            USING increment_birads AS source\n",
    "                ON target.ficha = source.ficha\n",
    "                AND target.id_item = source.id_item\n",
    "                AND target.id_subitem = source.id_subitem\n",
    "            WHEN MATCHED THEN\n",
    "                UPDATE SET *\n",
    "            WHEN NOT MATCHED THEN\n",
    "                INSERT *\n",
    "        \"\"\"\n",
    "        spark.sql(merge_query)\n",
    "    except Exception as e:\n",
    "        error_message(e)\n",
    " \n",
    "def save_data(df_spk: DataFrame, table_name: str, serializable: bool = False):\n",
    "    \"\"\"\n",
    "    Salva os dados do DataFrame em uma tabela Delta, realizando merge se a tabela já existir.\n",
    "    \n",
    "    Parâmetros:\n",
    "        df_spk (DataFrame): DataFrame Spark a ser salvo.\n",
    "        table_name (str): Nome da tabela destino.\n",
    "        serializable (bool): Define o nível de isolamento da transação Delta na inserção inicial.\n",
    "    \n",
    "    Comportamento:\n",
    "        - Se o DataFrame estiver vazio, não faz nada.\n",
    "        - Se a tabela já existe, realiza merge (upsert) dos dados.\n",
    "        - Se a tabela não existe, insere os dados criando a tabela.\n",
    "    \"\"\"\n",
    "    if df_spk.isEmpty():\n",
    "        return None\n",
    " \n",
    "    if spark.catalog.tableExists(table_name):\n",
    "        merge_data(df_spk, table_name)\n",
    "    else:\n",
    "        insert_data(df_spk, table_name, serializable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4859c142",
   "metadata": {},
   "source": [
    "### Aplicação das transformações aos DataFrames\n",
    "**Objetivo da Célula:** Aplicar as transformações definidas anteriormente aos dois DataFrames extraídos da consulta SQL.\n",
    "\n",
    "**Dependências:**\n",
    "- Função `transform_fields` definida anteriormente\n",
    "- DataFrames `df_spk` e `df_spk_ativacao` criados pela consulta SQL\n",
    "\n",
    "**Variáveis/Objetos Modificados:**\n",
    "- `df_spk`: DataFrame completo com todos os laudos\n",
    "- `df_spk_ativacao`: DataFrame com apenas laudos elegíveis para ativação\n",
    "\n",
    "**Lógica Detalhada:**\n",
    "- Chama a função `transform_fields()` para cada DataFrame\n",
    "- A função aplica as seguintes transformações (conforme definida anteriormente):\n",
    "  1. Adiciona coluna `retorno_cliente` (12 meses para BI-RADS 1/2, 6 meses para BI-RADS 3)\n",
    "  2. Calcula data prevista de retorno (360 dias para BI-RADS 1/2, 180 dias para BI-RADS 3)\n",
    "  3. Converte colunas de datas para timestamp\n",
    "  4. Calcula diferença em dias entre datas\n",
    "\n",
    "**Saída/Impacto:** \n",
    "- Ambos os DataFrames são enriquecidos com as colunas calculadas necessárias para análise e persistência\n",
    "- Os DataFrames mantêm seus filtros específicos:\n",
    "  - `df_spk`: todos os registros, possivelmente filtrados por datestamp\n",
    "  - `df_spk_ativacao`: apenas registros elegíveis para ativação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320e50b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar transformações nos dfs\n",
    "df_spk = transform_fields(df_spk)\n",
    "df_spk_ativacao = transform_fields(df_spk_ativacao)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633e902a",
   "metadata": {},
   "source": [
    "### Remoção de duplicados na lista de ativação\n",
    "**Objetivo da Célula:** Remover registros duplicados do DataFrame de ativação, garantindo que exista apenas um registro por ficha.\n",
    "\n",
    "**Motivação:** \n",
    "- Conforme comentado no código, o objetivo é enviar apenas 1 push (notificação) por ficha\n",
    "- Se não removesse os duplicados, poderia haver múltiplas notificações para o mesmo atendimento quando há vários exames relacionados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b226a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluir duplicados para considerar apenas a ficha na ativação e não os exames (itens). Assim vamos enviar apenas \n",
    "# 1 push por ficha\n",
    "df_spk_ativacao = df_spk_ativacao.dropDuplicates(['ficha'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b01cfa0",
   "metadata": {},
   "source": [
    "### Impressão da quantidade de registros\n",
    "**Objetivo da Célula:** Exibir a quantidade de registros em cada DataFrame para monitoramento do volume de dados processados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03449c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('quantidade de laudos salvos na tabela',df_spk.count())\n",
    "print('quantidade de laudos salvos na tabela de ativação', df_spk_ativacao.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acc875b",
   "metadata": {},
   "source": [
    "### Persistência dos dados nas tabelas Delta\n",
    "**Objetivo da Célula:** Salvar os dados processados nas tabelas Delta correspondentes, utilizando as funções de persistência apropriadas.\n",
    "\n",
    "**Dependências:**\n",
    "- Funções `save_data` e `insert_data` definidas anteriormente\n",
    "- DataFrames `df_spk` e `df_spk_ativacao` completamente processados\n",
    "- Variáveis `table_birads` e `table_birads_ativacao` com nomes das tabelas\n",
    "\n",
    "**Operações Executadas:**\n",
    "1. Para o DataFrame principal `df_spk`:\n",
    "   - Utiliza `save_data()`, que decide entre merge ou insert baseado na existência da tabela\n",
    "   - Salva na tabela definida em `table_birads`\n",
    "\n",
    "2. Para o DataFrame de ativação `df_spk_ativacao`:\n",
    "   - Utiliza `insert_data()` diretamente, que sempre sobrescreve os dados existentes\n",
    "   - Salva na tabela definida em `table_birads_ativacao`\n",
    "\n",
    "**Lógica da Diferença de Abordagem:**\n",
    "- Para dados completos (`df_spk`): usa abordagem incremental (merge) se a tabela já existir\n",
    "- Para dados de ativação (`df_spk_ativacao`): sempre sobrescreve, pois representa o estado atual das ativações necessárias\n",
    "\n",
    "**Comentários no Código:**\n",
    "- O primeiro comentário explica a lógica do `save_data` (verifica se DataFrame está vazio, decide entre merge/insert)\n",
    "- O segundo comentário explica a lógica do `insert_data` (sobrescreve dados, atualiza schema, define isolamento)\n",
    "\n",
    "**Saída/Impacto:**\n",
    "- Os dados processados são persistidos nas tabelas Delta correspondentes\n",
    "- As tabelas ficam disponíveis para consulta por outros notebooks e aplicações\n",
    "- Em caso de erro, o mecanismo de alerta do Sentinel será acionado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975e40bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Se o DataFrame estiver vazio, não faz nada.\n",
    "# - Se a tabela já existe, realiza merge (upsert) dos dados.\n",
    "# - Se a tabela não existe, insere os dados criando a tabela.\n",
    "save_data(df_spk, table_birads)\n",
    "\n",
    "# - Escreve o DataFrame na tabela Delta especificada, sobrescrevendo dados existentes.\n",
    "# - Atualiza o schema da tabela conforme necessário.\n",
    "# - Define o nível de isolamento da transação Delta.\n",
    "insert_data(df_spk_ativacao, table_birads_ativacao)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
