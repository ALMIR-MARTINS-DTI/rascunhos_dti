{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "151f8c64",
   "metadata": {},
   "source": [
    "# Documentação Técnica do Notebook: Extração e Processamento de Dados BI-RADS - Pardini\n",
    "\n",
    "## Objetivo Principal\n",
    "**Este notebook realiza a extração, transformação e persistência dos dados de laudos de mamografia do Pardini, com foco na classificação BI-RADS.** O objetivo é identificar e classificar exames de mama segundo o BI-RADS, além de gerar uma lista de ativação para notificação de pacientes com exames em atraso.\n",
    "\n",
    "## Tecnologias Utilizadas\n",
    "- **PySpark**: Manipulação de dados, consultas SQL, transformações e persistência em Delta Lake.\n",
    "- **Octoops**: Monitoramento, alertas e integração com sistemas de controle de execução.\n",
    "- **Delta Lake**: Persistência dos dados processados.\n",
    "- **Python (logging, traceback, etc.)**: Controle de fluxo, tratamento de erros e registro de eventos.\n",
    "\n",
    "## Fluxo de Trabalho/Etapas Principais\n",
    "1. Instalação de dependências (`octoops`).\n",
    "2. Configuração do ambiente (reinicialização do kernel e importação de bibliotecas).\n",
    "3. Definição de tabelas e filtros SQL para seleção incremental e ativação de pacientes.\n",
    "4. Consulta SQL principal para extração e enriquecimento dos dados dos laudos.\n",
    "5. Transformações nos DataFrames para cálculo de datas previstas de retorno, classificação e cálculo de diferenças de datas.\n",
    "6. Persistência dos dados nas tabelas Delta, com tratamento de erros e envio de alertas.\n",
    "7. Remoção de duplicados na lista de ativação.\n",
    "8. Impressão da quantidade de registros salvos.\n",
    "\n",
    "## Dados Envolvidos\n",
    "- **Fonte**: Tabela `refined.saude_preventiva.pardini_laudos`.\n",
    "- **Tabelas de destino**: `refined.saude_preventiva.pardini_laudos_mama_birads` e `refined.saude_preventiva.pardini_laudos_mama_birads_ativacao`.\n",
    "- **Colunas importantes**: `laudo_tratado`, `BIRADS`, `MIN_BIRADS`, `MAX_BIRADS`, `dth_pedido`, `dth_previsao_retorno`, `dias_ate_retorno`, entre outras.\n",
    "\n",
    "## Resultados/Saídas Esperadas\n",
    "- DataFrame completo com laudos processados e classificados segundo BI-RADS.\n",
    "- DataFrame filtrado para ativação de pacientes elegíveis para notificação.\n",
    "- Persistência dos dados nas tabelas Delta.\n",
    "- Alertas automáticos em caso de falhas ou ausência de dados.\n",
    "\n",
    "## Pré-requisitos\n",
    "- Ambiente Databricks ou Spark configurado.\n",
    "- Pacotes: `octoops`, permissões de escrita nas tabelas Delta.\n",
    "- Acesso às tabelas Delta e permissões de escrita.\n",
    "\n",
    "## Considerações Importantes\n",
    "- O notebook utiliza expressões regulares avançadas para correta identificação dos valores BI-RADS.\n",
    "- O merge/upsert garante atualização incremental dos dados sem duplicidade.\n",
    "- O monitoramento via Octoops/Sentinel facilita rastreabilidade e resposta rápida a falhas.\n",
    "- O filtro de idade e sexo garante que apenas pacientes do público-alvo sejam analisados.\n",
    "\n",
    "---\n",
    "A seguir, cada célula de código é precedida por uma explicação técnica detalhada sobre sua função e impacto no fluxo do notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0845ca",
   "metadata": {},
   "source": [
    "## Instalação do pacote octoops\n",
    "Esta célula instala o pacote `octoops`, utilizado para monitoramento, alertas e integração com sistemas de controle de execução. O comando `%pip install` garante que o pacote esteja disponível no ambiente do notebook. Caso o ambiente já possua o pacote, a instalação será ignorada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7df524",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install octoops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379b4981",
   "metadata": {},
   "source": [
    "## Reinicialização do ambiente Python (Databricks)\n",
    "Esta célula reinicia o kernel Python no ambiente Databricks, garantindo que todas as dependências recém-instaladas estejam disponíveis. O comando `dbutils.library.restartPython()` é específico do Databricks e não deve ser executado em ambientes fora dele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faf8931",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca61c9b",
   "metadata": {},
   "source": [
    "## Importação de bibliotecas\n",
    "Importa todas as bibliotecas necessárias para o processamento dos dados:\n",
    "- **PySpark**: Manipulação de DataFrames, funções SQL, tipos de dados e cálculos de datas.\n",
    "- **Octoops**: Classes para monitoramento (`OctoOps`, `Sentinel`).\n",
    "- **Logging, traceback, sys**: Controle de fluxo, registro de eventos e tratamento de erros.\n",
    "Essas bibliotecas são essenciais para todas as etapas do notebook, desde a extração até a persistência e monitoramento dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e19e71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, year, month, dayofmonth, when, lit, expr, to_timestamp\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import datediff, to_date\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import sys\n",
    "import traceback\n",
    "from octoops import OctoOps\n",
    "from octoops import Sentinel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb4c7ba",
   "metadata": {},
   "source": [
    "## Configuração do logger\n",
    "Cria um logger para registrar informações, avisos e erros durante a execução do notebook. O logger é utilizado nas funções de transformação e persistência para facilitar o monitoramento e depuração."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa0d6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c69aef",
   "metadata": {},
   "source": [
    "## Definição de tabelas e filtros\n",
    "Define os nomes das tabelas Delta de destino e os filtros SQL utilizados para seleção incremental dos dados e ativação de pacientes. O filtro incremental garante que apenas novos dados sejam processados, enquanto o filtro de ativação seleciona pacientes elegíveis para notificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2190a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_birads = \"refined.saude_preventiva.pardini_laudos_mama_birads\"\n",
    "table_birads_ativacao = \"refined.saude_preventiva.pardini_laudos_mama_birads_ativacao\"\n",
    "where_clause = \"\"\n",
    " \n",
    "if spark.catalog.tableExists(table_birads):\n",
    "    where_clause = f\"\"\"\n",
    "    WHERE        \n",
    "        flr._datestamp > (\n",
    "            SELECT MAX(brd._datestamp)\n",
    "            FROM {table_birads} brd\n",
    "        )\n",
    "        \n",
    "    \"\"\"\n",
    " \n",
    "filtro_ativacao = \"\"\"\n",
    "    WHERE\n",
    "        eleg.ficha IS NULL\n",
    "        AND brd.BIRADS IN (1, 2, 3)\n",
    "        AND flr.sigla_exame IN ('MAMODI','MAMO')\n",
    "        AND UPPER(flr.sexo_cliente) = 'F'\n",
    "        AND (\n",
    "            idade_cliente >= 40 AND idade_cliente < 76\n",
    "        )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b53870e",
   "metadata": {},
   "source": [
    "## Consulta SQL principal\n",
    "Esta célula define e executa a consulta SQL que extrai, processa e enriquece os dados dos laudos de mamografia. O SQL realiza:\n",
    "- Limpeza e padronização do texto dos laudos;\n",
    "- Extração dos valores BI-RADS (inclusive algarismos romanos);\n",
    "- Cálculo de idade do paciente;\n",
    "- Enriquecimento com dados de retorno elegível;\n",
    "- Aplicação dos filtros incremental e de ativação.\n",
    "Os resultados são carregados em dois DataFrames: um completo e outro filtrado para ativação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be103879",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH base AS (\n",
    "    SELECT\n",
    "        flr.ficha,\n",
    "        flr.sigla_exame, \n",
    "        flr.id_marca,\n",
    "        flr.sequencial,\n",
    "        flr.laudo_tratado,\n",
    "        REGEXP_EXTRACT_ALL(\n",
    "            REGEXP_EXTRACT(\n",
    "                REGEXP_REPLACE(UPPER(flr.laudo_tratado), r'[-:®]|\\xa0', ''),\n",
    "                r'(?mi)(AVALIA[CÇ][AÃ]O|CONCLUS[AÃ]O|IMPRESS[AÃ]O|OPINI[AÃ]O)?(.*)', 2\n",
    "            ),\n",
    "            r\"(?mi)(BIRADS|CATEGORI[AO]|CATEGORA|CATEGORIA R|CAT\\W)\\s*(\\d+\\w*|VI|V|IV|III|II|I)\\W*\\w?(BIRADS|CATEGORI[AO]|CATEGORA|CATEGORIA R)?(\\W|$)\", 2\n",
    "        )        \n",
    "        AS RAW_BIRADS,\n",
    "        FILTER(\n",
    "            TRANSFORM(RAW_BIRADS, x ->\n",
    "                CASE\n",
    "                    WHEN x = \"I\" THEN 1\n",
    "                    WHEN x = \"II\" THEN 2\n",
    "                    WHEN x = \"III\" THEN 3\n",
    "                    WHEN x = \"IV\" THEN 4\n",
    "                    WHEN x = \"V\" THEN 5\n",
    "                    WHEN x = \"VI\" THEN 6\n",
    "                    WHEN TRY_CAST(x AS INT) > 6 THEN NULL\n",
    "                    ELSE REGEXP_REPLACE(x, r'[^0-9]', '')\n",
    "                END\n",
    "            ), x -> x IS NOT NULL\n",
    "        ) AS CAT_BIRADS\n",
    "    FROM refined.saude_preventiva.pardini_laudos flr\n",
    "    WHERE\n",
    "        flr.linha_cuidado = 'mama'\n",
    "        AND UPPER(flr.sexo_cliente) = 'F' \n",
    "        AND flr.sigla_exame IN ('MAMO','MAMODI')\n",
    "),\n",
    " \n",
    "dados_birads AS (\n",
    "    SELECT\n",
    "        *,\n",
    "        ARRAY_MIN(CAT_BIRADS) AS MIN_BIRADS,\n",
    "        ARRAY_MAX(CAT_BIRADS) AS MAX_BIRADS,\n",
    "        TRY_ELEMENT_AT(CAST(CAT_BIRADS AS ARRAY<INT>), -1) AS BIRADS\n",
    "    FROM base\n",
    "),\n",
    " \n",
    "dados_laudos AS (\n",
    "    SELECT\n",
    "        flr.linha_cuidado,\n",
    "        flr.id_unidade,\n",
    "        flr.id_ficha,\n",
    "        flr.sigla_exame, \n",
    "        flr.id_marca,\n",
    "        flr.sequencial,\n",
    "        flr.ficha,\n",
    "        flr.id_cliente,\n",
    "        flr.pefi_cliente,\n",
    "        flr.sigla_exame,\n",
    "        flr.marca,\n",
    "        flr.laudo_tratado,\n",
    "        (\n",
    "          TIMESTAMPDIFF(DAY, flr.dth_nascimento_cliente, CURDATE()) / 365.25\n",
    "        ) AS idade_cliente,\n",
    "        flr.sexo_cliente,\n",
    "        flr.dth_pedido,\n",
    "        flr._datestamp\n",
    "    FROM refined.saude_preventiva.pardini_laudos flr\n",
    "    {where_clause}\n",
    ")\n",
    " \n",
    "SELECT\n",
    "    flr.* except(idade_cliente),\n",
    "    brd.MIN_BIRADS,\n",
    "    brd.MAX_BIRADS,\n",
    "    brd.BIRADS,\n",
    " \n",
    "    eleg.dth_pedido        AS dth_pedido_retorno_elegivel,\n",
    "    eleg.ficha             AS ficha_retorno_elegivel,\n",
    "    eleg.siglas_ficha      AS siglas_ficha_retorno_elegivel,\n",
    "    eleg.marca             AS marca_retorno_elegivel,\n",
    "    eleg.unidade           AS unidade_retorno_elegivel,\n",
    "    eleg.convenio          AS convenio_retorno_elegivel,\n",
    "    eleg.valores_exame     AS valores_exame_retorno_elegivel,\n",
    "    eleg.valores_ficha     AS valores_ficha_retorno_elegivel,\n",
    "    eleg.qtd_exame         AS qtd_exame_retorno_elegivel,\n",
    "    eleg.secao             AS secao_retorno_elegivel,\n",
    "    eleg.dias_entre_ficha  AS dias_entre_ficha_elegivel\n",
    "\n",
    "FROM dados_laudos flr\n",
    "INNER JOIN dados_birads brd\n",
    "    ON flr.ficha = brd.ficha\n",
    "    AND flr.id_exame = brd.id_exame\n",
    "    AND flr.id_marca = brd.id_marca\n",
    "    AND flr.sequencial = brd.sequencial    \n",
    "LEFT JOIN refined.saude_preventiva.pardini_retorno_elegivel_ficha eleg\n",
    "    ON eleg.ficha_origem = flr.ficha\n",
    "    AND eleg.id_cliente = flr.id_cliente\n",
    "    AND eleg.linha_cuidado = flr.linha_cuidado\n",
    "{filtro_ativacao}\n",
    "\"\"\"\n",
    "\n",
    "df_spk = spark.sql(query.format(where_clause=where_clause, filtro_ativacao=\"\"))\n",
    "\n",
    "df_spk_ativacao = spark.sql(\n",
    "    query.format(where_clause=\"\", filtro_ativacao=filtro_ativacao)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8544e5",
   "metadata": {},
   "source": [
    "## Função para calcular data prevista de retorno\n",
    "Esta função adiciona a coluna `dth_previsao_retorno` ao DataFrame, baseada no valor de BI-RADS. Para BI-RADS 1 ou 2, adiciona 360 dias à data do pedido; para BI-RADS 3, adiciona 180 dias; para outros valores, retorna None. Utiliza funções do PySpark para manipulação de datas e lógica condicional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6e7b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_data_prevista(df_spk: DataFrame):\n",
    "    \"\"\"\n",
    "    Adiciona uma coluna 'dth_previsao_retorno' ao DataFrame com base na coluna 'BIRADS'.\n",
    "\n",
    "    - Para BIRADS 1 ou 2, adiciona 360 dias à data da coluna 'dth_pedido'.\n",
    "    - Para BIRADS 3, adiciona 180 dias à data da coluna 'dth_pedido'.\n",
    "    - Para outros valores de BIRADS, define 'dth_previsao_retorno' como None.\n",
    "\n",
    "    Parâmetros:\n",
    "    df_spk (DataFrame): O DataFrame Spark contendo os dados de entrada.\n",
    "\n",
    "    Retorna:\n",
    "    DataFrame: O DataFrame atualizado com a nova coluna 'dth_previsao_retorno'.\n",
    "    \"\"\"\n",
    "    df_spk = df_spk.withColumn(\n",
    "        'dth_previsao_retorno',\n",
    "        when(\n",
    "            col('BIRADS').isin([1, 2]),\n",
    "            expr(\"date_add(dth_pedido, 360)\")\n",
    "        ).when(\n",
    "            col('BIRADS') == 3,\n",
    "            expr(\"date_add(dth_pedido, 180)\")  \n",
    "        ).otherwise(None)\n",
    "    )\n",
    "    return df_spk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7b1c20",
   "metadata": {},
   "source": [
    "## Função para transformar campos do DataFrame\n",
    "Esta função aplica diversas transformações ao DataFrame:\n",
    "- Verifica se o DataFrame está vazio e registra aviso;\n",
    "- Adiciona coluna `retorno_cliente` com valores baseados em BI-RADS (12 meses para 1/2, 6 meses para 3, 0 para outros);\n",
    "- Calcula a data prevista de retorno usando a função anterior;\n",
    "- Converte colunas de datas para timestamp;\n",
    "- Calcula a diferença em dias entre a data prevista de retorno e a data do pedido.\n",
    "Essas transformações facilitam a análise e o envio de notificações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4285dc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_fields(df_spk: DataFrame) -> DataFrame:\n",
    "  \"\"\"\n",
    "  Transforma os campos do DataFrame fornecido.\n",
    "\n",
    "  - Verifica se o DataFrame está vazio. Se estiver, registra um aviso e retorna o DataFrame sem alterações.\n",
    "  - Adiciona uma coluna 'retorno_cliente' com valores baseados na coluna 'BIRADS':\n",
    "    - 12 meses para BIRADS 1 ou 2\n",
    "    - 6 meses para BIRADS 3\n",
    "    - 0 para outros valores\n",
    "  - Calcula a data prevista de retorno usando a função 'calcular_data_prevista'.\n",
    "  - Converte a coluna 'dth_pedido_retorno_elegivel' para o tipo timestamp.\n",
    "  - Converte a coluna 'dth_previsao_retorno' para o tipo timestamp.\n",
    "  - Calcula a diferença em dias entre 'dth_previsao_retorno' e 'dth_pedido', armazenando o resultado na coluna 'dias_ate_retorno'.\n",
    "\n",
    "  Parâmetros:\n",
    "  df_spk (DataFrame): O DataFrame a ser transformado.\n",
    "\n",
    "  Retorna:\n",
    "  DataFrame: O DataFrame transformado com as novas colunas.\n",
    "  \"\"\"\n",
    "\n",
    "  if df_spk.isEmpty():\n",
    "      logger.warning(\"No Data Found!\")\n",
    "      return df_spk\n",
    "\n",
    "  df_spk = df_spk.withColumn(\n",
    "      'retorno_cliente',\n",
    "      when(col('BIRADS').isin([1, 2]), 12).when(col('BIRADS') == 3, 6).otherwise(0)\n",
    "  )\n",
    "\n",
    "  df_spk = calcular_data_prevista(df_spk)\n",
    "  df_spk = df_spk.withColumn('dth_pedido_retorno_elegivel', to_timestamp(col('dth_pedido_retorno_elegivel')))\n",
    "  df_spk = df_spk.withColumn('dth_previsao_retorno', to_timestamp(col('dth_previsao_retorno')))\n",
    "  df_spk = df_spk.withColumn('dias_ate_retorno', datediff(to_date(col('dth_previsao_retorno')), to_date(col('dth_pedido'))))\n",
    "  return df_spk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badc8d50",
   "metadata": {},
   "source": [
    "## Função para tratamento de erros e alerta\n",
    "Esta função trata exceções durante a persistência dos dados. Em caso de erro, formata o traceback, envia alerta para o Sentinel (monitoramento), imprime o erro no console e relança a exceção. Isso garante rastreabilidade e monitoramento automático de falhas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3540d798",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEBHOOK_DS_AI_BUSINESS_STG = 'prd'\n",
    "\n",
    "def error_message(e):\n",
    "    \"\"\"\n",
    "    Envia alerta para o Sentinel e exibe o traceback em caso de erro ao salvar dados.\n",
    "\n",
    "    Parâmetros:\n",
    "        e (Exception): Exceção capturada.\n",
    "\n",
    "    Comportamento:\n",
    "        - Formata o traceback do erro.\n",
    "        - Envia alerta para o Sentinel com detalhes do erro.\n",
    "        - Exibe o traceback no console.\n",
    "        - Relança a exceção.\n",
    "    \"\"\"\n",
    "    error_message = traceback.format_exc()\n",
    "    summary_message = f\"\"\"Erro ao salvar dados.\\n{error_message}\"\"\"\n",
    "    sentinela_ds_ai_business = Sentinel(\n",
    "        project_name='Monitor_Linhas_Cuidado_Mama',\n",
    "        env_type=WEBHOOK_DS_AI_BUSINESS_STG,\n",
    "        task_title='Pardini Mama'\n",
    "    )\n",
    "    sentinela_ds_ai_business.alerta_sentinela(\n",
    "        categoria='Alerta', \n",
    "        mensagem=summary_message,\n",
    "        job_id_descritivo='1_pardini_mama_birads'\n",
    "    )\n",
    "    traceback.print_exc()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcb4aa4",
   "metadata": {},
   "source": [
    "## Funções para salvar dados nas tabelas Delta\n",
    "Estas funções realizam a persistência dos dados processados:\n",
    "- `insert_data`: Insere o DataFrame na tabela Delta, sobrescrevendo dados existentes e atualizando o schema;\n",
    "- `merge_data`: Realiza merge (upsert) dos dados, atualizando registros existentes e inserindo novos, usando SQL;\n",
    "- `save_data`: Decide entre insert ou merge conforme existência da tabela e verifica se o DataFrame está vazio.\n",
    "Todas as funções utilizam tratamento de erros e logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c546f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data(df_spk: DataFrame, table_name: str):\n",
    "    \"\"\"\n",
    "    Insere os dados do DataFrame na tabela Delta especificada, sobrescrevendo o conteúdo existente.\n",
    "    \n",
    "    Parâmetros:\n",
    "        df_spk (DataFrame): DataFrame a ser salvo.\n",
    "        table_name (str): Nome da tabela Delta de destino.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Inserting Data: {table_name}\")\n",
    "        (\n",
    "            df_spk.write\n",
    "                .mode('overwrite')\n",
    "                .option('mergeSchema','true')\n",
    "                .option('overwriteSchema','true')\n",
    "                .format('delta')\n",
    "                .partitionBy('_datestamp')\n",
    "                .saveAsTable(table_name)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        error_message(e)\n",
    " \n",
    "def merge_data(df_spk: DataFrame, table_name: str):\n",
    "    \"\"\"\n",
    "    Realiza merge dos dados do DataFrame na tabela Delta, atualizando registros existentes e inserindo novos.\n",
    "    \n",
    "    Parâmetros:\n",
    "        df_spk (DataFrame): DataFrame com os dados incrementais.\n",
    "        table_name (str): Nome da tabela Delta de destino.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Merging Data: {table_name}\")\n",
    "        df_spk.createOrReplaceTempView(f\"increment_birads\")\n",
    "        merge_query = f\"\"\"\n",
    "            MERGE INTO {table_name} AS target\n",
    "            USING increment_birads AS source\n",
    "                ON target.ficha = source.ficha\n",
    "                AND target.sequencial = source.sequencial\n",
    "                AND target.sigla_exame = source.sigla_exame\n",
    "            WHEN MATCHED THEN\n",
    "                UPDATE SET *\n",
    "            WHEN NOT MATCHED THEN\n",
    "                INSERT *\n",
    "        \"\"\"\n",
    "        spark.sql(merge_query)\n",
    "    except Exception as e:\n",
    "        error_message(e)\n",
    " \n",
    "def save_data(df_spk: DataFrame, table_name: str):\n",
    "    \"\"\"\n",
    "    Salva os dados do DataFrame na tabela Delta, utilizando merge se a tabela existir ou insert se não existir.\n",
    "    \n",
    "    Parâmetros:\n",
    "        df_spk (DataFrame): DataFrame a ser salvo.\n",
    "        table_name (str): Nome da tabela Delta de destino.\n",
    "    \"\"\"\n",
    "    if df_spk.isEmpty():\n",
    "        return None\n",
    " \n",
    "    if spark.catalog.tableExists(table_name):\n",
    "        merge_data(df_spk, table_name)\n",
    "    else:\n",
    "        insert_data(df_spk, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537addb3",
   "metadata": {},
   "source": [
    "## Aplicação das transformações nos DataFrames\n",
    "Aplica as funções de transformação nos DataFrames extraídos das consultas SQL. Isso garante que todos os campos necessários estejam presentes e corretamente formatados para persistência e análise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1981944",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spk = transform_fields(df_spk)\n",
    "df_spk_ativacao = transform_fields(df_spk_ativacao)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb05776",
   "metadata": {},
   "source": [
    "## Remoção de duplicados na lista de ativação\n",
    "Remove duplicados do DataFrame de ativação, considerando apenas a ficha para notificação. Isso garante que cada paciente receba apenas uma notificação por ficha, evitando notificações duplicadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e74f43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluir duplicados para considerar apenas a ficha na ativação e não os exames (itens). Assim vamos enviar apenas \n",
    "# 1 push por ficha\n",
    "df_spk_ativacao = df_spk_ativacao.dropDuplicates(['ficha'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c5ed92",
   "metadata": {},
   "source": [
    "## Impressão da quantidade de laudos salvos\n",
    "Imprime a quantidade de laudos salvos nas tabelas principais e de ativação, permitindo o acompanhamento do volume de dados processados e persistidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f81baff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('quantidade de laudos salvos na tabela',df_spk.count())\n",
    "print('quantidade de laudos salvos na tabela de ativação', df_spk_ativacao.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd22e66",
   "metadata": {},
   "source": [
    "## Salvamento dos dados nas tabelas Delta\n",
    "Salva os dados processados nas tabelas Delta, utilizando merge ou insert conforme necessário. O DataFrame completo é salvo na tabela principal e o DataFrame de ativação na tabela de ativação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eccab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(df_spk, table_birads)\n",
    "insert_data(df_spk_ativacao, table_birads_ativacao)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
