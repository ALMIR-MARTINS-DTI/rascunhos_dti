{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extração de dado - BI_RADS\n",
    "\n",
    "## Objetivo\n",
    "- Extrair BI-RADIS 0,1,2,3,4,5,6 e sem birads.\n",
    "- Salvar **lista de ativação**: considera apenas BI_RADS 1, 2 e 3 e tem com objetivo identificar pacientes com exames em atraso para envio de notificação.\n",
    "\n",
    "## Resumo do Código SQL/PySpark\n",
    "\n",
    "Este código SQL, executado via PySpark (`spark.sql`), tem como objetivo principal processar dados de laudos médicos de mamografia para extrair, padronizar e enriquecer informações relacionadas à classificação BIRADS, combinando-as com outros dados do paciente e informações de retorno elegível.\n",
    "\n",
    "### Tópicos Principais:\n",
    "\n",
    "#### Tabela e filtros:\n",
    "*   A consulta começa selecionando dados de uma tabela de laudos médicos (`refined.saude_preventiva.fleury_laudos`), que contém informações detalhadas sobre exames e enriququece com dados de retorno da tabela `refined.saude_preventiva.fleury_retorno_elegivel_ficha`.\n",
    "*   Coluna com informações de BIRADS: `laudo_tratado` (texto do laudo médico).\n",
    "*   Filtros:\n",
    "    *   linha_cuidado = 'mama'\n",
    "    *   sigla_exame IN ('MAMOG', 'MAMOGDIG', 'MAMOPROT', 'MAMOG3D')\n",
    "    *   **where_clause:** _datestamp (`refined.saude_preventiva.fleury_laudos`) >= _datestamp (`refined.saude_preventiva.fleury_laudos_mama_birads`)\n",
    "    *   **filtro_ativacao:**\n",
    "        *    eleg.ficha IS NULL\n",
    "        *    brd.BIRADS IN (1, 2, 3)\n",
    "        *    flr.sigla_exame IN ('MAMOG', 'MAMOGDIG', 'MAMOPROT', 'MAMOG3D')\n",
    "        *    UPPER(flr.sexo_cliente) = 'F'\n",
    "        *    idade_cliente >= 40 AND idade_cliente < 76\n",
    "\n",
    "#### Extração e Padronização BIRADS (`base` CTE):\n",
    "*   **Limpeza de Texto:** Remove caracteres indesejados (`-`, `:`, `®`, `\\xa0`) e converte o texto do laudo para maiúsculas.\n",
    "*   **Extração de Conteúdo:** Utiliza `REGEXP_EXTRACT` para isolar seções relevantes do laudo (Avaliação, Conclusão, Impressão, Opinião).\n",
    "*   **Extração de BIRADS Bruto:** Aplica `REGEXP_EXTRACT_ALL` para encontrar todos os valores BIRADS (numéricos ou romanos como I-VI) dentro do texto extraído.\n",
    "*   **Categorização BIRADS (`CAT_BIRADS`):** Transforma os valores BIRADS brutos em inteiros padronizados (1 a 6), tratando algarismos romanos e filtrando valores inválidos.\n",
    "\n",
    "#### Análise de BIRADS Extraído (`dados_birads` CTE):\n",
    "*   Calcula o valor **mínimo (`MIN_BIRADS`)** e **máximo (`MAX_BIRADS`)** das categorias BIRADS encontradas em um laudo.\n",
    "*   Identifica o **último valor BIRADS (`BIRADS`)** presente no array de categorias.\n",
    "\n",
    "*   **Seleção de Dados de Laudos (`dados_laudos` CTE):**\n",
    "*   Seleciona diversas colunas de identificação e características dos laudos (`linha_cuidado`, `id_unidade`, `ficha`, `sigla_exame`, etc.).\n",
    "*   Calcula a **idade do cliente (`idade_cliente`)** com base na data de nascimento e na data atual.\n",
    "*   Inclui um placeholder `{where_clause}` para filtros dinâmicos na seleção dos laudos.\n",
    "\n",
    "#### Consulta Principal (`SELECT` final):\n",
    "*   **Junção de Dados:** Realiza um `INNER JOIN` entre os dados dos laudos (`dados_laudos`) e as informações BIRADS processadas (`dados_birads`) usando `ficha`, `id_item` e `id_subitem` como chaves.\n",
    "*   **Enriquecimento com Retorno Elegível:** Executa um `LEFT JOIN` com a tabela `refined.saude_preventiva.fleury_retorno_elegivel_ficha` para adicionar detalhes sobre possíveis retornos ou acompanhamentos elegíveis para o paciente.\n",
    "*   **Seleção de Colunas:** Seleciona todas as colunas dos laudos (exceto `idade_cliente`), as colunas BIRADS (`MIN_BIRADS`, `MAX_BIRADS`, `BIRADS`), e as colunas de retorno elegível.\n",
    "*   Inclui um placeholder `{filtro_ativacao}` para filtros dinâmicos adicionais.\n",
    "\n",
    "#### Fluxo de Execução do Notebook\n",
    "\n",
    "O notebook segue o seguinte fluxo de processamento:\n",
    "\n",
    "1.  **Definição de Variáveis e Filtros:** `table_birads`, `table_birads_ativacao`, `where_clause` (com lógica incremental) e `filtro_ativacao` são definidos.\n",
    "2.  **Execução da Query SQL Principal:**\n",
    "    *   A `query` SQL é executada duas vezes usando `spark.sql`:\n",
    "        *   `df_spk`: Gerado com a `where_clause` incremental e `filtro_ativacao` vazio. Contém todos os laudos de mamografia com BIRADS extraídos e enriquecidos.\n",
    "        *   `df_spk_ativacao`: Gerado com `where_clause` vazio e o `filtro_ativacao` específico para a lista de ativação.\n",
    "3.  **Transformações nos DataFrames:**\n",
    "    *   A função `transform_fields` é aplicada a `df_spk` e `df_spk_ativacao`. Esta função adiciona colunas como `retorno_cliente`, `dth_previsao_retorno` e `dias_ate_retorno`.\n",
    "4.  **Desduplicação da Lista de Ativação:**\n",
    "    *   `df_spk_ativacao` é desduplicado pela coluna `ficha` (`dropDuplicates(['ficha'])`) para garantir uma única entrada por paciente para fins de notificação.\n",
    "5.  **Persistência dos Dados:**\n",
    "    *   `save_data(df_spk, table_birads)`: Salva o DataFrame completo de BIRADS na tabela `table_birads`, realizando um `MERGE` se a tabela já existir (para atualizações incrementais) ou um `INSERT` se for a primeira vez.\n",
    "    *   `insert_data(df_spk_ativacao, table_birads_ativacao)`: Salva a lista de ativação na tabela `table_birads_ativacao`, sempre sobrescrevendo o conteúdo existente para refletir a lista mais recente de pacientes elegíveis para notificação.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://almir_martins:****@gitlab.com/api/v4/projects/65902035/packages/pypi/simple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement octoops (from versions: none)\n",
      "ERROR: No matching distribution found for octoops\n"
     ]
    }
   ],
   "source": [
    "!pip install --extra-index-url https://almir_martins:glpat-s01_R5xw79N_6syRa5Tz8G86MQp1OmhweWc0Cw.01.12091y3ha@gitlab.com/api/v4/projects/65902035/packages/pypi/simple octoops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3456005e-c606-4d3f-961c-8009da6ff502",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Package(s) not found: octoops\n"
     ]
    }
   ],
   "source": [
    "pip show octoops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "443392e1-a616-4b95-9e99-7fb8daac06e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement octoops (from versions: none)\n",
      "ERROR: No matching distribution found for octoops\n"
     ]
    }
   ],
   "source": [
    "%pip install octoops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf58106b-ed7c-4ad5-b071-83b943a5e122",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement octoops==0.21.0 (from versions: none)\n",
      "ERROR: No matching distribution found for octoops==0.21.0\n"
     ]
    }
   ],
   "source": [
    "%pip install octoops==0.21.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71b057cc-5838-47e8-8147-73f10be895eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d548738c-990c-4538-af60-737df18cb213",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, year, month, dayofmonth, when, lit, expr, to_timestamp\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import datediff, to_date\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import sys\n",
    "import traceback\n",
    "from octoops import OctoOps\n",
    "from octoops import Sentinel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9eab1447-b3d0-4352-a153-b0f3b4904c89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d172fa1-4531-4c67-9074-958f531553ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tabela com todos os BIRADS extraídos dos laudos de mamografia\n",
    "table_birads = \"refined.saude_preventiva.fleury_laudos_mama_birads\"\n",
    "\n",
    "# Somente os laudos que estão dentro das regras de negócio para ativação (BIRADS 1, 2 e 3)\n",
    "table_birads_ativacao = \"refined.saude_preventiva.fleury_laudos_mama_birads_ativacao\"\n",
    "where_clause = \"\"\n",
    " \n",
    "# datestamp => data em que recebemos os dados na plataforma\n",
    "# Pegar da tabela de laudos\n",
    "if spark.catalog.tableExists(table_birads):\n",
    "    where_clause = f\"\"\"\n",
    "    WHERE\n",
    "        flr._datestamp >= (\n",
    "            SELECT MAX(brd._datestamp)\n",
    "            FROM {table_birads} brd\n",
    "        )\n",
    "    \"\"\"\n",
    "# Regra de negócio para ativação: BIRADS 1, 2 e 3, mulheres entre 40 e 75 anos, sem ficha ativa \n",
    "filtro_ativacao = \"\"\"\n",
    "    WHERE\n",
    "        eleg.ficha IS NULL\n",
    "        AND brd.BIRADS IN (1, 2, 3)\n",
    "        AND flr.sigla_exame IN ('MAMOG', 'MAMOGDIG', 'MAMOPROT', 'MAMOG3D')\n",
    "        AND UPPER(flr.sexo_cliente) = 'F'\n",
    "        AND (\n",
    "            idade_cliente >= 40 AND idade_cliente < 76\n",
    "        )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtro de ativação - Estevão\n",
    "\n",
    "Inserir as alterações abaixo na variável `filtro_ativacao` do código SQL principal.\n",
    "\n",
    "**Filtro Estevão na íntegra:**\n",
    "```python\n",
    "AND flr.ficha_retorno_elegivel IS NULL\n",
    "AND atv.BIRADS IN (1, 2, 3)\n",
    "AND UPPER(flr.marca) ILIKE ANY(\n",
    "    'AMAIS - BA',\n",
    "    'AMAIS - PE',\n",
    "    'AMAIS - SP',\n",
    "    'Felippe Mattoso',\n",
    "    'FLEURY',\n",
    "    'IR',\n",
    "    'LABS AMAIS'\n",
    ")\n",
    "AND (\n",
    "    TIMESTAMPDIFF(DAY, flr.dth_nascimento_cliente, CURDATE()) / 365.25 >= 40\n",
    "    AND TIMESTAMPDIFF(DAY, flr.dth_nascimento_cliente, CURDATE()) / 365.25 < 76\n",
    ")\n",
    "AND UPPER(flr.sexo_cliente) = 'F\n",
    "```\n",
    "\n",
    "**Filtro Estevão editado:**\n",
    "```python\n",
    "AND UPPER(flr.marca) ILIKE ANY(\n",
    "    'AMAIS - BA',\n",
    "    'AMAIS - PE',\n",
    "    'AMAIS - SP',\n",
    "    'Felippe Mattoso',\n",
    "    'FLEURY',\n",
    "    'IR',\n",
    "    'LABS AMAIS'\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11cd5d41-3663-40b4-86a8-5508e0e12604",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH base AS (\n",
    "    SELECT\n",
    "        flr.ficha,\n",
    "        flr.id_item,\n",
    "        flr.id_subitem,\n",
    "        REGEXP_EXTRACT_ALL(\n",
    "            REGEXP_EXTRACT(\n",
    "                REGEXP_REPLACE(UPPER(flr.laudo_tratado), r'[-:®]|\\xa0', ''),\n",
    "                r'(?mi)(AVALIA[CÇ][AÃ]O|CONCLUS[AÃ]O|IMPRESS[AÃ]O|OPINI[AÃ]O)?(.*)', 2\n",
    "            ),\n",
    "            r\"(?mi)(BIRADS|CATEGORIA|CAT\\W)\\s*(\\d+\\w*|VI|V|IV|III|II|I)\\W*\\w?(BIRADS|CATEGORIA)?(\\W|$)\", 2\n",
    "        ) AS RAW_BIRADS,\n",
    "        FILTER(\n",
    "            TRANSFORM(RAW_BIRADS, x ->\n",
    "                CASE\n",
    "                    WHEN x = \"I\" THEN 1\n",
    "                    WHEN x = \"II\" THEN 2\n",
    "                    WHEN x = \"III\" THEN 3\n",
    "                    WHEN x = \"IV\" THEN 4\n",
    "                    WHEN x = \"V\" THEN 5\n",
    "                    WHEN x = \"VI\" THEN 6\n",
    "                    WHEN TRY_CAST(x AS INT) > 6 THEN NULL\n",
    "                    ELSE REGEXP_REPLACE(x, r'[^0-9]', '')\n",
    "                END\n",
    "            ), x -> x IS NOT NULL\n",
    "        ) AS CAT_BIRADS\n",
    "    FROM refined.saude_preventiva.fleury_laudos flr\n",
    "    WHERE\n",
    "        flr.linha_cuidado = 'mama'\n",
    "        AND flr.sigla_exame IN ('MAMOG', 'MAMOGDIG', 'MAMOPROT', 'MAMOG3D')\n",
    "),\n",
    " \n",
    "dados_birads AS (\n",
    "    SELECT\n",
    "        *,\n",
    "        ARRAY_MIN(CAT_BIRADS) AS MIN_BIRADS,\n",
    "        ARRAY_MAX(CAT_BIRADS) AS MAX_BIRADS,\n",
    "        TRY_ELEMENT_AT(CAST(CAT_BIRADS AS ARRAY<INT>), -1) AS BIRADS\n",
    "    FROM base\n",
    "),\n",
    " \n",
    "dados_laudos AS (\n",
    "    SELECT\n",
    "        flr.linha_cuidado,\n",
    "        flr.id_unidade,\n",
    "        flr.id_ficha,\n",
    "        flr.id_item,\n",
    "        flr.id_subitem,\n",
    "        flr.ficha,\n",
    "        flr.id_exame,\n",
    "        flr.id_cliente,\n",
    "        flr.pefi_cliente,\n",
    "        flr.sigla_exame,\n",
    "        flr.id_marca,\n",
    "        flr.marca,\n",
    "        (\n",
    "          TIMESTAMPDIFF(DAY, flr.dth_nascimento_cliente, CURDATE()) / 365.25\n",
    "        ) AS idade_cliente,\n",
    "        flr.sexo_cliente,\n",
    "        flr.dth_pedido,\n",
    "        flr._datestamp\n",
    "    FROM refined.saude_preventiva.fleury_laudos flr\n",
    "    {where_clause}\n",
    ")\n",
    " \n",
    "SELECT\n",
    "    flr.* except(idade_cliente),\n",
    "    brd.MIN_BIRADS,\n",
    "    brd.MAX_BIRADS,\n",
    "    brd.BIRADS,\n",
    " \n",
    "    eleg.dth_pedido        AS dth_pedido_retorno_elegivel,\n",
    "    eleg.ficha             AS ficha_retorno_elegivel,\n",
    "    eleg.siglas_ficha      AS siglas_ficha_retorno_elegivel,\n",
    "    eleg.marca             AS marca_retorno_elegivel,\n",
    "    eleg.unidade           AS unidade_retorno_elegivel,\n",
    "    eleg.convenio          AS convenio_retorno_elegivel,\n",
    "    eleg.valores_exame     AS valores_exame_retorno_elegivel,\n",
    "    eleg.valores_ficha     AS valores_ficha_retorno_elegivel,\n",
    "    eleg.qtd_exame         AS qtd_exame_retorno_elegivel,\n",
    "    eleg.secao             AS secao_retorno_elegivel,\n",
    "    eleg.dias_entre_ficha  AS dias_entre_ficha_elegivel\n",
    "FROM dados_laudos flr\n",
    "INNER JOIN dados_birads brd\n",
    "    ON flr.ficha = brd.ficha\n",
    "    AND flr.id_item = brd.id_item\n",
    "    AND flr.id_subitem = brd.id_subitem\n",
    "LEFT JOIN refined.saude_preventiva.fleury_retorno_elegivel_ficha eleg\n",
    "    ON eleg.ficha_origem = flr.ficha\n",
    "    AND eleg.id_cliente = flr.id_cliente\n",
    "    AND eleg.linha_cuidado = flr.linha_cuidado\n",
    "{filtro_ativacao}\n",
    "\"\"\"\n",
    "\n",
    "# Executa a consulta SQL com o filtro de _datestamp e sem o filtro de ativação\n",
    "df_spk = spark.sql(query.format(\n",
    "    where_clause = where_clause,\n",
    "    filtro_ativacao = \"\"\n",
    "    )\n",
    ")\n",
    " \n",
    "# Executa a consulta SQL sem o filtro de _datestamp e com o filtro de ativação \n",
    "df_spk_ativacao = spark.sql(query.format(\n",
    "    where_clause = \"\",\n",
    "    filtro_ativacao = filtro_ativacao\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f429161-4558-44b9-be53-f75e42cb1e94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def calcular_data_prevista(df_spk: DataFrame):\n",
    "    \"\"\"\n",
    "    Adiciona uma coluna 'dth_previsao_retorno' ao DataFrame com base na coluna 'BIRADS'.\n",
    "\n",
    "    - Para BIRADS 1 ou 2, adiciona 360 dias à data da coluna 'dth_pedido'.\n",
    "    - Para BIRADS 3, adiciona 180 dias à data da coluna 'dth_pedido'.\n",
    "    - Para outros valores de BIRADS, define 'dth_previsao_retorno' como None.\n",
    "\n",
    "    Parâmetros:\n",
    "    df_spk (DataFrame): O DataFrame Spark contendo os dados de entrada.\n",
    "\n",
    "    Retorna:\n",
    "    DataFrame: O DataFrame atualizado com a nova coluna 'dth_previsao_retorno'.\n",
    "    \"\"\"\n",
    "    df_spk = df_spk.withColumn(\n",
    "        'dth_previsao_retorno',\n",
    "        when(\n",
    "            col('BIRADS').isin([1, 2]),\n",
    "            expr(\"date_add(dth_pedido, 360)\")\n",
    "        ).when(\n",
    "            col('BIRADS') == 3,\n",
    "            expr(\"date_add(dth_pedido, 180)\")  \n",
    "        ).otherwise(None)\n",
    "    )\n",
    "    return df_spk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5e20f90-617d-4e12-918d-8d7490e2728d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def transform_fields(df_spk: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transforma os campos do DataFrame fornecido.\n",
    "\n",
    "    - Verifica se o DataFrame está vazio. Se estiver, registra um aviso e retorna o DataFrame sem alterações.\n",
    "    - Adiciona uma coluna 'retorno_cliente' com valores baseados na coluna 'BIRADS':\n",
    "      - 12 meses para BIRADS 1 ou 2\n",
    "      - 6 meses para BIRADS 3\n",
    "      - 0 para outros valores\n",
    "    - Calcula a data prevista de retorno usando a função 'calcular_data_prevista'.\n",
    "    - Converte a coluna 'dth_pedido_retorno_elegivel' para o tipo timestamp.\n",
    "    - Converte a coluna 'dth_previsao_retorno' para o tipo timestamp.\n",
    "    - Calcula a diferença em dias entre 'dth_previsao_retorno' e 'dth_pedido', armazenando o resultado na coluna 'dias_ate_retorno'.\n",
    "\n",
    "    Parâmetros:\n",
    "    df_spk (DataFrame): O DataFrame a ser transformado.\n",
    "\n",
    "    Retorna:\n",
    "    DataFrame: O DataFrame transformado com as novas colunas.\n",
    "    \"\"\"\n",
    "    if df_spk.isEmpty():\n",
    "        logger.warning(\"No Data Found!\")\n",
    "        return df_spk\n",
    "    \n",
    "    df_spk = df_spk.withColumn(\n",
    "        'retorno_cliente',\n",
    "        when(col('BIRADS').isin([1, 2]), 12).when(col('BIRADS') == 3, 6).otherwise(0)\n",
    "    )\n",
    "\n",
    "    df_spk = calcular_data_prevista(df_spk)\n",
    "    df_spk = df_spk.withColumn('dth_pedido_retorno_elegivel', to_timestamp(col('dth_pedido_retorno_elegivel')))\n",
    "    df_spk = df_spk.withColumn('dth_previsao_retorno', to_timestamp(col('dth_previsao_retorno')))\n",
    "    df_spk = df_spk.withColumn('dias_ate_retorno', datediff(to_date(col('dth_previsao_retorno')), to_date(col('dth_pedido'))))\n",
    "    return df_spk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f978ef9-3fb2-4a21-86f5-96b14a0ba3c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "WEBHOOK_DS_AI_BUSINESS_STG = 'prd'\n",
    "\n",
    "def error_message(e):\n",
    "    \"\"\"\n",
    "    Envia alerta para o Sentinel e exibe o traceback em caso de erro ao salvar dados.\n",
    "\n",
    "    Parâmetros:\n",
    "        e (Exception): Exceção capturada.\n",
    "\n",
    "    Comportamento:\n",
    "        - Formata o traceback do erro.\n",
    "        - Envia alerta para o Sentinel com detalhes do erro.\n",
    "        - Exibe o traceback no console.\n",
    "        - Relança a exceção.\n",
    "    \"\"\"\n",
    "    error_message = traceback.format_exc()\n",
    "    summary_message = f\"\"\"Erro ao salvar dados.\\n{error_message}\"\"\"\n",
    "    sentinela_ds_ai_business = Sentinel(\n",
    "        project_name='Monitor_Linhas_Cuidado_Mama',\n",
    "        env_type=WEBHOOK_DS_AI_BUSINESS_STG,\n",
    "        task_title='Fleury Mamografia'\n",
    "    )\n",
    "    sentinela_ds_ai_business.alerta_sentinela(\n",
    "        categoria='Alerta', \n",
    "        mensagem=summary_message,\n",
    "        job_id_descritivo='1_fleury_mama_birads'\n",
    "    )\n",
    "    traceback.print_exc()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "120cbf21-552e-4add-a91a-d11607657c67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def insert_data(df_spk: DataFrame, table_name: str, serializable: bool = False):\n",
    "    \"\"\"\n",
    "    Insere os dados do DataFrame em uma tabela Delta no Databricks.\n",
    "    \n",
    "    Parâmetros:\n",
    "        df_spk (DataFrame): DataFrame Spark a ser salvo.\n",
    "        table_name (str): Nome da tabela destino.\n",
    "        serializable (bool): Define o nível de isolamento da transação Delta. \n",
    "            Se True, usa 'Serializable', caso contrário 'WriteSerializable'.\n",
    "    \n",
    "    Comportamento:\n",
    "        - Escreve o DataFrame na tabela Delta especificada, sobrescrevendo dados existentes.\n",
    "        - Atualiza o schema da tabela conforme necessário.\n",
    "        - Define o nível de isolamento da transação Delta.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Inserting Data: {table_name}\")\n",
    "        isolation_level = 'Serializable' if serializable else 'WriteSerializable'\n",
    "        (\n",
    "            df_spk.write\n",
    "                .format('delta')\n",
    "                .option('overwriteSchema', 'true')\n",
    "                .option('delta.isolationLevel', isolation_level)\n",
    "                .mode('overwrite')\n",
    "                .saveAsTable(table_name)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        error_message(e)\n",
    " \n",
    "def merge_data(df_spk: DataFrame, table_name: str):\n",
    "    \"\"\"\n",
    "    Realiza um merge (upsert) dos dados do DataFrame em uma tabela Delta existente.\n",
    "    \n",
    "    Parâmetros:\n",
    "        df_spk (DataFrame): DataFrame Spark a ser mesclado.\n",
    "        table_name (str): Nome da tabela destino.\n",
    "    \n",
    "    Comportamento:\n",
    "        - Cria uma view temporária com os dados do DataFrame.\n",
    "        - Executa um comando MERGE SQL para atualizar registros existentes (com base em ficha, id_item e id_subitem)\n",
    "          ou inserir novos registros na tabela Delta.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Merging Data: {table_name}\")\n",
    "        df_spk.createOrReplaceTempView(f\"increment_birads\")\n",
    "        merge_query = f\"\"\"\n",
    "            MERGE INTO {table_name} AS target\n",
    "            USING increment_birads AS source\n",
    "                ON target.ficha = source.ficha\n",
    "                AND target.id_item = source.id_item\n",
    "                AND target.id_subitem = source.id_subitem\n",
    "            WHEN MATCHED THEN\n",
    "                UPDATE SET *\n",
    "            WHEN NOT MATCHED THEN\n",
    "                INSERT *\n",
    "        \"\"\"\n",
    "        spark.sql(merge_query)\n",
    "    except Exception as e:\n",
    "        error_message(e)\n",
    " \n",
    "def save_data(df_spk: DataFrame, table_name: str, serializable: bool = False):\n",
    "    \"\"\"\n",
    "    Salva os dados do DataFrame em uma tabela Delta, realizando merge se a tabela já existir.\n",
    "    \n",
    "    Parâmetros:\n",
    "        df_spk (DataFrame): DataFrame Spark a ser salvo.\n",
    "        table_name (str): Nome da tabela destino.\n",
    "        serializable (bool): Define o nível de isolamento da transação Delta na inserção inicial.\n",
    "    \n",
    "    Comportamento:\n",
    "        - Se o DataFrame estiver vazio, não faz nada.\n",
    "        - Se a tabela já existe, realiza merge (upsert) dos dados.\n",
    "        - Se a tabela não existe, insere os dados criando a tabela.\n",
    "    \"\"\"\n",
    "    if df_spk.isEmpty():\n",
    "        return None\n",
    " \n",
    "    if spark.catalog.tableExists(table_name):\n",
    "        merge_data(df_spk, table_name)\n",
    "    else:\n",
    "        insert_data(df_spk, table_name, serializable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "968a8d6d-839f-4da8-96cb-f1eb08971880",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aplicar transformações nos dfs\n",
    "df_spk = transform_fields(df_spk)\n",
    "df_spk_ativacao = transform_fields(df_spk_ativacao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7148f273-32fe-47b8-b14e-36cc9f7478b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Excluir duplicados para considerar apenas a ficha na ativação e não os exames (itens). Assim vamos enviar apenas \n",
    "# 1 push por ficha\n",
    "df_spk_ativacao = df_spk_ativacao.dropDuplicates(['ficha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5481a4c3-1cc9-4af4-96a1-44c77fd052b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print('quantidade de laudos salvos na tabela',df_spk.count())\n",
    "print('quantidade de laudos salvos na tabela de ativação', df_spk_ativacao.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d00bff5f-7773-49c4-9c4c-dc2598d05a33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# - Se o DataFrame estiver vazio, não faz nada.\n",
    "# - Se a tabela já existe, realiza merge (upsert) dos dados.\n",
    "# - Se a tabela não existe, insere os dados criando a tabela.\n",
    "save_data(df_spk, table_birads)\n",
    "\n",
    "# - Escreve o DataFrame na tabela Delta especificada, sobrescrevendo dados existentes.\n",
    "# - Atualiza o schema da tabela conforme necessário.\n",
    "# - Define o nível de isolamento da transação Delta.\n",
    "insert_data(df_spk_ativacao, table_birads_ativacao)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7440347683651919,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1_fleury_mama_birads",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "fleury",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
