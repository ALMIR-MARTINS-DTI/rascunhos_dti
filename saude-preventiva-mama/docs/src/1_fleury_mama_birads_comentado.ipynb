{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eddc859c",
   "metadata": {},
   "source": [
    "# Extração e Classificação de BI-RADS em Laudos de Mamografia\n",
    "\n",
    "## Introdução Técnica\n",
    "\n",
    "Este notebook implementa um sistema automatizado para extração, classificação e processamento de informações de BI-RADS (Breast Imaging Reporting and Data System) a partir de laudos de mamografia do Grupo Fleury. O sistema é projetado para identificar pacientes com exames em atraso com base na classificação BI-RADS e programar lembretes para acompanhamento médico adequado.\n",
    "\n",
    "### Objetivo Principal\n",
    "\n",
    "O objetivo principal deste notebook é extrair automaticamente as classificações BI-RADS (0-6) de laudos de mamografia, calcular datas de retorno recomendadas com base nessa classificação, e identificar pacientes elegíveis para notificações de acompanhamento. Pacientes com BI-RADS 1, 2 e 3 são incluídos em uma lista de ativação para monitoramento e notificações de retorno.\n",
    "\n",
    "### Tecnologias Utilizadas\n",
    "\n",
    "- **Framework de Processamento**: Apache Spark (PySpark)\n",
    "- **Armazenamento de Dados**: Delta Lake\n",
    "- **Manipulação de Dados**: PySpark SQL e DataFrame API\n",
    "- **Processamento de Texto**: Expressões Regulares (RegEx)\n",
    "- **Monitoramento e Alertas**: Octoops (Sentinel)\n",
    "- **Logging**: Biblioteca padrão Python (logging)\n",
    "\n",
    "### Fluxo de Trabalho/Etapas Principais\n",
    "\n",
    "1. **Configuração do Ambiente**: Instalação de dependências e configuração de logging\n",
    "2. **Definição de Parâmetros**: Configuração das tabelas e filtros para consultas SQL\n",
    "3. **Extração de BI-RADS**: Consulta SQL complexa que:\n",
    "   - Utiliza RegEx para extrair classificações BI-RADS dos laudos\n",
    "   - Transforma códigos romanos em valores numéricos (I→1, II→2, etc.)\n",
    "   - Calcula valores mínimo, máximo e atual de BI-RADS\n",
    "4. **Processamento de Datas de Retorno**: Cálculo das datas previstas de retorno com base na classificação BI-RADS\n",
    "5. **Filtragem e Transformação**: Aplicação de transformações nos dados e filtragem de pacientes elegíveis\n",
    "6. **Persistência de Dados**: Salvamento dos dados processados em tabelas Delta\n",
    "\n",
    "### Dados Envolvidos\n",
    "\n",
    "- **Fonte de Dados**: \n",
    "  - Tabela: `refined.saude_preventiva.fleury_laudos`\n",
    "  - Filtro principal: `linha_cuidado = 'mama'`\n",
    "  - Tipos de exame: 'MAMOG', 'MAMOGDIG', 'MAMOPROT', 'MAMOG3D'\n",
    "\n",
    "- **Tabelas de Destino**:\n",
    "  - `refined.saude_preventiva.fleury_laudos_mama_birads`: Dados gerais dos laudos com classificação BI-RADS\n",
    "  - `refined.saude_preventiva.fleury_laudos_mama_birads_ativacao`: Subconjunto com pacientes elegíveis para notificações\n",
    "\n",
    "- **Colunas Principais**:\n",
    "  - `ficha`, `id_item`, `id_subitem`: Identificadores do exame\n",
    "  - `MIN_BIRADS`, `MAX_BIRADS`, `BIRADS`: Valores de classificação BI-RADS\n",
    "  - `dth_pedido`: Data do pedido do exame\n",
    "  - `dth_previsao_retorno`: Data calculada para retorno do paciente\n",
    "  - `retorno_cliente`: Tempo recomendado para retorno em meses (12 para BI-RADS 1-2, 6 para BI-RADS 3)\n",
    "\n",
    "### Resultados/Saídas Esperadas\n",
    "\n",
    "1. **Tabela Estruturada de BI-RADS**: Tabela Delta contendo todos os exames de mamografia com classificação BI-RADS extraída\n",
    "2. **Lista de Ativação**: Tabela Delta contendo apenas pacientes:\n",
    "   - Com BI-RADS 1, 2 ou 3\n",
    "   - Sem retorno registrado\n",
    "   - Do sexo feminino entre 40-75 anos\n",
    "   - Pertencentes a marcas específicas do grupo\n",
    "\n",
    "### Pré-requisitos\n",
    "\n",
    "- **Ambiente Databricks**: Com acesso ao Delta Lake e capacidade de execução de PySpark\n",
    "- **Dependência Principal**: Pacote `octoops` para monitoramento e alertas\n",
    "- **Acesso a Dados**: Permissões de leitura na tabela `refined.saude_preventiva.fleury_laudos`\n",
    "- **Permissões de Escrita**: Para criar ou atualizar as tabelas de destino\n",
    "\n",
    "### Considerações Importantes\n",
    "\n",
    "- **Extração via RegEx**: A extração de BI-RADS usa expressões regulares sofisticadas para identificar padrões como \"BIRADS X\", \"CATEGORIA X\" ou notações em algarismos romanos\n",
    "- **Recomendações de Retorno**: Seguem o protocolo médico padrão:\n",
    "  - BI-RADS 1-2: Retorno em 12 meses (360 dias)\n",
    "  - BI-RADS 3: Retorno em 6 meses (180 dias)\n",
    "  - BI-RADS 4-6: Sem recomendação automática de retorno (requer avaliação médica)\n",
    "- **Deduplicação**: A tabela de ativação remove duplicatas por `ficha` para evitar múltiplas notificações para o mesmo paciente\n",
    "- **Tratamento de Erros**: Sistema robusto com captura de exceções e notificações via Sentinel em caso de falhas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778e1fde",
   "metadata": {},
   "source": [
    "## Instalação de Dependências: Octoops\n",
    "\n",
    "Esta célula realiza a instalação da biblioteca `octoops` via pip. Esta biblioteca é essencial para o funcionamento do notebook, pois:\n",
    "\n",
    "- Fornece funcionalidades de monitoramento e alerta para pipelines de dados\n",
    "- Permite enviar notificações em caso de falhas no processamento\n",
    "- Integra-se com sistemas de monitoramento para rastreamento de execuções\n",
    "\n",
    "O comando `%pip install octoops` utiliza a sintaxe \"magic command\" do Jupyter para instalar a biblioteca diretamente no ambiente atual de execução, sem necessidade de reiniciar o kernel manualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabdc9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install octoops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fd6a4e",
   "metadata": {},
   "source": [
    "## Verificação da Instalação do Octoops\n",
    "\n",
    "Esta célula executa o comando `pip show octoops` para verificar se a biblioteca foi instalada corretamente e exibir informações detalhadas sobre a versão instalada. \n",
    "\n",
    "A saída esperada incluirá:\n",
    "- Nome do pacote\n",
    "- Versão\n",
    "- Sumário (descrição breve)\n",
    "- Autor\n",
    "- Localização de instalação\n",
    "- Dependências\n",
    "\n",
    "Esta verificação é importante para confirmar que a biblioteca está disponível no ambiente antes de prosseguir com o restante do código que depende dela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423d4400",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show octoops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4da4f6",
   "metadata": {},
   "source": [
    "## Reinicialização do Python\n",
    "\n",
    "Esta célula reinicia o interpretador Python usando a função `dbutils.library.restartPython()`, específica do ambiente Databricks. \n",
    "\n",
    "Esta operação é necessária após a instalação de novas bibliotecas para garantir que:\n",
    "- As novas dependências sejam carregadas corretamente no ambiente\n",
    "- As alterações de versões de bibliotecas sejam aplicadas\n",
    "- Quaisquer configurações de ambiente modificadas pela instalação sejam ativadas\n",
    "\n",
    "Após a reinicialização, o kernel Python será reiniciado, mantendo as células já executadas, mas com as novas bibliotecas disponíveis para uso nas células subsequentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dc52c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900b7691",
   "metadata": {},
   "source": [
    "## Importação de Bibliotecas\n",
    "\n",
    "Esta célula importa todas as bibliotecas e módulos necessários para o funcionamento do notebook. As importações podem ser categorizadas da seguinte forma:\n",
    "\n",
    "### PySpark SQL e DataFrame\n",
    "- `col`, `year`, `month`, `dayofmonth`, `when`, `lit`, `expr`, `to_timestamp`: Funções para manipulação de colunas em DataFrames\n",
    "- `DateType`: Tipo de dados para datas\n",
    "- `DataFrame`: Classe para tipagem\n",
    "- `datediff`, `to_date`: Funções para operações com datas\n",
    "\n",
    "### Bibliotecas Padrão Python\n",
    "- `datetime`: Para manipulação de datas e horários\n",
    "- `logging`: Para registro de logs\n",
    "- `sys`: Para interação com o sistema\n",
    "- `traceback`: Para captura detalhada de erros\n",
    "\n",
    "### Octoops (Monitoramento)\n",
    "- `OctoOps`: Biblioteca principal para monitoramento\n",
    "- `Sentinel`: Componente específico para envio de alertas\n",
    "\n",
    "Estas importações fornecem as ferramentas necessárias para processar dados com PySpark, manipular datas, gerenciar erros e enviar alertas durante a execução do pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1087079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, year, month, dayofmonth, when, lit, expr, to_timestamp\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import datediff, to_date\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import sys\n",
    "import traceback\n",
    "from octoops import OctoOps\n",
    "from octoops import Sentinel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3dc3b3",
   "metadata": {},
   "source": [
    "## Configuração do Logger\n",
    "\n",
    "Esta célula inicializa um objeto logger para registrar eventos durante a execução do notebook. O logger é configurado usando:\n",
    "\n",
    "- `logging.getLogger(__name__)`: Cria um logger associado ao nome do módulo atual\n",
    "\n",
    "A configuração do logger permite:\n",
    "- Registro padronizado de mensagens informativas, avisos e erros\n",
    "- Rastreamento centralizado das etapas de execução\n",
    "- Integração com o sistema de logging do Databricks\n",
    "\n",
    "O logger será utilizado posteriormente no código para registrar eventos importantes, como avisos sobre DataFrames vazios ou informações sobre operações de salvamento de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b02a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4953c2",
   "metadata": {},
   "source": [
    "## Configuração das Tabelas e Filtros\n",
    "\n",
    "Esta célula define parâmetros cruciais para a execução do pipeline, incluindo nomes de tabelas de destino e filtros SQL.\n",
    "\n",
    "### Variáveis/Tabelas Definidas\n",
    "\n",
    "- **`table_birads`**: Nome da tabela principal de destino para armazenar todos os laudos processados com BI-RADS\n",
    "- **`table_birads_ativacao`**: Nome da tabela de ativação para pacientes elegíveis para notificações\n",
    "- **`where_clause`**: Cláusula SQL dinâmica para processamento incremental\n",
    "\n",
    "### Lógica Detalhada\n",
    "\n",
    "1. **Definição de Tabelas**: Define os caminhos completos para as tabelas Delta\n",
    "2. **Processamento Incremental**:\n",
    "   - Verifica se a tabela principal já existe usando `spark.catalog.tableExists()`\n",
    "   - Se existir, configura a cláusula WHERE para processar apenas registros mais recentes que o último processamento\n",
    "   - Isso otimiza o processamento ao evitar reprocessar registros antigos\n",
    "\n",
    "3. **Definição do Filtro de Ativação**:\n",
    "   - Configura um filtro SQL complexo para selecionar pacientes elegíveis para notificações\n",
    "   - Critérios incluem:\n",
    "     - Pacientes sem retorno registrado (`eleg.ficha IS NULL`)\n",
    "     - Com BI-RADS 1, 2 ou 3 (`brd.BIRADS IN (1, 2, 3)`)\n",
    "     - Realizaram exames específicos (`sigla_exame IN ('MAMOG', 'MAMOGDIG', 'MAMOPROT', 'MAMOG3D')`)\n",
    "     - Sexo feminino (`UPPER(flr.sexo_cliente) = 'F'`)\n",
    "     - Faixa etária apropriada (`idade_cliente >= 40 AND idade_cliente < 76`)\n",
    "     - Pertencentes a marcas específicas do grupo Fleury\n",
    "\n",
    "Estes parâmetros determinam quais registros serão processados e como serão filtrados para as tabelas de destino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d748789",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_birads = \"refined.saude_preventiva.fleury_laudos_mama_birads\"\n",
    "table_birads_ativacao = \"refined.saude_preventiva.fleury_laudos_mama_birads_ativacao\"\n",
    "where_clause = \"\"\n",
    " \n",
    "if spark.catalog.tableExists(table_birads):\n",
    "    where_clause = f\"\"\"\n",
    "    WHERE\n",
    "        flr._datestamp >= (\n",
    "            SELECT MAX(brd._datestamp)\n",
    "            FROM {table_birads} brd\n",
    "        )\n",
    "    \"\"\"\n",
    " \n",
    "filtro_ativacao = \"\"\"\n",
    "    WHERE\n",
    "        eleg.ficha IS NULL\n",
    "        AND brd.BIRADS IN (1, 2, 3)\n",
    "        AND flr.sigla_exame IN ('MAMOG', 'MAMOGDIG', 'MAMOPROT', 'MAMOG3D')\n",
    "        AND UPPER(flr.sexo_cliente) = 'F'\n",
    "        AND (\n",
    "            idade_cliente >= 40 AND idade_cliente < 76\n",
    "        )\n",
    "        AND UPPER(flr.marca) ILIKE ANY(\n",
    "            'AMAIS - BA',\n",
    "            'AMAIS - PE',\n",
    "            'AMAIS - SP',\n",
    "            'Felippe Mattoso',\n",
    "            'FLEURY',\n",
    "            'IR',\n",
    "            'LABS AMAIS'\n",
    "        )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec46bc1",
   "metadata": {},
   "source": [
    "## Consulta SQL para Extração de BI-RADS\n",
    "\n",
    "Esta célula implementa uma consulta SQL complexa para extrair classificações BI-RADS de laudos de mamografia e criar dois DataFrames principais: um com todos os laudos processados e outro apenas com pacientes elegíveis para ativação.\n",
    "\n",
    "### Estrutura da Consulta\n",
    "\n",
    "A consulta utiliza Common Table Expressions (CTEs) para organizar o processamento em etapas lógicas:\n",
    "\n",
    "1. **CTE `base`**: \n",
    "   - Extrai os identificadores básicos dos exames (`ficha`, `id_item`, `id_subitem`)\n",
    "   - Usa expressões regulares sofisticadas para extrair menções a BI-RADS dos laudos\n",
    "   - Transforma algarismos romanos (I, II, III, etc.) em valores numéricos\n",
    "   - Filtra laudos da linha de cuidado 'mama' e tipos específicos de exame\n",
    "\n",
    "2. **CTE `dados_birads`**: \n",
    "   - Calcula métricas agregadas de BI-RADS:\n",
    "     - `MIN_BIRADS`: Menor valor de BI-RADS encontrado no laudo\n",
    "     - `MAX_BIRADS`: Maior valor de BI-RADS encontrado no laudo\n",
    "     - `BIRADS`: Valor mais recente/final (último elemento do array)\n",
    "\n",
    "3. **CTE `dados_laudos`**: \n",
    "   - Coleta dados demográficos e administrativos\n",
    "   - Calcula a idade do cliente \n",
    "   - Aplica o filtro incremental (`where_clause`)\n",
    "\n",
    "4. **Consulta Final**: \n",
    "   - Combina os dados das CTEs através de JOINs\n",
    "   - Une com a tabela de retornos elegíveis (`fleury_retorno_elegivel_ficha`)\n",
    "   - Aplica o filtro de ativação quando apropriado\n",
    "\n",
    "### DataFrames Gerados\n",
    "\n",
    "- **`df_spk`**: DataFrame principal contendo todos os laudos com classificações BI-RADS extraídas\n",
    "- **`df_spk_ativacao`**: DataFrame filtrado contendo apenas pacientes elegíveis para ativação (notificações)\n",
    "\n",
    "### Aspectos Técnicos Importantes\n",
    "\n",
    "- **Processamento de Texto**: Uso extensivo de funções RegEx para limpar e extrair BI-RADS\n",
    "- **Transformação de Tipos**: Conversão entre tipos de dados (string para inteiro, romanos para arábicos)\n",
    "- **Funções Analíticas**: Uso de `ARRAY_MIN`, `ARRAY_MAX` e `TRY_ELEMENT_AT` para análise de arrays\n",
    "- **Formatação Dinâmica**: A consulta usa `.format()` para inserir dinamicamente as cláusulas SQL geradas anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5927fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH base AS (\n",
    "    SELECT\n",
    "        flr.ficha,\n",
    "        flr.id_item,\n",
    "        flr.id_subitem,\n",
    "        REGEXP_EXTRACT_ALL(\n",
    "            REGEXP_EXTRACT(\n",
    "                REGEXP_REPLACE(UPPER(flr.laudo_tratado), r'[-:®]|\\xa0', ''),\n",
    "                r'(?mi)(AVALIA[CÇ][AÃ]O|CONCLUS[AÃ]O|IMPRESS[AÃ]O|OPINI[AÃ]O)?(.*)', 2\n",
    "            ),\n",
    "            r\"(?mi)(BIRADS|CATEGORIA|CAT\\W)\\s*(\\d+\\w*|VI|V|IV|III|II|I)\\W*\\w?(BIRADS|CATEGORIA)?(\\W|$)\", 2\n",
    "        ) AS RAW_BIRADS,\n",
    "        FILTER(\n",
    "            TRANSFORM(RAW_BIRADS, x ->\n",
    "                CASE\n",
    "                    WHEN x = \"I\" THEN 1\n",
    "                    WHEN x = \"II\" THEN 2\n",
    "                    WHEN x = \"III\" THEN 3\n",
    "                    WHEN x = \"IV\" THEN 4\n",
    "                    WHEN x = \"V\" THEN 5\n",
    "                    WHEN x = \"VI\" THEN 6\n",
    "                    WHEN TRY_CAST(x AS INT) > 6 THEN NULL\n",
    "                    ELSE REGEXP_REPLACE(x, r'[^0-9]', '')\n",
    "                END\n",
    "            ), x -> x IS NOT NULL\n",
    "        ) AS CAT_BIRADS\n",
    "    FROM refined.saude_preventiva.fleury_laudos flr\n",
    "    WHERE\n",
    "        flr.linha_cuidado = 'mama'\n",
    "        AND flr.sigla_exame IN ('MAMOG', 'MAMOGDIG', 'MAMOPROT', 'MAMOG3D')\n",
    "),\n",
    " \n",
    "dados_birads AS (\n",
    "    SELECT\n",
    "        *,\n",
    "        ARRAY_MIN(CAT_BIRADS) AS MIN_BIRADS,\n",
    "        ARRAY_MAX(CAT_BIRADS) AS MAX_BIRADS,\n",
    "        TRY_ELEMENT_AT(CAST(CAT_BIRADS AS ARRAY<INT>), -1) AS BIRADS\n",
    "    FROM base\n",
    "),\n",
    " \n",
    "dados_laudos AS (\n",
    "    SELECT\n",
    "        flr.linha_cuidado,\n",
    "        flr.id_unidade,\n",
    "        flr.id_ficha,\n",
    "        flr.id_item,\n",
    "        flr.id_subitem,\n",
    "        flr.ficha,\n",
    "        flr.id_exame,\n",
    "        flr.id_cliente,\n",
    "        flr.pefi_cliente,\n",
    "        flr.sigla_exame,\n",
    "        flr.id_marca,\n",
    "        flr.marca,\n",
    "        (\n",
    "          TIMESTAMPDIFF(DAY, flr.dth_nascimento_cliente, CURDATE()) / 365.25\n",
    "        ) AS idade_cliente,\n",
    "        flr.sexo_cliente,\n",
    "        flr.dth_pedido,\n",
    "        flr._datestamp\n",
    "    FROM refined.saude_preventiva.fleury_laudos flr\n",
    "    {where_clause}\n",
    ")\n",
    " \n",
    "SELECT\n",
    "    flr.* except(idade_cliente),\n",
    "    brd.MIN_BIRADS,\n",
    "    brd.MAX_BIRADS,\n",
    "    brd.BIRADS,\n",
    " \n",
    "    eleg.dth_pedido        AS dth_pedido_retorno_elegivel,\n",
    "    eleg.ficha             AS ficha_retorno_elegivel,\n",
    "    eleg.siglas_ficha      AS siglas_ficha_retorno_elegivel,\n",
    "    eleg.marca             AS marca_retorno_elegivel,\n",
    "    eleg.unidade           AS unidade_retorno_elegivel,\n",
    "    eleg.convenio          AS convenio_retorno_elegivel,\n",
    "    eleg.valores_exame     AS valores_exame_retorno_elegivel,\n",
    "    eleg.valores_ficha     AS valores_ficha_retorno_elegivel,\n",
    "    eleg.qtd_exame         AS qtd_exame_retorno_elegivel,\n",
    "    eleg.secao             AS secao_retorno_elegivel,\n",
    "    eleg.dias_entre_ficha  AS dias_entre_ficha_elegivel\n",
    "FROM dados_laudos flr\n",
    "INNER JOIN dados_birads brd\n",
    "    ON flr.ficha = brd.ficha\n",
    "    AND flr.id_item = brd.id_item\n",
    "    AND flr.id_subitem = brd.id_subitem\n",
    "LEFT JOIN refined.saude_preventiva.fleury_retorno_elegivel_ficha eleg\n",
    "    ON eleg.ficha_origem = flr.ficha\n",
    "    AND eleg.id_cliente = flr.id_cliente\n",
    "    AND eleg.linha_cuidado = flr.linha_cuidado\n",
    "{filtro_ativacao}\n",
    "\"\"\"\n",
    " \n",
    "df_spk = spark.sql(query.format(\n",
    "    where_clause = where_clause,\n",
    "    filtro_ativacao = \"\"\n",
    "    )\n",
    ")\n",
    " \n",
    "df_spk_ativacao = spark.sql(query.format(\n",
    "    where_clause = \"\",\n",
    "    filtro_ativacao = filtro_ativacao\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d2fa56",
   "metadata": {},
   "source": [
    "## Função de Cálculo de Data Prevista de Retorno\n",
    "\n",
    "Esta célula define a função `calcular_data_prevista`, que adiciona uma coluna com a data recomendada de retorno para exames de acompanhamento com base na classificação BI-RADS. Esta função é crucial para o sistema de notificações, pois determina quando os pacientes devem retornar para exames de controle.\n",
    "\n",
    "### Detalhes da Função\n",
    "\n",
    "**Parâmetros:**\n",
    "- `df_spk` (DataFrame): DataFrame Spark contendo dados dos laudos com a coluna `BIRADS`\n",
    "\n",
    "**Retorno:**\n",
    "- DataFrame com a coluna adicional `dth_previsao_retorno`\n",
    "\n",
    "**Lógica de Cálculo:**\n",
    "- Para **BI-RADS 1 ou 2**: Adiciona 360 dias (aproximadamente 1 ano) à data do pedido\n",
    "- Para **BI-RADS 3**: Adiciona 180 dias (aproximadamente 6 meses) à data do pedido\n",
    "- Para **outros valores de BI-RADS** (4, 5, 6): Define como None (sem recomendação automática)\n",
    "\n",
    "**Implementação Técnica:**\n",
    "- Usa `withColumn()` para adicionar uma nova coluna ao DataFrame\n",
    "- Utiliza `when().otherwise()` para expressões condicionais\n",
    "- Aplica a função SQL `date_add()` para realizar os cálculos de data\n",
    "\n",
    "Esta função segue diretrizes médicas padrão para acompanhamento de exames de mamografia baseado na classificação BI-RADS, onde categorias 1-2 representam baixo risco e categoria 3 representa risco intermediário que requer acompanhamento mais próximo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d032286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_data_prevista(df_spk: DataFrame):\n",
    "    \"\"\"\n",
    "    Adiciona uma coluna 'dth_previsao_retorno' ao DataFrame com base na coluna 'BIRADS'.\n",
    "\n",
    "    - Para BIRADS 1 ou 2, adiciona 360 dias à data da coluna 'dth_pedido'.\n",
    "    - Para BIRADS 3, adiciona 180 dias à data da coluna 'dth_pedido'.\n",
    "    - Para outros valores de BIRADS, define 'dth_previsao_retorno' como None.\n",
    "\n",
    "    Parâmetros:\n",
    "    df_spk (DataFrame): O DataFrame Spark contendo os dados de entrada.\n",
    "\n",
    "    Retorna:\n",
    "    DataFrame: O DataFrame atualizado com a nova coluna 'dth_previsao_retorno'.\n",
    "    \"\"\"\n",
    "    df_spk = df_spk.withColumn(\n",
    "        'dth_previsao_retorno',\n",
    "        when(\n",
    "            col('BIRADS').isin([1, 2]),\n",
    "            expr(\"date_add(dth_pedido, 360)\")\n",
    "        ).when(\n",
    "            col('BIRADS') == 3,\n",
    "            expr(\"date_add(dth_pedido, 180)\")  \n",
    "        ).otherwise(None)\n",
    "    )\n",
    "    return df_spk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04544804",
   "metadata": {},
   "source": [
    "## Função de Transformação de Campos\n",
    "\n",
    "Esta célula define a função `transform_fields`, responsável por enriquecer os DataFrames com campos calculados e transformações adicionais. Esta função é uma peça central do pipeline, pois prepara os dados para análise e persistência após a extração inicial dos valores de BI-RADS.\n",
    "\n",
    "### Objetivo da Função\n",
    "\n",
    "A função realiza várias transformações importantes nos dados:\n",
    "1. Adiciona informação sobre o tempo recomendado de retorno em meses\n",
    "2. Calcula a data prevista de retorno\n",
    "3. Converte campos de data para formatos apropriados\n",
    "4. Calcula a diferença em dias entre datas relevantes\n",
    "\n",
    "### Parâmetros e Retorno\n",
    "\n",
    "**Parâmetros:**\n",
    "- `df_spk` (DataFrame): DataFrame Spark a ser transformado\n",
    "\n",
    "**Retorno:**\n",
    "- DataFrame transformado com colunas adicionais\n",
    "\n",
    "### Lógica Detalhada\n",
    "\n",
    "1. **Verificação de DataFrame Vazio**:\n",
    "   - Se o DataFrame estiver vazio, registra um aviso via logger e retorna sem alterações\n",
    "\n",
    "2. **Adição da Coluna de Retorno**:\n",
    "   - Cria a coluna `retorno_cliente` que indica o tempo recomendado para retorno em meses\n",
    "   - BI-RADS 1-2: 12 meses\n",
    "   - BI-RADS 3: 6 meses\n",
    "   - Outros valores: 0 (sem recomendação automática)\n",
    "\n",
    "3. **Cálculo de Datas**:\n",
    "   - Chama a função `calcular_data_prevista()` definida anteriormente\n",
    "   - Converte colunas de data para tipo timestamp\n",
    "   - Calcula a diferença em dias entre a data prevista de retorno e a data do pedido\n",
    "\n",
    "### Campos Criados/Modificados\n",
    "\n",
    "- **`retorno_cliente`**: Tempo de retorno recomendado em meses (12, 6 ou 0)\n",
    "- **`dth_previsao_retorno`**: Data calculada para o próximo exame\n",
    "- **`dth_pedido_retorno_elegivel`**: Data do pedido convertida para timestamp\n",
    "- **`dias_ate_retorno`**: Número de dias entre o exame atual e o próximo recomendado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec552f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_fields(df_spk: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transforma os campos do DataFrame fornecido.\n",
    "\n",
    "    - Verifica se o DataFrame está vazio. Se estiver, registra um aviso e retorna o DataFrame sem alterações.\n",
    "    - Adiciona uma coluna 'retorno_cliente' com valores baseados na coluna 'BIRADS':\n",
    "      - 12 meses para BIRADS 1 ou 2\n",
    "      - 6 meses para BIRADS 3\n",
    "      - 0 para outros valores\n",
    "    - Calcula a data prevista de retorno usando a função 'calcular_data_prevista'.\n",
    "    - Converte a coluna 'dth_pedido_retorno_elegivel' para o tipo timestamp.\n",
    "    - Converte a coluna 'dth_previsao_retorno' para o tipo timestamp.\n",
    "    - Calcula a diferença em dias entre 'dth_previsao_retorno' e 'dth_pedido', armazenando o resultado na coluna 'dias_ate_retorno'.\n",
    "\n",
    "    Parâmetros:\n",
    "    df_spk (DataFrame): O DataFrame a ser transformado.\n",
    "\n",
    "    Retorna:\n",
    "    DataFrame: O DataFrame transformado com as novas colunas.\n",
    "    \"\"\"\n",
    "    if df_spk.isEmpty():\n",
    "        logger.warning(\"No Data Found!\")\n",
    "        return df_spk\n",
    "    \n",
    "    df_spk = df_spk.withColumn(\n",
    "        'retorno_cliente',\n",
    "        when(col('BIRADS').isin([1, 2]), 12).when(col('BIRADS') == 3, 6).otherwise(0)\n",
    "    )\n",
    "\n",
    "    df_spk = calcular_data_prevista(df_spk)\n",
    "    df_spk = df_spk.withColumn('dth_pedido_retorno_elegivel', to_timestamp(col('dth_pedido_retorno_elegivel')))\n",
    "    df_spk = df_spk.withColumn('dth_previsao_retorno', to_timestamp(col('dth_previsao_retorno')))\n",
    "    df_spk = df_spk.withColumn('dias_ate_retorno', datediff(to_date(col('dth_previsao_retorno')), to_date(col('dth_pedido'))))\n",
    "    return df_spk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a5a2b5",
   "metadata": {},
   "source": [
    "## Configuração de Webhook e Função de Tratamento de Erros\n",
    "\n",
    "Esta célula configura o sistema de alerta e define a função `error_message` para tratamento padronizado de erros. O tratamento de erros é fundamental para garantir a robustez do pipeline e facilitar o diagnóstico de problemas.\n",
    "\n",
    "### Variáveis e Configurações\n",
    "\n",
    "- **`WEBHOOK_DS_AI_BUSINESS_STG`**: Define o ambiente para o webhook do Sentinel ('stg' para staging/homologação)\n",
    "\n",
    "### Detalhes da Função `error_message`\n",
    "\n",
    "A função `error_message` é responsável por tratar e reportar erros de maneira padronizada:\n",
    "\n",
    "**Parâmetros:**\n",
    "- `e` (Exception): A exceção capturada durante a execução\n",
    "\n",
    "**Comportamento:**\n",
    "1. **Captura detalhada do erro**: Utiliza `traceback.format_exc()` para obter a pilha de chamadas completa\n",
    "2. **Formatação da mensagem**: Cria uma mensagem informativa com detalhes do erro\n",
    "3. **Envio de alerta**: Inicializa o Sentinel com parâmetros específicos e envia o alerta:\n",
    "   - `project_name`: 'Monitor_Linhas_Cuidado_Mama'\n",
    "   - `task_title`: 'Fleury Mamografia'\n",
    "   - Categoria do alerta: 'Alerta'\n",
    "4. **Exibição do traceback**: Exibe o traceback no console para depuração imediata\n",
    "5. **Propagação do erro**: Relança a exceção para interromper a execução do pipeline\n",
    "\n",
    "Esta função centraliza o tratamento de erros em todo o notebook, garantindo consistência no monitoramento e notificação de problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5f141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEBHOOK_DS_AI_BUSINESS_STG = 'stg'\n",
    "\n",
    "def error_message(e):\n",
    "    \"\"\"\n",
    "    Envia alerta para o Sentinel e exibe o traceback em caso de erro ao salvar dados.\n",
    "\n",
    "    Parâmetros:\n",
    "        e (Exception): Exceção capturada.\n",
    "\n",
    "    Comportamento:\n",
    "        - Formata o traceback do erro.\n",
    "        - Envia alerta para o Sentinel com detalhes do erro.\n",
    "        - Exibe o traceback no console.\n",
    "        - Relança a exceção.\n",
    "    \"\"\"\n",
    "    error_message = traceback.format_exc()\n",
    "    summary_message = f\"\"\"Erro ao salvar dados.\\n{error_message}\"\"\"\n",
    "    sentinela_ds_ai_business = Sentinel(\n",
    "        project_name='Monitor_Linhas_Cuidado_Mama',\n",
    "        env_type=WEBHOOK_DS_AI_BUSINESS_STG,\n",
    "        task_title='Fleury Mamografia'\n",
    "    )\n",
    "    sentinela_ds_ai_business.alerta_sentinela(\n",
    "        categoria='Alerta', \n",
    "        mensagem=summary_message,\n",
    "        job_id_descritivo='1_fleury_mama_birads'\n",
    "    )\n",
    "    traceback.print_exc()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8671b4ec",
   "metadata": {},
   "source": [
    "## Funções de Gerenciamento de Dados em Delta Lake\n",
    "\n",
    "Esta célula define três funções essenciais para gerenciar a persistência de dados no formato Delta Lake:\n",
    "\n",
    "### 1. Função `insert_data`\n",
    "\n",
    "**Objetivo:** Inserir dados em uma tabela Delta, sobrescrevendo completamente o conteúdo anterior.\n",
    "\n",
    "**Parâmetros:**\n",
    "- `df_spk` (DataFrame): DataFrame Spark com os dados a serem inseridos\n",
    "- `table_name` (str): Nome da tabela Delta de destino\n",
    "- `serializable` (bool, opcional): Define o nível de isolamento da transação\n",
    "\n",
    "**Comportamento:**\n",
    "- Registra a operação no log\n",
    "- Configura o nível de isolamento apropriado ('Serializable' ou 'WriteSerializable')\n",
    "- Escreve os dados com opções otimizadas:\n",
    "  - Permite atualização do schema se necessário (`overwriteSchema=true`)\n",
    "  - Usa modo 'overwrite' para substituição completa dos dados\n",
    "- Captura exceções e aciona o tratamento de erros via `error_message()`\n",
    "\n",
    "### 2. Função `merge_data`\n",
    "\n",
    "**Objetivo:** Realizar operações de mesclagem (upsert) em tabelas Delta existentes.\n",
    "\n",
    "**Parâmetros:**\n",
    "- `df_spk` (DataFrame): DataFrame Spark com os dados a serem mesclados\n",
    "- `table_name` (str): Nome da tabela Delta de destino\n",
    "\n",
    "**Comportamento:**\n",
    "- Registra a operação no log\n",
    "- Cria uma view temporária (`increment_birads`) com os dados do DataFrame\n",
    "- Executa um comando MERGE SQL que:\n",
    "  - Atualiza registros existentes (identificados pela chave composta `ficha`, `id_item`, `id_subitem`)\n",
    "  - Insere novos registros quando não existem correspondências\n",
    "- Captura exceções e aciona o tratamento de erros via `error_message()`\n",
    "\n",
    "### 3. Função `save_data`\n",
    "\n",
    "**Objetivo:** Função de alto nível que decide entre inserção ou mesclagem com base no contexto.\n",
    "\n",
    "**Parâmetros:**\n",
    "- `df_spk` (DataFrame): DataFrame Spark com os dados a serem salvos\n",
    "- `table_name` (str): Nome da tabela Delta de destino\n",
    "- `serializable` (bool, opcional): Parâmetro passado para `insert_data` se usado\n",
    "\n",
    "**Lógica:**\n",
    "- Verifica se o DataFrame está vazio - se estiver, retorna sem ação\n",
    "- Verifica se a tabela já existe:\n",
    "  - Se existir, chama `merge_data()` para realizar upsert\n",
    "  - Se não existir, chama `insert_data()` para criar a tabela\n",
    "\n",
    "Este conjunto de funções fornece uma abstração completa para persistência de dados, gerenciando automaticamente a criação de novas tabelas ou atualização das existentes, com tratamento adequado de erros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c2585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data(df_spk: DataFrame, table_name: str, serializable: bool = False):\n",
    "    \"\"\"\n",
    "    Insere os dados do DataFrame em uma tabela Delta no Databricks.\n",
    "    \n",
    "    Parâmetros:\n",
    "        df_spk (DataFrame): DataFrame Spark a ser salvo.\n",
    "        table_name (str): Nome da tabela destino.\n",
    "        serializable (bool): Define o nível de isolamento da transação Delta. \n",
    "            Se True, usa 'Serializable', caso contrário 'WriteSerializable'.\n",
    "    \n",
    "    Comportamento:\n",
    "        - Escreve o DataFrame na tabela Delta especificada, sobrescrevendo dados existentes.\n",
    "        - Atualiza o schema da tabela conforme necessário.\n",
    "        - Define o nível de isolamento da transação Delta.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Inserting Data: {table_name}\")\n",
    "        isolation_level = 'Serializable' if serializable else 'WriteSerializable'\n",
    "        (\n",
    "            df_spk.write\n",
    "                .format('delta')\n",
    "                .option('overwriteSchema', 'true')\n",
    "                .option('delta.isolationLevel', isolation_level)\n",
    "                .mode('overwrite')\n",
    "                .saveAsTable(table_name)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        error_message(e)\n",
    " \n",
    "def merge_data(df_spk: DataFrame, table_name: str):\n",
    "    \"\"\"\n",
    "    Realiza um merge (upsert) dos dados do DataFrame em uma tabela Delta existente.\n",
    "    \n",
    "    Parâmetros:\n",
    "        df_spk (DataFrame): DataFrame Spark a ser mesclado.\n",
    "        table_name (str): Nome da tabela destino.\n",
    "    \n",
    "    Comportamento:\n",
    "        - Cria uma view temporária com os dados do DataFrame.\n",
    "        - Executa um comando MERGE SQL para atualizar registros existentes (com base em ficha, id_item e id_subitem)\n",
    "          ou inserir novos registros na tabela Delta.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Merging Data: {table_name}\")\n",
    "        df_spk.createOrReplaceTempView(f\"increment_birads\")\n",
    "        merge_query = f\"\"\"\n",
    "            MERGE INTO {table_name} AS target\n",
    "            USING increment_birads AS source\n",
    "                ON target.ficha = source.ficha\n",
    "                AND target.id_item = source.id_item\n",
    "                AND target.id_subitem = source.id_subitem\n",
    "            WHEN MATCHED THEN\n",
    "                UPDATE SET *\n",
    "            WHEN NOT MATCHED THEN\n",
    "                INSERT *\n",
    "        \"\"\"\n",
    "        spark.sql(merge_query)\n",
    "    except Exception as e:\n",
    "        error_message(e)\n",
    " \n",
    "def save_data(df_spk: DataFrame, table_name: str, serializable: bool = False):\n",
    "    \"\"\"\n",
    "    Salva os dados do DataFrame em uma tabela Delta, realizando merge se a tabela já existir.\n",
    "    \n",
    "    Parâmetros:\n",
    "        df_spk (DataFrame): DataFrame Spark a ser salvo.\n",
    "        table_name (str): Nome da tabela destino.\n",
    "        serializable (bool): Define o nível de isolamento da transação Delta na inserção inicial.\n",
    "    \n",
    "    Comportamento:\n",
    "        - Se o DataFrame estiver vazio, não faz nada.\n",
    "        - Se a tabela já existe, realiza merge (upsert) dos dados.\n",
    "        - Se a tabela não existe, insere os dados criando a tabela.\n",
    "    \"\"\"\n",
    "    if df_spk.isEmpty():\n",
    "        return None\n",
    " \n",
    "    if spark.catalog.tableExists(table_name):\n",
    "        merge_data(df_spk, table_name)\n",
    "    else:\n",
    "        insert_data(df_spk, table_name, serializable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9572cef4",
   "metadata": {},
   "source": [
    "## Aplicação das Transformações nos DataFrames\n",
    "\n",
    "Esta célula aplica as transformações definidas anteriormente aos dois DataFrames principais gerados pela consulta SQL. Este é um passo essencial para preparar os dados para análise e persistência.\n",
    "\n",
    "### Objetivo da Célula\n",
    "\n",
    "Enriquecer os DataFrames `df_spk` e `df_spk_ativacao` com:\n",
    "- Tempo recomendado para retorno (em meses)\n",
    "- Datas previstas para retorno\n",
    "- Conversões adequadas de tipos de dados\n",
    "- Cálculo de dias até o retorno previsto\n",
    "\n",
    "### Dependências\n",
    "\n",
    "- Função `transform_fields()` definida anteriormente\n",
    "- DataFrames `df_spk` e `df_spk_ativacao` criados pela consulta SQL\n",
    "\n",
    "### Lógica Detalhada\n",
    "\n",
    "A célula simplesmente chama a função `transform_fields()` para cada DataFrame:\n",
    "\n",
    "1. Para `df_spk` (todos os laudos com classificação BI-RADS)\n",
    "2. Para `df_spk_ativacao` (subset de pacientes elegíveis para notificações)\n",
    "\n",
    "### Impacto\n",
    "\n",
    "Após a execução desta célula:\n",
    "- Os DataFrames terão novas colunas como `retorno_cliente`, `dth_previsao_retorno` e `dias_ate_retorno`\n",
    "- As datas estarão convertidas para o formato apropriado (timestamp)\n",
    "- Os DataFrames estarão prontos para análise e para serem salvos nas tabelas Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d12438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar transformações nos dfs\n",
    "df_spk = transform_fields(df_spk)\n",
    "df_spk_ativacao = transform_fields(df_spk_ativacao)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5af319",
   "metadata": {},
   "source": [
    "## Deduplicação de Registros para Ativação\n",
    "\n",
    "Esta célula realiza um processamento importante no DataFrame de ativação, removendo registros duplicados para garantir que cada paciente receba apenas uma notificação por ficha, independentemente de quantos exames (itens) foram realizados.\n",
    "\n",
    "### Objetivo da Célula\n",
    "\n",
    "Eliminar registros duplicados do DataFrame `df_spk_ativacao` para que cada ficha (que representa um atendimento) apareça apenas uma vez na lista de ativação, evitando o envio de múltiplas notificações para o mesmo paciente sobre o mesmo exame.\n",
    "\n",
    "### Lógica Implementada\n",
    "\n",
    "A célula utiliza o método `.dropDuplicates(['ficha'])` do PySpark DataFrame API para:\n",
    "\n",
    "1. Identificar registros com o mesmo valor na coluna `ficha`\n",
    "2. Manter apenas a primeira ocorrência de cada valor único de `ficha`\n",
    "3. Descartar todas as ocorrências duplicadas\n",
    "\n",
    "### Contexto e Importância\n",
    "\n",
    "Na estrutura de dados do Fleury:\n",
    "- Uma `ficha` corresponde a um atendimento/visita\n",
    "- Um atendimento pode conter vários `id_item` e `id_subitem` (exames e procedimentos)\n",
    "\n",
    "Sem esta deduplicação, o sistema enviaria múltiplas notificações para o mesmo paciente sobre o mesmo atendimento, apenas porque foram realizados vários procedimentos durante aquela visita. A deduplicação garante que a comunicação com o paciente seja apropriada e não excessiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0113d61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluir duplicados para considerar apenas a ficha na ativação e não os exames (itens). Assim vamos enviar apenas \n",
    "# 1 push por ficha\n",
    "df_spk_ativacao = df_spk_ativacao.dropDuplicates(['ficha'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac85a71",
   "metadata": {},
   "source": [
    "## Contagem de Laudos Processados\n",
    "\n",
    "Esta célula exibe a quantidade de registros em cada DataFrame após todas as transformações e filtragens. Este passo fornece um importante controle de qualidade e visibilidade sobre o volume de dados que serão persistidos.\n",
    "\n",
    "### Objetivo da Célula\n",
    "\n",
    "- Contar e exibir o número de registros nos DataFrames principais\n",
    "- Fornecer uma verificação visual do volume de dados processados\n",
    "- Permitir validar se os filtros e transformações estão gerando a quantidade esperada de registros\n",
    "\n",
    "### Dados Exibidos\n",
    "\n",
    "Duas métricas principais são exibidas:\n",
    "\n",
    "1. **Quantidade de laudos na tabela principal**: Número total de laudos de mamografia com classificação BI-RADS extraída e processada\n",
    "   \n",
    "2. **Quantidade de laudos na tabela de ativação**: Número de registros únicos (por ficha) que atendem aos critérios para notificação:\n",
    "   - BI-RADS 1, 2 ou 3\n",
    "   - Pacientes do sexo feminino entre 40-75 anos\n",
    "   - Sem retorno já registrado\n",
    "   - De marcas específicas\n",
    "\n",
    "### Importância para o Pipeline\n",
    "\n",
    "Estas contagens servem tanto para fins de monitoramento quanto para diagnóstico:\n",
    "- Valores muito baixos podem indicar problemas nos filtros ou na extração de BI-RADS\n",
    "- Valores muito altos podem sugerir processamento duplicado ou filtros muito amplos\n",
    "- A comparação entre as duas contagens dá uma visão da proporção de pacientes elegíveis para notificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef57c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('quantidade de laudos salvos na tabela',df_spk.count())\n",
    "print('quantidade de laudos salvos na tabela de ativação', df_spk_ativacao.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d71fa2e",
   "metadata": {},
   "source": [
    "## Salvamento das Tabelas Processadas\n",
    "\n",
    "Esta célula final realiza a persistência dos dados processados nas tabelas Delta Lake, concluindo o pipeline de processamento. Os dados serão utilizados para análises posteriores e para o sistema de notificação de pacientes.\n",
    "\n",
    "### Objetivo da Célula\n",
    "\n",
    "Salvar os dois DataFrames processados nas suas respectivas tabelas Delta:\n",
    "\n",
    "1. **Tabela Principal (BI-RADS)**: Contém todos os laudos com classificação BI-RADS extraída e dados enriquecidos\n",
    "2. **Tabela de Ativação**: Contém apenas os registros elegíveis para envio de notificações\n",
    "\n",
    "### Funções Utilizadas\n",
    "\n",
    "- **`save_data()`**: Para a tabela principal, utiliza a função inteligente que decide entre merge e insert com base na existência da tabela\n",
    "- **`insert_data()`**: Para a tabela de ativação, força a substituição completa dos dados, garantindo que apenas os pacientes atualmente elegíveis estejam presentes\n",
    "\n",
    "### Comportamento de Persistência\n",
    "\n",
    "- Para tabela principal: Dados são mesclados (merge) se a tabela já existe, preservando o histórico\n",
    "- Para tabela de ativação: Dados são completamente substituídos, mantendo apenas os pacientes atualmente elegíveis\n",
    "\n",
    "### Tabelas de Destino\n",
    "\n",
    "- **`refined.saude_preventiva.fleury_laudos_mama_birads`**: Tabela histórica com todos os laudos processados\n",
    "- **`refined.saude_preventiva.fleury_laudos_mama_birads_ativacao`**: Tabela com pacientes atualmente elegíveis para notificação\n",
    "\n",
    "Após a execução desta célula, os dados estarão disponíveis para consumo por outros sistemas, como dashboards de BI ou sistemas de notificação automática de pacientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46865a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Salvar tabelas\n",
    "save_data(df_spk, table_birads)\n",
    "insert_data(df_spk_ativacao, table_birads_ativacao)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
