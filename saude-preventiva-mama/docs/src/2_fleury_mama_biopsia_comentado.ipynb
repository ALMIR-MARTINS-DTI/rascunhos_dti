{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8815c42f",
   "metadata": {},
   "source": [
    "# Extração de Dados de Carcinoma em Exames de Biópsia de Mama do Fleury\n",
    "\n",
    "## Introdução Técnica Detalhada\n",
    "\n",
    "Este notebook implementa um pipeline para identificação e extração automatizada de resultados de carcinoma em laudos de biópsia de mama do laboratório Fleury. O sistema utiliza técnicas de processamento de linguagem natural (especificamente, expressões regulares) para detectar menções a carcinoma em relatórios de biópsia, focando na relação entre o termo \"carcinoma\" e a topografia \"mama\".\n",
    "\n",
    "### Objetivo Principal\n",
    "\n",
    "O objetivo principal deste notebook é processar laudos médicos de biópsia de mama para identificar aqueles que contêm diagnósticos de carcinoma. O sistema extrai esta informação crítica e a armazena de forma estruturada em uma tabela Delta para análises posteriores e integração com sistemas de monitoramento de saúde preventiva.\n",
    "\n",
    "### Tecnologias Utilizadas\n",
    "\n",
    "- **Framework de Processamento**: Apache Spark (PySpark)\n",
    "- **Armazenamento de Dados**: Delta Lake\n",
    "- **Processamento de Texto**: Expressões Regulares (RegEx)\n",
    "- **Monitoramento e Alertas**: Octoops (Sentinel)\n",
    "- **Logging**: Biblioteca padrão Python (logging)\n",
    "- **Gerenciamento de Feature Store**: Databricks Feature Store\n",
    "\n",
    "### Fluxo de Trabalho/Etapas Principais\n",
    "\n",
    "1. **Configuração do Ambiente**: Instalação de dependências e configuração de logging\n",
    "2. **Definição de Filtros**: Configuração de cláusulas SQL para filtragem incremental e por tipo de exame\n",
    "3. **Extração de Carcinoma**: \n",
    "   - Consulta SQL com expressões regulares para identificar menções a carcinoma relacionadas à mama\n",
    "   - Geração de indicador booleano `HAS_CARCINOMA`\n",
    "4. **Seleção de Colunas**: Filtragem das colunas relevantes para análise e persistência\n",
    "5. **Persistência dos Dados**: Salvamento dos dados processados em tabela Delta usando operações de merge\n",
    "6. **Monitoramento**: Sistema de alertas para notificar em caso de ausência de dados ou erros\n",
    "\n",
    "### Dados Envolvidos\n",
    "\n",
    "- **Fonte de Dados**: \n",
    "  - Tabela: `refined.saude_preventiva.fleury_laudos`\n",
    "  - Filtro principal: `linha_cuidado = 'mama'`\n",
    "  - Tipos de exame específicos para biópsia: 'BIOMAMUSCORE', 'BIOMAMUSPAAF', 'BIOMAMUS', 'MAMOTOMIA', 'MAMOTOMUS', 'MAMCLIPE', 'MAMOTORM'\n",
    "\n",
    "- **Tabela de Destino**:\n",
    "  - `refined.saude_preventiva.fleury_laudos_mama_biopsia`: Tabela Delta contendo os dados extraídos de laudos de biópsia de mama com indicação de presença de carcinoma\n",
    "\n",
    "- **Colunas Principais**:\n",
    "  - `ficha`, `id_item`, `id_subitem`: Identificadores do exame\n",
    "  - `laudo_tratado`: Texto do laudo médico processado\n",
    "  - `RAW_CARCINOMA`: Extração bruta do termo encontrado\n",
    "  - `HAS_CARCINOMA`: Indicador booleano da presença de carcinoma\n",
    "\n",
    "### Resultados/Saídas Esperadas\n",
    "\n",
    "1. **DataFrame Processado**: DataFrame contendo laudos de biópsia de mama com indicação de presença ou ausência de carcinoma\n",
    "2. **Tabela Delta**: Persistência dos dados processados na tabela `refined.saude_preventiva.fleury_laudos_mama_biopsia`\n",
    "3. **Contagem de Registros**: Exibição do número total de registros processados e salvos\n",
    "4. **Alertas (se necessário)**: Notificações em caso de erros ou ausência de dados para processamento\n",
    "\n",
    "### Pré-requisitos\n",
    "\n",
    "- **Ambiente Databricks**: Com acesso ao Delta Lake e capacidade de execução de PySpark\n",
    "- **Dependência Principal**: Pacote `octoops` para monitoramento e alertas\n",
    "- **Acesso a Dados**: Permissões de leitura na tabela `refined.saude_preventiva.fleury_laudos`\n",
    "- **Permissões de Escrita**: Para criar ou atualizar a tabela de destino\n",
    "\n",
    "### Considerações Importantes\n",
    "\n",
    "- **Processamento Incremental**: O notebook implementa um mecanismo de processamento incremental, processando apenas os registros mais recentes que os já salvos na tabela de destino\n",
    "- **Expressões Regulares**: O padrão regex utilizado foi projetado especificamente para identificar a relação entre \"carcinoma\" e \"mama\" no texto dos laudos\n",
    "- **Filtragem Demográfica**: O sistema foca em pacientes do sexo feminino na faixa etária de 40 a 75 anos\n",
    "- **Tratamento de Erros**: Implementação robusta com captura de exceções e sistema de alertas via Sentinel\n",
    "- **Processamento de Dados Sensíveis**: As informações processadas são sensíveis e relacionadas a diagnósticos de câncer, exigindo tratamento adequado de privacidade e segurança"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ead0653",
   "metadata": {},
   "source": [
    "# Extrair dados dos exames de Biopsia\n",
    "+ Filtrar os laudos que possuem o termo topografia MAMA e desses laudos filtrar aqueles que possuem a palavra carcinoma (extrair toda a frase que está assoaciada ao termo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c84b2d",
   "metadata": {},
   "source": [
    "## Instalação de Dependências\n",
    "\n",
    "Esta célula instala a biblioteca `octoops`, que é necessária para o monitoramento e envio de alertas no pipeline de processamento. O Octoops é uma ferramenta interna utilizada para monitoramento de processos de dados, permitindo o envio de notificações e a gestão de feature stores.\n",
    "\n",
    "A célula utiliza o comando mágico `%pip` do Jupyter para instalar a biblioteca diretamente no ambiente de execução. O código comentado mostra que originalmente também havia a intenção de instalar o pacote `databricks-feature-store` com a flag `-q` (quiet, para minimizar a saída do comando), mas esta instalação foi comentada.\n",
    "\n",
    "Após a instalação desta biblioteca, será possível utilizar funcionalidades como:\n",
    "- Sistema de alertas via Sentinel\n",
    "- Gerenciamento de Feature Store\n",
    "- Monitoramento de execução do pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b338933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install databricks-feature-store -q\n",
    "%pip install octoops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7525a442",
   "metadata": {},
   "source": [
    "## Reinicialização do Ambiente Python\n",
    "\n",
    "Esta célula reinicia o interpretador Python no ambiente Databricks através da função `dbutils.library.restartPython()`. Esta etapa é necessária após a instalação de novos pacotes (como o octoops instalado na célula anterior) para garantir que eles sejam carregados corretamente no ambiente de execução.\n",
    "\n",
    "A reinicialização:\n",
    "- Finaliza a sessão Python atual\n",
    "- Inicia uma nova sessão com as bibliotecas recém-instaladas disponíveis\n",
    "- Mantém o contexto do notebook (células já executadas)\n",
    "- Garante que a biblioteca Octoops estará acessível para as células seguintes\n",
    "\n",
    "Esta é uma prática recomendada ao instalar novas bibliotecas em notebooks Databricks, pois alguns pacotes só ficam disponíveis após a reinicialização do ambiente Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bccdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a6a10a",
   "metadata": {},
   "source": [
    "## Importações Comentadas\n",
    "\n",
    "Esta célula contém importações de bibliotecas que foram comentadas durante o desenvolvimento do notebook. Estas linhas representam uma abordagem alternativa ou anterior para configurar o ambiente Spark e o Feature Store, que não estão sendo utilizadas na versão atual do notebook.\n",
    "\n",
    "As importações comentadas incluem:\n",
    "- Módulos core do PySpark (`SparkSession`, `Window`, funções)\n",
    "- Função `row_number` para potencial processamento de dados com numeração de linhas\n",
    "- Inicialização personalizada de uma sessão Spark com nome específico (\"LLM_Extractor_Carcinoma\")\n",
    "- Cliente do Databricks Feature Store\n",
    "\n",
    "Estas importações foram mantidas como referência para possíveis modificações futuras ou para documentar abordagens alternativas que foram consideradas durante o desenvolvimento. Na implementação atual, as dependências necessárias são importadas na próxima célula de código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba24dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.window import Window\n",
    "# from pyspark.sql import functions as F\n",
    "# from pyspark.sql.functions import row_number\n",
    "# spark = SparkSession.builder.appName(\"LLM_Extractor_Carcinoma\").getOrCreate()\n",
    "# from databricks.feature_store import FeatureStoreClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41162211",
   "metadata": {},
   "source": [
    "## Importação de Bibliotecas\n",
    "\n",
    "Esta célula importa todas as bibliotecas e módulos necessários para o processamento dos dados, manipulação de DataFrames, operações com datas, logging e envio de alertas. As importações são essenciais para o funcionamento do pipeline de extração e processamento de dados de biópsia.\n",
    "\n",
    "### Bibliotecas Importadas\n",
    "\n",
    "#### PySpark SQL e Funções\n",
    "- **Funções de coluna e manipulação de dados**: `col`, `year`, `month`, `dayofmonth`, `when`, `lit`, `expr`, `to_timestamp`\n",
    "- **Tipos de dados**: `DateType` para manipulação de datas\n",
    "- **DataFrame API**: `DataFrame` (para tipagem)\n",
    "- **Funções de data**: `datediff`, `to_date` para cálculos com datas\n",
    "\n",
    "#### Bibliotecas Padrão Python\n",
    "- **Manipulação de datas**: `datetime`\n",
    "- **Logging**: `logging` para registro de eventos e mensagens\n",
    "- **Interação com sistema**: `sys`\n",
    "- **Tratamento de erros**: `traceback` para formatação de mensagens de erro\n",
    "\n",
    "#### Octoops (Monitoramento)\n",
    "- `OctoOps`: Framework principal\n",
    "- `Sentinel`: Componente para envio de alertas\n",
    "- `FeatureStoreManager`: Gerenciador de feature store\n",
    "\n",
    "#### Databricks\n",
    "- `FeatureStoreClient`: Cliente para interagir com o Databricks Feature Store\n",
    "\n",
    "Esta ampla gama de importações fornece as ferramentas necessárias para extrair, processar, validar e persistir os dados de laudos de biópsia de mama, bem como para monitorar a execução do pipeline e lidar com possíveis erros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7c2bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, year, month, dayofmonth, when, lit, expr, to_timestamp\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import datediff, to_date\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import sys\n",
    "import traceback\n",
    "from octoops import OctoOps\n",
    "from octoops import Sentinel\n",
    "from octoops import FeatureStoreManager\n",
    "from databricks.feature_store import FeatureStoreClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa6fcfd",
   "metadata": {},
   "source": [
    "## Configuração do Logger\n",
    "\n",
    "Esta célula configura um objeto logger para registrar mensagens de log durante a execução do notebook. O comentário incluído na célula original já explica sua função básica.\n",
    "\n",
    "O logger é inicializado com a função `logging.getLogger(__name__)`, que cria um logger específico para o módulo atual (identificado por `__name__`). Esta abordagem é uma prática recomendada em Python para registrar eventos de forma organizada e hierárquica.\n",
    "\n",
    "Benefícios desta configuração:\n",
    "1. **Identificação da Fonte**: As mensagens de log são associadas ao módulo que as gerou\n",
    "2. **Controle Granular**: Permite ajustar os níveis de logging (INFO, WARNING, ERROR) especificamente para este notebook\n",
    "3. **Integração**: Funciona com o sistema de logging mais amplo do ambiente Databricks\n",
    "\n",
    "Embora simples, esta configuração é essencial para o monitoramento eficiente do pipeline, permitindo que mensagens informativas e de erro sejam registradas de forma consistente ao longo da execução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a92436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# O código configura um logger para registrar mensagens de log no módulo atual (__name__).\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dacd3e2",
   "metadata": {},
   "source": [
    "## Definição de Filtros para Extração de Dados\n",
    "\n",
    "Esta célula configura os filtros e parâmetros para a extração dos dados de biópsia. Ela define três elementos principais: a tabela de destino e duas cláusulas SQL que serão utilizadas na consulta principal.\n",
    "\n",
    "### Configurações Definidas\n",
    "\n",
    "1. **Tabela de Destino**:\n",
    "   ```python\n",
    "   table_biopsia = \"refined.saude_preventiva.fleury_laudos_mama_biopsia\"\n",
    "   ```\n",
    "   Esta variável especifica o nome completo da tabela Delta onde os resultados serão salvos.\n",
    "\n",
    "2. **Cláusula WHERE para Processamento Incremental** (`where_clause`):\n",
    "   Esta cláusula implementa um mecanismo de processamento incremental, garantindo que apenas os registros mais recentes sejam processados. O filtro:\n",
    "   - Compara o campo `_datestamp` dos registros na tabela fonte\n",
    "   - Seleciona apenas registros com timestamp posterior ao último registro processado\n",
    "   - Utiliza uma subconsulta para determinar o valor máximo de `_datestamp` na tabela de destino\n",
    "\n",
    "3. **Filtro de Extração** (`filtro_extracao`):\n",
    "   Este filtro define os critérios específicos para selecionar exames de biópsia de mama:\n",
    "   - Linha de cuidado: 'mama'\n",
    "   - Tipos específicos de exame: 'BIOMAMUSCORE', 'BIOMAMUSPAAF', 'BIOMAMUS', 'MAMOTOMIA', 'MAMOTOMUS', 'MAMCLIPE', 'MAMOTORM'\n",
    "   - Sexo do paciente: feminino ('F')\n",
    "   - Faixa etária: 40 a 75 anos\n",
    "\n",
    "### Impacto no Pipeline\n",
    "\n",
    "Estas configurações:\n",
    "- Otimizam o processamento ao evitar reprocessamento desnecessário de dados\n",
    "- Garantem que apenas exames relevantes de biópsia de mama sejam processados\n",
    "- Aplicam filtros demográficos para focar na população-alvo\n",
    "- Permitem a referência à tabela de destino em diferentes partes do código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5093438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtros para a extração\n",
    "table_biopsia = \"refined.saude_preventiva.fleury_laudos_mama_biopsia\" \n",
    "\n",
    "where_clause = f\"\"\"\n",
    "    WHERE\n",
    "        _datestamp > (\n",
    "            SELECT MAX(biop._datestamp)\n",
    "            FROM {table_biopsia} biop\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    " \n",
    "filtro_extracao = \"\"\"\n",
    "    WHERE\n",
    "        linha_cuidado   = 'mama'\n",
    "        AND sigla_exame IN (\n",
    "            \"BIOMAMUSCORE\",\n",
    "            \"BIOMAMUSPAAF\",\n",
    "            \"BIOMAMUS\",\n",
    "            \"MAMOTOMIA\",\n",
    "            \"MAMOTOMUS\",\n",
    "            \"MAMCLIPE\",\n",
    "            \"MAMOTORM\"\n",
    "        )\n",
    "        AND UPPER(sexo_cliente) = 'F'\n",
    "        AND (\n",
    "            idade_cliente >= 40 AND idade_cliente < 76\n",
    "        )       \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b27a2c0",
   "metadata": {},
   "source": [
    "## Consulta SQL para Extração de Carcinoma em Laudos de Biópsia\n",
    "\n",
    "Esta célula contém o núcleo do processamento de dados, implementando uma consulta SQL complexa para identificar e extrair menções a carcinoma em laudos de biópsia de mama. Os comentários incluídos no código original já fornecem uma explicação básica sobre o funcionamento da expressão regular utilizada.\n",
    "\n",
    "### Objetivo da Célula\n",
    "\n",
    "A célula realiza três operações principais:\n",
    "1. Define uma consulta SQL para extrair e processar laudos de biópsia de mama\n",
    "2. Executa esta consulta utilizando o Spark SQL\n",
    "3. Exibe o resultado em forma de tabela\n",
    "\n",
    "### Estrutura da Consulta SQL\n",
    "\n",
    "A consulta utiliza Common Table Expressions (CTEs) para organizar a lógica:\n",
    "\n",
    "1. **CTE `base`**: \n",
    "   - Seleciona colunas da tabela `refined.saude_preventiva.fleury_laudos`\n",
    "   - Calcula a idade do paciente a partir da data de nascimento\n",
    "   - Aplica expressões regulares para extrair menções a carcinoma relacionadas à mama\n",
    "   - Cria um campo booleano `HAS_CARCINOMA` que indica presença ou ausência de carcinoma\n",
    "\n",
    "2. **Consulta Principal**: \n",
    "   - Seleciona todos os campos da CTE `base`\n",
    "   - Aplica o filtro de extração definido anteriormente\n",
    "\n",
    "### Expressões Regulares Utilizadas\n",
    "\n",
    "O ponto central da consulta é o uso de expressões regulares para identificar carcinoma. Duas formas são identificadas:\n",
    "\n",
    "```sql\n",
    "REGEXP_EXTRACT(\n",
    "    flr.laudo_tratado,\n",
    "    r'(?mi).*Topografia:.*MAMA.*(CARCINOMA).*|.*(CARCINOMA).*Topografia:.*MAMA.*',\n",
    "    1\n",
    ")\n",
    "```\n",
    "\n",
    "Esta expressão:\n",
    "1. Utiliza flags `(?mi)` para busca case-insensitive e multiline\n",
    "2. Procura padrões onde:\n",
    "   - \"Topografia:\" seguido de \"MAMA\" aparece antes de \"CARCINOMA\" OU\n",
    "   - \"CARCINOMA\" aparece antes de \"Topografia:\" seguido de \"MAMA\"\n",
    "3. Captura o termo \"CARCINOMA\" quando encontrado nestes contextos\n",
    "4. Armazena o resultado em `RAW_CARCINOMA`\n",
    "\n",
    "### Variáveis Criadas\n",
    "\n",
    "- **`df_spk_biopsia`**: DataFrame Spark contendo os laudos de biópsia processados com indicação de presença de carcinoma\n",
    "\n",
    "### Saída e Visualização\n",
    "\n",
    "A função `display(df_spk_biopsia)` ao final exibe o DataFrame resultante, permitindo visualizar os laudos processados e verificar a detecção de carcinoma em cada um deles.\n",
    "\n",
    "### Impacto\n",
    "\n",
    "Esta consulta é o coração do notebook, realizando o processamento essencial para identificar diagnósticos de carcinoma em laudos de biópsia. O DataFrame criado será utilizado nas células posteriores para persistência e análise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d106c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliza REGEXP_EXTRACT para buscar, no texto do campo 'laudo_tratado', \n",
    "# a ocorrência da palavra \"CARCINOMA\" relacionada à \"MAMA\" (seja antes ou depois de \"Topografia\").\n",
    "# O padrão regex considera maiúsculas/minúsculas e múltiplas linhas.\n",
    "# Se encontrar, extrai \"CARCINOMA\" para a coluna RAW_CARCINOMA; caso contrário, retorna string vazia.\n",
    "# O CASE seguinte verifica se RAW_CARCINOMA está vazia:\n",
    "# - Se estiver vazia, HAS_CARCINOMA recebe FALSE (não há carcinoma relacionado à mama no laudo).\n",
    "# - Se não estiver vazia, HAS_CARCINOMA recebe TRUE (há carcinoma relacionado à mama no laudo).\n",
    "query = f\"\"\"\n",
    "WITH base AS (\n",
    "    SELECT\n",
    "        flr.id_marca,\n",
    "        flr.id_unidade,\n",
    "        flr.id_cliente, \n",
    "        flr.id_ficha,\n",
    "        flr.ficha,\n",
    "        flr.id_item, \n",
    "        flr.id_subitem, \n",
    "        flr.id_exame, \n",
    "        flr.dth_pedido,\n",
    "        flr.dth_resultado,\n",
    "        flr.sigla_exame,\n",
    "        flr.laudo_tratado,\n",
    "        flr.linha_cuidado,\n",
    "        flr.sexo_cliente,\n",
    "        flr.`_datestamp`,\n",
    "        (\n",
    "          TIMESTAMPDIFF(DAY, flr.dth_nascimento_cliente, CURDATE()) / 365.25\n",
    "        ) AS idade_cliente,\n",
    "        REGEXP_EXTRACT(\n",
    "            flr.laudo_tratado,\n",
    "            r'(?mi).*Topografia:.*MAMA.*(CARCINOMA).*|.*(CARCINOMA).*Topografia:.*MAMA.*',\n",
    "            1\n",
    "        ) AS RAW_CARCINOMA,\n",
    "        CASE\n",
    "            WHEN REGEXP_EXTRACT(\n",
    "                    flr.laudo_tratado,\n",
    "                    r'(?mi).*Topografia:.*MAMA.*(CARCINOMA).*|.*(CARCINOMA).*Topografia:.*MAMA.*',\n",
    "                    1\n",
    "                 ) = ''\n",
    "            THEN FALSE\n",
    "            ELSE TRUE\n",
    "        END AS HAS_CARCINOMA\n",
    "    FROM refined.saude_preventiva.fleury_laudos flr\n",
    "    {where_clause}\n",
    ")\n",
    "SELECT *\n",
    "FROM base\n",
    "{filtro_extracao}\n",
    "\"\"\"\n",
    "df_spk_biopsia = spark.sql(query)\n",
    "display(df_spk_biopsia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28760577",
   "metadata": {},
   "source": [
    "## Visualização Adicional dos Dados Processados\n",
    "\n",
    "Esta célula realiza uma visualização adicional do DataFrame `df_spk_biopsia` gerado na célula anterior. Esta segunda exibição pode parecer redundante (já que o DataFrame foi exibido ao final da célula anterior), mas ela serve a propósitos específicos:\n",
    "\n",
    "1. **Inspeção Detalhada**: Permite visualizar o DataFrame de forma isolada, sem competir com a saída da execução da consulta SQL\n",
    "2. **Verificação de Resultados**: Oferece uma oportunidade para examinar o conteúdo do DataFrame após possíveis manipulações intermediárias (caso existam)\n",
    "3. **Depuração**: Facilita a verificação visual da estrutura e conteúdo dos dados antes de prosseguir para etapas de persistência\n",
    "\n",
    "A função `display()` do Databricks apresenta os dados em formato tabular interativo, permitindo ordenação, filtragem e análise dos registros de forma mais conveniente do que métodos padrão como `df.show()`.\n",
    "\n",
    "Esta visualização adicional é particularmente útil durante o desenvolvimento do notebook, permitindo aos analistas verificar visualmente o sucesso da detecção de carcinoma nos laudos de biópsia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eee10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_spk_biopsia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c172adc",
   "metadata": {},
   "source": [
    "## Configuração do Feature Store (Comentada)\n",
    "\n",
    "Esta célula contém código comentado relacionado à criação de uma tabela no Databricks Feature Store. O código foi mantido como referência para implementação futura ou como documentação de uma abordagem alternativa que foi considerada durante o desenvolvimento.\n",
    "\n",
    "### Funcionalidades Comentadas\n",
    "\n",
    "O código comentado realiza as seguintes operações:\n",
    "\n",
    "1. **Inicialização do Feature Store Client**:\n",
    "   ```python\n",
    "   fs = FeatureStoreClient()\n",
    "   ```\n",
    "\n",
    "2. **Criação de Tabela no Feature Store**:\n",
    "   ```python\n",
    "   fs.create_table(\n",
    "       name=\"refined.saude_preventiva.fleury_laudos_mama_biopsia\",\n",
    "       primary_keys=[\"ficha\",'id_item','id_subitem'],\n",
    "       schema=df_spk.schema,    \n",
    "       description=\"Jornada de Mama - Features extraídas de laudos de biopsia Fleury. Siglas: BIOMAMUSCORE, BIOMAMUSPAAF, BIOMAMUS, MAMOTOMIA, MAMOTOMUS, MAMCLIPE, MAMOTORM\")\n",
    "   ```\n",
    "\n",
    "### Configurações Importantes\n",
    "\n",
    "O código comentado especifica:\n",
    "- **Nome da tabela**: Idêntico ao definido na variável `table_biopsia`\n",
    "- **Chaves primárias**: Combinação de `ficha`, `id_item` e `id_subitem` para identificação única de registros\n",
    "- **Schema**: Baseado no DataFrame `df_spk` (que será definido na próxima célula)\n",
    "- **Descrição**: Documentação detalhada sobre o propósito e o conteúdo da tabela\n",
    "\n",
    "Na implementação atual, o notebook opta por usar diretamente as tabelas Delta para persistência dos dados, em vez de utilizar o Feature Store. Este código comentado é mantido para referência caso a abordagem de Feature Store seja adotada no futuro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0882dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FeatureStore\n",
    "# fs = FeatureStoreClient()\n",
    "# fs.create_table(\n",
    "#     name=\"refined.saude_preventiva.fleury_laudos_mama_biopsia\",\n",
    "#     primary_keys=[\"ficha\",'id_item','id_subitem'],\n",
    "#     schema=df_spk.schema,    \n",
    "#     description=\"Jornada de Mama - Features extraídas de laudos de biopsia Fleury. Siglas: BIOMAMUSCORE, BIOMAMUSPAAF, BIOMAMUS, MAMOTOMIA, MAMOTOMUS, MAMCLIPE, MAMOTORM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e3817b",
   "metadata": {},
   "source": [
    "## Persistência de Dados e Tratamento de Erros\n",
    "\n",
    "Esta célula final é responsável por selecionar, processar e persistir os dados extraídos na tabela Delta, além de implementar um robusto sistema de tratamento de erros. Esta é a etapa que efetivamente salva os resultados do processamento e garante que o sistema notifique os responsáveis em caso de falhas.\n",
    "\n",
    "### Objetivo da Célula\n",
    "\n",
    "Esta célula realiza várias operações críticas:\n",
    "1. Importa bibliotecas adicionais para tratamento de erros e manipulação de tabelas Delta\n",
    "2. Define constantes e caminhos para persistência de dados\n",
    "3. Seleciona apenas as colunas relevantes do DataFrame\n",
    "4. Define uma função para inserção de dados com operações de merge\n",
    "5. Executa a persistência dos dados com tratamento de erros\n",
    "6. Emite alertas em caso de ausência de dados ou erros\n",
    "\n",
    "### Importações e Configurações\n",
    "\n",
    "- **Importações adicionais**: Módulos `sys`, `traceback`, `Sentinel` e `DeltaTable`\n",
    "- **Configuração de Webhook**: `WEBHOOK_DS_AI_BUSINESS_STG = 'stg'` para ambiente de homologação\n",
    "- **Caminho de saída**: `OUTPUT_DATA_PATH = 'refined.saude_preventiva.fleury_laudos_mama_biopsia'`\n",
    "\n",
    "### Seleção de Colunas\n",
    "\n",
    "```python\n",
    "df_spk = df_spk_biopsia.select(\"ficha\",\"id_item\",\"id_subitem\",\"id_cliente\",\"dth_pedido\",\n",
    "                           \"dth_resultado\", \"sigla_exame\",\"laudo_tratado\",\"linha_cuidado\",\n",
    "                           \"_datestamp\",\"RAW_CARCINOMA\",\"HAS_CARCINOMA\")\n",
    "```\n",
    "\n",
    "Esta operação seleciona apenas as colunas necessárias para análise e persistência, eliminando colunas intermediárias ou desnecessárias.\n",
    "\n",
    "### Função de Inserção de Dados\n",
    "\n",
    "A função `insert_data()` implementa uma operação de merge (upsert) utilizando a API DeltaTable:\n",
    "1. Carrega a tabela Delta existente\n",
    "2. Configura uma operação de merge baseada na chave composta (`ficha`, `id_item`, `id_subitem`)\n",
    "3. Atualiza registros existentes quando há correspondência\n",
    "4. Insere novos registros quando não há correspondência\n",
    "\n",
    "### Lógica de Tratamento de Erros\n",
    "\n",
    "O bloco `try-except` implementa a seguinte lógica:\n",
    "1. Verifica se o DataFrame contém registros (`df_spk.count() > 0`)\n",
    "2. Se houver registros, persiste os dados utilizando a função `insert_data()`\n",
    "3. Exibe a contagem de registros salvos\n",
    "4. Se não houver registros, envia um alerta via Sentinel\n",
    "5. Em caso de exceção em qualquer etapa, captura e relança o erro\n",
    "\n",
    "### Sistema de Alertas\n",
    "\n",
    "O sistema de alertas via Sentinel é configurado com:\n",
    "- Nome do projeto: 'Monitor_Linhas_Cuidado_Mama'\n",
    "- Ambiente: Definido pela constante `WEBHOOK_DS_AI_BUSINESS_STG`\n",
    "- Título da tarefa: 'Fleury Mama Biopsia'\n",
    "- Categoria do alerta: 'Alerta'\n",
    "- ID descritivo: '2_fleury_mama_biopsia'\n",
    "\n",
    "### Impacto\n",
    "\n",
    "Esta célula finaliza o pipeline de processamento, garantindo que os dados extraídos sejam corretamente persistidos e que quaisquer problemas sejam prontamente notificados. O uso de operações de merge garante que os dados sejam atualizados de forma incremental, sem duplicações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3fe0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import traceback\n",
    "from octoops import Sentinel\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "WEBHOOK_DS_AI_BUSINESS_STG = 'stg'\n",
    "\n",
    "\n",
    "#OUTPUT_DATA_PATH = dbutils.widgets.get('OUTPUT_DATA_PATH')\n",
    "OUTPUT_DATA_PATH = 'refined.saude_preventiva.fleury_laudos_mama_biopsia'\n",
    "\n",
    "# Selecionar colunas de interesse\n",
    "df_spk = df_spk_biopsia.select(\"ficha\",\"id_item\",\"id_subitem\",\"id_cliente\",\"dth_pedido\",\"dth_resultado\", \"sigla_exame\",\"laudo_tratado\",\"linha_cuidado\",\"_datestamp\",\"RAW_CARCINOMA\",\"HAS_CARCINOMA\")\n",
    "\n",
    "# função para salvar dados na tabela\n",
    "def insert_data(df_spk, output_data_path):  \n",
    "\n",
    "    # Carrega a tabela Delta existente\n",
    "    delta_table = DeltaTable.forName(spark, output_data_path)\n",
    "\n",
    "    # Faz o merge (upsert)\n",
    "    (delta_table.alias(\"target\")\n",
    "     .merge(\n",
    "         df_spk.alias(\"source\"),\n",
    "         \"target.ficha = source.ficha AND target.id_item = source.id_item AND target.id_subitem = source.id_subitem\"\n",
    "     )\n",
    "     .whenMatchedUpdateAll()\n",
    "     .whenNotMatchedInsertAll()\n",
    "     .execute())      \n",
    "try:\n",
    "    if df_spk.count() > 0:\n",
    "        \n",
    "        #1/0 \n",
    "        insert_data(df_spk, OUTPUT_DATA_PATH)\n",
    "        # fs.write_table(\n",
    "        #     name=\"refined.saude_preventiva.fleury_laudos_mama_biopsia\",\n",
    "        #     df=df_spk,\n",
    "        #     mode=\"merge\",\n",
    "        # )\n",
    "        print(f'Total de registros salvos na tabela: {df_spk.count()}')\n",
    "            \n",
    "    else:\n",
    "        error_message = traceback.format_exc()\n",
    "        summary_message = f\"\"\"Não há laudos para extração.\\n{error_message}\"\"\"\n",
    "        sentinela_ds_ai_business = Sentinel(\n",
    "            project_name='Monitor_Linhas_Cuidado_Mama',\n",
    "            env_type=WEBHOOK_DS_AI_BUSINESS_STG,\n",
    "            task_title='Fleury Mama Biopsia'\n",
    "        )\n",
    "\n",
    "        sentinela_ds_ai_business.alerta_sentinela(\n",
    "            categoria='Alerta', \n",
    "            mensagem=summary_message,\n",
    "            job_id_descritivo='2_fleury_mama_biopsia'\n",
    "        )\n",
    "except Exception as e:\n",
    "    traceback.print_exc()        \n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
