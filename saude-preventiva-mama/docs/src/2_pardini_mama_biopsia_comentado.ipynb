{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e56ad032",
   "metadata": {},
   "source": [
    "# Extração Automatizada de Dados de Biópsia Mamária - Pardini\n",
    "\n",
    "## Introdução Técnica Detalhada\n",
    "\n",
    "Este notebook implementa um pipeline completo de extração, processamento e persistência de dados de laudos de biópsia mamária provenientes da base de dados Pardini. O foco principal é identificar ocorrências de carcinoma em laudos relacionados à mama, extraindo informações estruturadas que podem ser utilizadas para análises clínicas e epidemiológicas.\n",
    "\n",
    "### Objetivo Principal\n",
    "\n",
    "O objetivo principal deste notebook é:\n",
    "- Identificar laudos de biópsia mamária que contenham menção a carcinoma\n",
    "- Extrair e estruturar essas informações de forma automatizada\n",
    "- Persistir os resultados em uma tabela Delta para análises posteriores\n",
    "- Monitorar o processo e gerar alertas em caso de falhas\n",
    "\n",
    "Este processo é parte de uma solução mais ampla de saúde preventiva, focada especificamente na detecção precoce de câncer de mama através da análise automatizada de laudos médicos.\n",
    "\n",
    "### Tecnologias Utilizadas\n",
    "\n",
    "O notebook utiliza diversas tecnologias modernas:\n",
    "\n",
    "- **Apache Spark / PySpark**: Framework de processamento distribuído para manipulação de grandes volumes de dados\n",
    "  - `pyspark.sql`: Para manipulação de dados estruturados (DataFrames)\n",
    "  - `pyspark.sql.functions`: Funções para transformação de dados\n",
    "  - `pyspark.sql.types`: Definição de tipos de dados\n",
    "\n",
    "- **Delta Lake**: Tecnologia de armazenamento para tabelas transacionais no data lake\n",
    "  - `delta.tables.DeltaTable`: Para operações de merge/upsert em tabelas Delta\n",
    "\n",
    "- **OctoOps**: Framework interno para orquestração e monitoramento de pipelines de dados\n",
    "  - `Sentinel`: Para envio de alertas e notificações\n",
    "  - `FeatureStoreManager`: Para gerenciamento de features\n",
    "\n",
    "- **Bibliotecas Python Padrão**:\n",
    "  - `logging`: Para registro de eventos\n",
    "  - `datetime`: Para manipulação de datas\n",
    "  - `traceback`: Para captura e formatação de erros\n",
    "\n",
    "### Fluxo de Trabalho/Etapas Principais\n",
    "\n",
    "O notebook segue um fluxo de trabalho sequencial e lógico:\n",
    "\n",
    "1. **Configuração do Ambiente**:\n",
    "   - Instalação de dependências (octoops)\n",
    "   - Importação de bibliotecas necessárias\n",
    "   - Configuração de logging\n",
    "\n",
    "2. **Definição de Filtros e Parâmetros**:\n",
    "   - Especificação da tabela de destino\n",
    "   - Definição de cláusulas WHERE para filtrar dados incrementais\n",
    "   - Configuração de critérios de extração (linha de cuidado, siglas de exame, faixa etária)\n",
    "\n",
    "3. **Extração e Transformação de Dados**:\n",
    "   - Construção de consulta SQL com expressões regulares para identificação de carcinoma\n",
    "   - Extração dos dados da tabela fonte `refined.saude_preventiva.pardini_laudos`\n",
    "   - Cálculo de idade e aplicação de transformações\n",
    "\n",
    "4. **Validação dos Dados Extraídos**:\n",
    "   - Contagem de registros\n",
    "   - Verificação das siglas de exame disponíveis\n",
    "\n",
    "5. **Persistência dos Dados Processados**:\n",
    "   - Seleção das colunas relevantes\n",
    "   - Implementação de operação merge/upsert na tabela Delta\n",
    "   - Tratamento de erros e notificação\n",
    "\n",
    "### Dados Envolvidos\n",
    "\n",
    "#### Fonte de Dados\n",
    "- **Tabela Fonte**: `refined.saude_preventiva.pardini_laudos`\n",
    "  - Contém laudos médicos de exames realizados no laboratório Pardini\n",
    "  - Inclui informações sobre pacientes, exames e resultados em formato semi-estruturado\n",
    "\n",
    "#### Colunas Importantes\n",
    "- `id_marca`, `id_unidade`, `id_cliente`: Identificadores do laboratório, unidade e paciente\n",
    "- `id_ficha`, `ficha`, `id_item`, `id_subitem`: Identificadores do exame\n",
    "- `sigla_exame`: Código que identifica o tipo de exame realizado\n",
    "- `dth_pedido`, `dth_resultado`: Datas de solicitação e resultado do exame\n",
    "- `laudo_tratado`: Texto completo do laudo médico (conteúdo principal para análise)\n",
    "- `linha_cuidado`: Categoria do exame (filtrado para 'mama')\n",
    "- `sexo_cliente`, `idade_cliente`: Dados demográficos do paciente\n",
    "\n",
    "#### Tabela de Destino\n",
    "- **Tabela Delta**: `refined.saude_preventiva.pardini_laudos_mama_biopsia`\n",
    "  - Armazena os resultados da extração com campos adicionais\n",
    "  - Inclui flags como `HAS_CARCINOMA` e o texto extraído `RAW_CARCINOMA`\n",
    "\n",
    "### Resultados/Saídas Esperadas\n",
    "\n",
    "O notebook produz como resultado principal:\n",
    "\n",
    "1. **DataFrame Processado** (`df_spk_biopsia`):\n",
    "   - Contém todos os laudos de biópsia de mama filtrados\n",
    "   - Inclui colunas derivadas como `RAW_CARCINOMA` e `HAS_CARCINOMA`\n",
    "\n",
    "2. **Tabela Delta Persistida**:\n",
    "   - Dados armazenados em formato Delta no caminho `refined.saude_preventiva.pardini_laudos_mama_biopsia`\n",
    "   - Disponíveis para consulta SQL e análises posteriores\n",
    "\n",
    "3. **Logs e Alertas**:\n",
    "   - Mensagens de log durante a execução\n",
    "   - Alertas via Sentinel em caso de falhas ou ausência de dados para processamento\n",
    "\n",
    "### Pré-requisitos\n",
    "\n",
    "Para executar este notebook com sucesso, são necessários:\n",
    "\n",
    "- **Ambiente Databricks** configurado com:\n",
    "  - Runtime que suporte PySpark e Delta Lake\n",
    "  - Acesso às tabelas mencionadas no data lake\n",
    "  - Permissões para leitura/escrita nas tabelas relevantes\n",
    "\n",
    "- **Bibliotecas Instaladas**:\n",
    "  - `octoops`: Para integração com sistema de monitoramento\n",
    "  - `databricks-feature-store`: Para gerenciamento de feature store\n",
    "\n",
    "- **Variáveis de Ambiente**:\n",
    "  - Configurações necessárias para acesso ao ambiente Databricks\n",
    "\n",
    "### Considerações Importantes/Observações\n",
    "\n",
    "- **Processamento Incremental**: O notebook implementa uma estratégia de processamento incremental, processando apenas registros mais recentes que os já armazenados na tabela de destino.\n",
    "\n",
    "- **Expressões Regulares**: A detecção de carcinoma utiliza expressões regulares complexas que procuram pelo termo \"CARCINOMA\" em contextos relacionados à \"MAMA\". A precisão desta detecção depende da qualidade e estrutura dos laudos.\n",
    "\n",
    "- **Filtro Demográfico**: Aplica-se um filtro para selecionar apenas pacientes do sexo feminino (UPPER(sexo_cliente) = 'F') na faixa etária entre 40 e 75 anos.\n",
    "\n",
    "- **Tipos de Exame**: O notebook filtra por siglas específicas de exames relacionados a biópsias mamárias (USPCOB, CBGEST, USPAFI, etc.), que podem variar dependendo do protocolo do laboratório.\n",
    "\n",
    "- **Tratamento de Erros**: Implementa mecanismos robustos para captura e notificação de erros, garantindo que falhas sejam prontamente identificadas.\n",
    "\n",
    "- **Feature Store**: O notebook inclui código comentado para integração com Databricks Feature Store, que pode ser habilitado conforme necessário.\n",
    "  \n",
    "-----------  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5217f73",
   "metadata": {},
   "source": [
    "## Instalação de Dependências\n",
    "\n",
    "Esta célula realiza a instalação das bibliotecas necessárias para a execução do notebook. Duas bibliotecas são mencionadas:\n",
    "\n",
    "1. **databricks-feature-store**: Esta linha está comentada (`# %pip install databricks-feature-store -q`), indicando que a biblioteca está disponível no ambiente ou será instalada posteriormente conforme necessidade. Esta biblioteca permite interagir com o Databricks Feature Store, um repositório centralizado para armazenar e gerenciar features.\n",
    "\n",
    "2. **octoops**: Esta biblioteca está sendo efetivamente instalada (`%pip install octoops`) e é uma ferramenta interna utilizada para operacionalização de modelos e pipelines, incluindo funcionalidades de monitoramento, alerta e gerenciamento de feature store.\n",
    "\n",
    "A instalação usa o comando mágico `%pip` do Jupyter, que permite instalar pacotes Python diretamente no ambiente de execução do notebook. O flag `-q` (na linha comentada) indica instalação em modo \"quiet\", reduzindo a verbosidade da saída.\n",
    "\n",
    "Esta célula precisa ser executada no início do notebook para garantir que todas as dependências estejam disponíveis para as células subsequentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e995c417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install databricks-feature-store -q\n",
    "%pip install octoops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d149ec58",
   "metadata": {},
   "source": [
    "## Importação de Bibliotecas e Módulos\n",
    "\n",
    "Esta célula realiza a importação de todas as bibliotecas e módulos necessários para o funcionamento do pipeline de extração e processamento de dados. As importações abrangem diversas categorias de funcionalidades:\n",
    "\n",
    "### Manipulação de Dados com PySpark\n",
    "\n",
    "- **Funções SQL e Manipulação de DataFrames**:\n",
    "  - `pyspark.sql.functions`: Importação de funções como `col`, `year`, `month`, `dayofmonth`, `when`, `lit` e `expr` para manipulação de colunas e transformações de dados\n",
    "  - `pyspark.sql.types`: Importação do tipo `DateType` para trabalhar com dados temporais\n",
    "  - `pyspark.sql`: Importação da classe `DataFrame` para manipulação estruturada de dados\n",
    "  - `datediff`, `to_date`, `to_timestamp`: Funções específicas para manipulação de datas\n",
    "\n",
    "### Utilitários Python Padrão\n",
    "\n",
    "- **Manipulação de Tempo e Datas**:\n",
    "  - `datetime`: Módulo para manipulação de datas e horários\n",
    "  \n",
    "- **Logging e Tratamento de Erros**:\n",
    "  - `logging`: Para registro de mensagens de log\n",
    "  - `sys`: Acesso a variáveis e funcionalidades específicas do interpretador\n",
    "  - `traceback`: Para captura e formatação de informações de exceções\n",
    "\n",
    "### Ferramentas de Operacionalização (OctoOps)\n",
    "\n",
    "- **Gerenciamento de Features e Monitoramento**:\n",
    "  - `octoops.OctoOps`: Framework principal para operacionalização\n",
    "  - `octoops.Sentinel`: Sistema de alerta e monitoramento\n",
    "  - `octoops.FeatureStoreManager`: Gerenciador da feature store\n",
    "\n",
    "- **Feature Store do Databricks**:\n",
    "  - `databricks.feature_store.FeatureStoreClient`: Cliente para interação com o Feature Store do Databricks\n",
    "\n",
    "Estas importações fornecem todas as ferramentas necessárias para implementar cada etapa do pipeline, desde a extração dos dados brutos até o processamento, transformação, persistência e monitoramento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722668ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, year, month, dayofmonth, when, lit, expr, to_timestamp\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import datediff, to_date\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import sys\n",
    "import traceback\n",
    "from octoops import OctoOps\n",
    "from octoops import Sentinel\n",
    "from octoops import FeatureStoreManager\n",
    "from databricks.feature_store import FeatureStoreClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1793f924",
   "metadata": {},
   "source": [
    "## Configuração do Logger\n",
    "\n",
    "Esta célula configura um sistema de logging básico para o notebook, permitindo o registro de mensagens durante a execução. A configuração do logger é uma prática recomendada em desenvolvimento de pipelines de dados, pois facilita o diagnóstico de problemas e o monitoramento do fluxo de execução.\n",
    "\n",
    "A linha de código utiliza o módulo `logging` do Python (importado anteriormente) para criar um logger específico para este módulo:\n",
    "\n",
    "```python\n",
    "logger = logging.getLogger(__name__)\n",
    "```\n",
    "\n",
    "O parâmetro `__name__` é uma variável especial do Python que contém o nome do módulo atual. Em um notebook Jupyter, isso geralmente corresponde a `__main__`, mas a prática de usar `__name__` é consistente com o desenvolvimento de módulos Python reutilizáveis.\n",
    "\n",
    "Este logger será utilizado posteriormente para registrar informações, avisos e erros durante a execução do pipeline, permitindo:\n",
    "- Rastreamento de eventos durante o processamento\n",
    "- Identificação de problemas em etapas específicas\n",
    "- Registro histórico da execução\n",
    "\n",
    "Embora esta configuração seja simples, ela estabelece a estrutura básica para um sistema de logging que poderia ser expandido com configurações mais avançadas (como níveis de log, formatos personalizados ou redirecionamento para arquivos ou serviços externos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638f7219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# O código configura um logger para registrar mensagens de log no módulo atual (__name__).\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8953c138",
   "metadata": {},
   "source": [
    "## Definição de Filtros para Extração de Dados\n",
    "\n",
    "Esta célula configura os parâmetros e filtros que serão utilizados para extrair dados relevantes do banco de dados. Ela define três elementos principais:\n",
    "\n",
    "### 1. Tabela de Destino\n",
    "\n",
    "```python\n",
    "table_biopsia = \"refined.saude_preventiva.pardini_laudos_mama_biopsia\"\n",
    "```\n",
    "\n",
    "Esta variável armazena o caminho completo da tabela Delta onde os dados processados serão persistidos. A tabela está localizada no schema `refined.saude_preventiva` e é específica para laudos de biópsia mamária do Pardini.\n",
    "\n",
    "### 2. Cláusula WHERE para Processamento Incremental\n",
    "\n",
    "```python\n",
    "where_clause = f\"\"\"\n",
    "    WHERE\n",
    "        _datestamp > (\n",
    "            SELECT MAX(biop._datestamp)\n",
    "            FROM {table_biopsia} biop\n",
    "        )\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "Esta cláusula SQL implementa uma estratégia de processamento incremental, garantindo que apenas registros novos (com `_datestamp` mais recente que o último registro processado) sejam extraídos. Isso é fundamental para:\n",
    "- Evitar o reprocessamento de dados já analisados\n",
    "- Reduzir o custo computacional\n",
    "- Manter a consistência dos dados\n",
    "\n",
    "### 3. Filtro de Extração Detalhado\n",
    "\n",
    "```python\n",
    "filtro_extracao = \"\"\"\n",
    "    WHERE\n",
    "        linha_cuidado   = 'mama'\n",
    "        AND sigla_exame IN (...)\n",
    "        AND UPPER(sexo_cliente) = 'F'\n",
    "        AND (idade_cliente >= 40 AND idade_cliente < 76)\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "Este filtro mais complexo implementa critérios específicos de seleção:\n",
    "\n",
    "- **Linha de Cuidado**: Restringe a exames relacionados à linha de cuidado 'mama'\n",
    "- **Siglas de Exame**: Seleciona apenas exames específicos relacionados a biópsia mamária:\n",
    "  - \"USPCOB\": Ultrassonografia com core biopsy\n",
    "  - \"CBGEST\": Core biopsy guiada por estereotaxia\n",
    "  - \"USPAFI\": Ultrassonografia com punção aspirativa\n",
    "  - \"ESMATO\": Estudo anatomopatológico mamário\n",
    "  - \"USMATO\": Ultrassonografia mamária\n",
    "  - \"MPMFECG\": Mamografia com nódulo palpável\n",
    "  - \"RMMATO\": Ressonância magnética mamária\n",
    "  - \"USPMAF\": Ultrassonografia de mama feminina\n",
    "- **Demografia do Paciente**:\n",
    "  - Apenas sexo feminino: `UPPER(sexo_cliente) = 'F'`\n",
    "  - Faixa etária de 40 a 75 anos: `idade_cliente >= 40 AND idade_cliente < 76`\n",
    "\n",
    "Estes filtros garantem que apenas dados relevantes para o objetivo do notebook (detecção de carcinoma em biópsias mamárias) sejam processados, otimizando o desempenho e a precisão da análise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d4ebfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtros para extração\n",
    "\n",
    "table_biopsia = \"refined.saude_preventiva.pardini_laudos_mama_biopsia\"\n",
    " \n",
    "where_clause = f\"\"\"\n",
    "    WHERE\n",
    "        _datestamp > (\n",
    "            SELECT MAX(biop._datestamp)\n",
    "            FROM {table_biopsia} biop\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    " \n",
    "filtro_extracao = \"\"\"\n",
    "    WHERE\n",
    "        linha_cuidado   = 'mama'\n",
    "        AND sigla_exame IN (\n",
    "            \"USPCOB\",\n",
    "            \"CBGEST\",\n",
    "            \"USPAFI\",\n",
    "            \"ESMATO\",\n",
    "            \"USMATO\",\n",
    "            \"MPMFECG\",\n",
    "            \"RMMATO\",\n",
    "            \"USPMAF\"\n",
    "        )\n",
    "        AND UPPER(sexo_cliente) = 'F'\n",
    "        AND (\n",
    "            idade_cliente >= 40 AND idade_cliente < 76\n",
    "        ) \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf537606",
   "metadata": {},
   "source": [
    "## Consulta SQL para Extração e Transformação de Laudos\n",
    "\n",
    "Esta célula contém a parte central do processamento de dados: uma consulta SQL complexa que extrai, filtra e transforma os dados de laudos, identificando a presença de carcinoma em laudos de biópsia mamária.\n",
    "\n",
    "### Objetivo da Célula\n",
    "\n",
    "Esta célula:\n",
    "1. Define uma consulta SQL para extrair laudos relevantes da tabela fonte\n",
    "2. Implementa expressões regulares para detecção de carcinoma\n",
    "3. Cria colunas derivadas como `RAW_CARCINOMA` e `HAS_CARCINOMA`\n",
    "4. Executa a consulta e armazena os resultados em um DataFrame\n",
    "5. Exibe o DataFrame resultante para análise\n",
    "\n",
    "### Estrutura da Consulta SQL\n",
    "\n",
    "A consulta utiliza uma Common Table Expression (CTE) chamada `base` para organizar a lógica:\n",
    "\n",
    "1. **Seleção de Colunas Originais**:\n",
    "   - Identificadores: `id_marca`, `id_unidade`, `id_cliente`, `id_ficha`, `ficha`, etc.\n",
    "   - Dados temporais: `dth_pedido`, `dth_resultado`, `_datestamp`\n",
    "   - Conteúdo do laudo: `laudo_tratado`\n",
    "   - Metadados: `linha_cuidado`, `sexo_cliente`\n",
    "\n",
    "2. **Cálculo de Idade**:\n",
    "   ```sql\n",
    "   TIMESTAMPDIFF(DAY, flr.dth_nascimento_cliente, CURDATE()) / 365.25\n",
    "   ```\n",
    "   Calcula a idade em anos com precisão, usando a diferença em dias entre a data de nascimento e a data atual, dividida por 365.25 (considerando anos bissextos).\n",
    "\n",
    "3. **Detecção de Carcinoma** usando expressões regulares:\n",
    "   ```sql\n",
    "   REGEXP_EXTRACT(\n",
    "       flr.laudo_tratado,\n",
    "       r'(?mi).*Topografia:.*MAMA.*(CARCINOMA).*|.*(CARCINOMA).*Topografia:.*MAMA.*',\n",
    "       1\n",
    "   ) AS RAW_CARCINOMA\n",
    "   ```\n",
    "   Esta expressão regular procura dois padrões:\n",
    "   - \"Topografia: MAMA\" seguido por \"CARCINOMA\"\n",
    "   - \"CARCINOMA\" seguido por \"Topografia: MAMA\"\n",
    "   \n",
    "   As flags `(?mi)` indicam modo multiline e case-insensitive.\n",
    "\n",
    "4. **Flag de Presença de Carcinoma**:\n",
    "   ```sql\n",
    "   CASE\n",
    "       WHEN REGEXP_EXTRACT(...) = ''\n",
    "       THEN FALSE\n",
    "       ELSE TRUE\n",
    "   END AS HAS_CARCINOMA\n",
    "   ```\n",
    "   Cria uma coluna booleana que indica se carcinoma foi detectado no laudo.\n",
    "\n",
    "### Aplicação dos Filtros\n",
    "\n",
    "A consulta incorpora o filtro definido anteriormente (`filtro_extracao`), garantindo que apenas laudos relevantes sejam selecionados.\n",
    "\n",
    "### Execução e Visualização\n",
    "\n",
    "As últimas duas linhas:\n",
    "1. Executam a consulta SQL usando o SparkSession e armazenam o resultado em `df_spk_biopsia`\n",
    "2. Exibem o DataFrame resultante usando `display(df_spk_biopsia)`\n",
    "\n",
    "### Impacto\n",
    "\n",
    "Esta célula é fundamental para o pipeline, pois:\n",
    "1. Implementa a lógica central de detecção de carcinoma\n",
    "2. Cria os indicadores que serão utilizados para análises posteriores\n",
    "3. Garante que apenas dados relevantes sejam processados, aplicando os filtros demográficos e de tipo de exame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e59ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliza REGEXP_EXTRACT para buscar, no texto do campo 'laudo_tratado', \n",
    "# a ocorrência da palavra \"CARCINOMA\" relacionada à \"MAMA\" (seja antes ou depois de \"Topografia\").\n",
    "# O padrão regex considera maiúsculas/minúsculas e múltiplas linhas.\n",
    "# Se encontrar, extrai \"CARCINOMA\" para a coluna RAW_CARCINOMA; caso contrário, retorna string vazia.\n",
    "# O CASE seguinte verifica se RAW_CARCINOMA está vazia:\n",
    "# - Se estiver vazia, HAS_CARCINOMA recebe FALSE (não há carcinoma relacionado à mama no laudo).\n",
    "# - Se não estiver vazia, HAS_CARCINOMA recebe TRUE (há carcinoma relacionado à mama no laudo).\n",
    "\n",
    "query = f\"\"\"\n",
    "WITH base AS (\n",
    "    SELECT\n",
    "        flr.id_marca,\n",
    "        flr.id_unidade,\n",
    "        flr.id_cliente, \n",
    "        flr.id_ficha,\n",
    "        flr.ficha,\n",
    "        flr.id_item, \n",
    "        flr.id_subitem, \n",
    "        flr.sigla_exame, \n",
    "        flr.dth_pedido,\n",
    "        flr.dth_resultado,\n",
    "        flr.sigla_exame,\n",
    "        flr.laudo_tratado,\n",
    "        flr.linha_cuidado,\n",
    "        flr.sexo_cliente,\n",
    "        flr.`_datestamp`,\n",
    "        (\n",
    "          TIMESTAMPDIFF(DAY, flr.dth_nascimento_cliente, CURDATE()) / 365.25\n",
    "        ) AS idade_cliente,\n",
    "        REGEXP_EXTRACT(\n",
    "            flr.laudo_tratado,\n",
    "            r'(?mi).*Topografia:.*MAMA.*(CARCINOMA).*|.*(CARCINOMA).*Topografia:.*MAMA.*',\n",
    "            1\n",
    "        ) AS RAW_CARCINOMA,\n",
    "        CASE\n",
    "            WHEN REGEXP_EXTRACT(\n",
    "                    flr.laudo_tratado,\n",
    "                    r'(?mi).*Topografia:.*MAMA.*(CARCINOMA).*|.*(CARCINOMA).*Topografia:.*MAMA.*',\n",
    "                    1\n",
    "                 ) = ''\n",
    "            THEN FALSE\n",
    "            ELSE TRUE\n",
    "        END AS HAS_CARCINOMA\n",
    "    FROM refined.saude_preventiva.pardini_laudos flr\n",
    "    \n",
    ")\n",
    "SELECT *\n",
    "FROM base\n",
    "{filtro_extracao}\n",
    "\"\"\"\n",
    "df_spk_biopsia = spark.sql(query)\n",
    "display(df_spk_biopsia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdc1ffc",
   "metadata": {},
   "source": [
    "## Contagem de Registros Extraídos\n",
    "\n",
    "Esta célula executa uma operação simples mas importante: conta o número total de registros no DataFrame `df_spk` resultante da consulta SQL anterior. \n",
    "\n",
    "A função `count()` é uma operação de ação em Spark que percorre todos os registros no DataFrame e retorna o número total de linhas. Esta contagem é útil para:\n",
    "\n",
    "1. **Validação da Extração**: Confirmar se a consulta SQL retornou algum resultado\n",
    "2. **Monitoramento**: Acompanhar o volume de dados sendo processado\n",
    "3. **Diagnóstico**: Identificar possíveis problemas na consulta ou nos filtros se o número for muito diferente do esperado\n",
    "\n",
    "Note que há um problema potencial nesta célula: ela referencia um DataFrame `df_spk` que não foi explicitamente definido nas células anteriores (onde o DataFrame foi nomeado como `df_spk_biopsia`). Isso pode indicar uma inconsistência na nomenclatura ou que esta célula assume que `df_spk` já existe de alguma execução anterior ou outra parte do notebook.\n",
    "\n",
    "Esta discrepância pode causar um erro do tipo `NameError: name 'df_spk' is not defined` se o DataFrame `df_spk` não existir no momento da execução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa74860",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spk.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdfc8c8",
   "metadata": {},
   "source": [
    "## Verificação de Siglas de Exame Disponíveis\n",
    "\n",
    "Esta célula realiza uma análise exploratória simples mas importante: verifica quais siglas de exame estão efetivamente presentes nos dados extraídos. O comentário inicial explica o objetivo: \"Não temos todas as siglas na base de dados\", indicando que esta célula foi criada para verificar quais das siglas definidas no filtro estão realmente disponíveis nos dados.\n",
    "\n",
    "### Objetivo da Célula\n",
    "\n",
    "Esta verificação é importante porque:\n",
    "1. Valida se os filtros estão funcionando corretamente\n",
    "2. Identifica potenciais problemas de cobertura de dados\n",
    "3. Fornece insights sobre a distribuição dos tipos de exame\n",
    "\n",
    "### Operações Realizadas\n",
    "\n",
    "A célula executa duas operações:\n",
    "\n",
    "1. **Seleção e Distinção**:\n",
    "   ```python\n",
    "   df_spk.select(\"sigla_exame\").distinct()\n",
    "   ```\n",
    "   Esta operação:\n",
    "   - Seleciona apenas a coluna \"sigla_exame\" do DataFrame\n",
    "   - Aplica a função `distinct()` para retornar apenas valores únicos (sem duplicatas)\n",
    "   - O resultado é um novo DataFrame contendo apenas os valores únicos de siglas\n",
    "\n",
    "2. **Visualização**:\n",
    "   ```python\n",
    "   display(df_spk.select(\"sigla_exame\").distinct())\n",
    "   ```\n",
    "   Esta linha exibe o DataFrame resultante em formato tabular para análise visual.\n",
    "\n",
    "### Considerações\n",
    "\n",
    "Assim como na célula anterior, há uma inconsistência na nomenclatura do DataFrame - esta célula referencia `df_spk` enquanto o DataFrame criado anteriormente foi nomeado `df_spk_biopsia`. Esta discrepância pode causar um erro se `df_spk` não existir no momento da execução.\n",
    "\n",
    "O resultado desta célula permite comparar quais das siglas definidas no filtro de extração (`\"USPCOB\"`, `\"CBGEST\"`, `\"USPAFI\"`, etc.) estão realmente presentes nos dados, identificando possíveis gaps na cobertura dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbd244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Não temos todas as siglas na base de dados\n",
    "df_spk.select(\"sigla_exame\").distinct()\n",
    "display(df_spk.select(\"sigla_exame\").distinct())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53db70a3",
   "metadata": {},
   "source": [
    "## Configuração do Feature Store (Comentado)\n",
    "\n",
    "Esta célula contém código comentado para a configuração e criação de uma tabela no Databricks Feature Store. Embora o código não esteja ativo (está comentado), ele fornece a estrutura para persistir os dados processados como uma feature table gerenciada.\n",
    "\n",
    "### Objetivo da Célula\n",
    "\n",
    "O código, quando descomentado, criaria uma tabela no Feature Store do Databricks para armazenar os resultados da extração de forma gerenciada, com benefícios como:\n",
    "- Rastreamento de linhagem de dados\n",
    "- Versionamento\n",
    "- Metadados e documentação\n",
    "- Integração com MLflow e outras ferramentas do ecossistema Databricks\n",
    "\n",
    "### Detalhes do Código Comentado\n",
    "\n",
    "```python\n",
    "# fs = FeatureStoreClient()\n",
    "# fs.create_table(\n",
    "#     name=\"refined.saude_preventiva.pardini_laudos_mama_biopsia\",\n",
    "#     primary_keys=[\"ficha\",'id_item','id_subitem'],\n",
    "#     schema=df_spk.schema,    \n",
    "#     description=\"Jornada de Mama - Features extraídas de laudos de biopsia Pardini. Siglas: USPCOB, CBGEST, USPAFI, ESMATO, USMATO, MPMFECG, RMMATO, USPMAF\")\n",
    "```\n",
    "\n",
    "Os parâmetros definidos incluem:\n",
    "\n",
    "1. **Nome da Tabela**: `refined.saude_preventiva.pardini_laudos_mama_biopsia`\n",
    "   - Define o local onde a tabela será criada no data lake\n",
    "\n",
    "2. **Chaves Primárias**: `[\"ficha\",'id_item','id_subitem']`\n",
    "   - Define a combinação de colunas que identificam unicamente cada registro\n",
    "   - Essencial para operações de merge/upsert posteriores\n",
    "\n",
    "3. **Schema**: `df_spk.schema`\n",
    "   - Utiliza o schema do DataFrame `df_spk` como modelo para a tabela\n",
    "   - Isto garante compatibilidade de tipos de dados\n",
    "\n",
    "4. **Descrição**: Documentação detalhada da tabela\n",
    "   - Inclui o propósito (Features extraídas de laudos de biopsia Pardini)\n",
    "   - Lista as siglas de exames relevantes\n",
    "\n",
    "### Considerações\n",
    "\n",
    "Como nas células anteriores, há uma inconsistência na nomenclatura do DataFrame - o código referencia `df_spk.schema` enquanto o DataFrame criado foi nomeado `df_spk_biopsia`.\n",
    "\n",
    "Este código está comentado possivelmente porque:\n",
    "- A tabela já foi criada em uma execução anterior\n",
    "- Esta é uma versão de desenvolvimento/teste que ainda não deve persistir dados\n",
    "- A estratégia de persistência foi alterada para o método Delta mostrado nas células posteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9798896b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FeatureStore\n",
    "# fs = FeatureStoreClient()\n",
    "# fs.create_table(\n",
    "#     name=\"refined.saude_preventiva.pardini_laudos_mama_biopsia\",\n",
    "#     primary_keys=[\"ficha\",'id_item','id_subitem'],\n",
    "#     schema=df_spk.schema,    \n",
    "#     description=\"Jornada de Mama - Features extraídas de laudos de biopsia Pardini. Siglas: USPCOB, CBGEST, USPAFI, ESMATO, USMATO, MPMFECG, RMMATO, USPMAF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d0c140",
   "metadata": {},
   "source": [
    "## Processamento Final e Persistência dos Dados\n",
    "\n",
    "Esta última célula é responsável pelo processamento final e salvamento dos dados extraídos na tabela Delta. É a célula mais complexa do notebook, implementando várias etapas críticas do pipeline:\n",
    "\n",
    "### Objetivo da Célula\n",
    "\n",
    "Esta célula:\n",
    "1. Importa bibliotecas adicionais necessárias para tratamento de erros e manipulação de tabelas Delta\n",
    "2. Define constantes para o sistema de monitoramento\n",
    "3. Seleciona colunas específicas do DataFrame original\n",
    "4. Implementa uma função para inserir/atualizar dados na tabela Delta\n",
    "5. Executa o salvamento com tratamento de erros robusto\n",
    "6. Envia alertas em caso de problemas\n",
    "\n",
    "### Importações e Configurações Iniciais\n",
    "\n",
    "```python\n",
    "import sys\n",
    "import traceback\n",
    "from octoops import Sentinel\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "WEBHOOK_DS_AI_BUSINESS_STG = 'stg'\n",
    "\n",
    "OUTPUT_DATA_PATH = 'refined.saude_preventiva.pardini_laudos_mama_biopsia'\n",
    "```\n",
    "\n",
    "Estas linhas:\n",
    "- Importam módulos adicionais para tratamento de erros (`traceback`) e manipulação de tabelas Delta\n",
    "- Definem o ambiente do webhook para o sistema de alerta (`stg` para staging/teste)\n",
    "- Definem o caminho da tabela de saída (onde os dados serão persistidos)\n",
    "\n",
    "### Seleção de Colunas\n",
    "\n",
    "```python\n",
    "df_spk = df_spk_biopsia.select(\"ficha\",\"sequencial\",\"sigla_exame\",\"id_cliente\",\"dth_pedido\",\"dth_resultado\", \"laudo_tratado\",\"linha_cuidado\",\"_datestamp\",\"RAW_CARCINOMA\",\"HAS_CARCINOMA\").display()\n",
    "```\n",
    "\n",
    "Esta linha:\n",
    "- Seleciona apenas as colunas relevantes do DataFrame `df_spk_biopsia`\n",
    "- Cria um novo DataFrame chamado `df_spk` com o subconjunto de colunas\n",
    "- A função `.display()` ao final mostrará os dados na interface do notebook\n",
    "\n",
    "### Função de Inserção de Dados\n",
    "\n",
    "```python\n",
    "def insert_data(df_spk, output_data_path):  \n",
    "    # Carrega a tabela Delta existente\n",
    "    delta_table = DeltaTable.forName(spark, output_data_path)\n",
    "\n",
    "    # Faz o merge (upsert)\n",
    "    (delta_table.alias(\"target\")\n",
    "     .merge(\n",
    "         df_spk.alias(\"source\"),\n",
    "         \"target.ficha = source.ficha AND target.sequencial = source.sequencial AND target.sigla_exame = source.sigla_exame\"\n",
    "     )\n",
    "     .whenMatchedUpdateAll()\n",
    "     .whenNotMatchedInsertAll()\n",
    "     .execute())\n",
    "```\n",
    "\n",
    "Esta função implementa um padrão de merge (upsert) para tabelas Delta:\n",
    "- Carrega a tabela Delta existente usando `DeltaTable.forName()`\n",
    "- Configura uma operação de merge usando:\n",
    "  - Aliases para as tabelas source e target\n",
    "  - Condição de junção baseada nas chaves primárias\n",
    "  - Regras para atualização (quando há correspondência) e inserção (quando não há)\n",
    "- Executa a operação com `.execute()`\n",
    "\n",
    "### Bloco Principal com Tratamento de Erros\n",
    "\n",
    "O bloco `try/except` implementa o processamento com tratamento de erros:\n",
    "\n",
    "1. **Verificação de Dados Disponíveis**:\n",
    "   ```python\n",
    "   if df_spk.count() > 0:\n",
    "   ```\n",
    "   Verifica se há registros para processar\n",
    "\n",
    "2. **Salvamento de Dados** (quando há registros):\n",
    "   ```python\n",
    "   insert_data(df_spk, OUTPUT_DATA_PATH)\n",
    "   print(f'Total de registros salvos na tabela: {df_spk.count()}')\n",
    "   ```\n",
    "   Chama a função de inserção e exibe uma mensagem de confirmação\n",
    "\n",
    "3. **Notificação** (quando não há registros):\n",
    "   ```python\n",
    "   else:\n",
    "       error_message = traceback.format_exc()\n",
    "       summary_message = f\"\"\"Não há laudos para extração.\\n{error_message}\"\"\"\n",
    "       sentinela_ds_ai_business = Sentinel(...)\n",
    "       sentinela_ds_ai_business.alerta_sentinela(...)\n",
    "   ```\n",
    "   Cria e envia um alerta usando o sistema Sentinel\n",
    "\n",
    "4. **Tratamento de Exceções**:\n",
    "   ```python\n",
    "   except Exception as e:\n",
    "       traceback.print_exc()        \n",
    "       raise e\n",
    "   ```\n",
    "   Imprime o traceback completo e propaga a exceção para tratamento em nível superior\n",
    "\n",
    "### Detalhes Adicionais\n",
    "\n",
    "O código também inclui algumas linhas comentadas que indicam abordagens alternativas:\n",
    "- Código comentado `#1/0` que parece ser um teste para forçar um erro\n",
    "- Código comentado para salvar usando o Feature Store em vez da abordagem Delta direta\n",
    "\n",
    "### Considerações\n",
    "\n",
    "Esta célula representa o final do pipeline de processamento, garantindo que os dados sejam salvos de forma consistente e que problemas sejam prontamente notificados. A implementação do padrão de merge garante idempotência - o notebook pode ser executado múltiplas vezes sem criar duplicatas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383b947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import traceback\n",
    "from octoops import Sentinel\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "WEBHOOK_DS_AI_BUSINESS_STG = 'stg'\n",
    "\n",
    "\n",
    "#OUTPUT_DATA_PATH = dbutils.widgets.get('OUTPUT_DATA_PATH')\n",
    "OUTPUT_DATA_PATH = 'refined.saude_preventiva.pardini_laudos_mama_biopsia'\n",
    "\n",
    "# Selecionar colunas de interesse\n",
    "df_spk = df_spk_biopsia.select(\"ficha\",\"sequencial\",\"sigla_exame\",\"id_cliente\",\"dth_pedido\",\"dth_resultado\", \"laudo_tratado\",\"linha_cuidado\",\"_datestamp\",\"RAW_CARCINOMA\",\"HAS_CARCINOMA\").display()\n",
    "\n",
    "# função para salvar dados na tabela\n",
    "def insert_data(df_spk, output_data_path):  \n",
    "\n",
    "    # Carrega a tabela Delta existente\n",
    "    delta_table = DeltaTable.forName(spark, output_data_path)\n",
    "\n",
    "    # Faz o merge (upsert)\n",
    "    (delta_table.alias(\"target\")\n",
    "     .merge(\n",
    "         df_spk.alias(\"source\"),\n",
    "         \"target.ficha = source.ficha AND target.sequencial = source.sequencial AND target.sigla_exame = source.sigla_exame\"\n",
    "     )\n",
    "     .whenMatchedUpdateAll()\n",
    "     .whenNotMatchedInsertAll()\n",
    "     .execute())\n",
    "            \n",
    "\n",
    "\n",
    "# salvar dados\n",
    "try:\n",
    "    if df_spk.count() > 0:\n",
    "        \n",
    "        #1/0 \n",
    "        insert_data(df_spk, OUTPUT_DATA_PATH)\n",
    "        # fs.write_table(\n",
    "    #     name=\"refined.saude_preventiva.fleury_laudos_mama_biopsia\",\n",
    "    #     df=df_spk,\n",
    "    #     mode=\"merge\",\n",
    "        # )\n",
    "        print(f'Total de registros salvos na tabela: {df_spk.count()}')\n",
    "            \n",
    "    else:\n",
    "        error_message = traceback.format_exc()\n",
    "        summary_message = f\"\"\"Não há laudos para extração.\\n{error_message}\"\"\"\n",
    "        sentinela_ds_ai_business = Sentinel(\n",
    "            project_name='Monitor_Linhas_Cuidado_Torax',\n",
    "            env_type=WEBHOOK_DS_AI_BUSINESS_STG,\n",
    "            task_title='Pardini Mama Biopsia'\n",
    "        )\n",
    "\n",
    "        sentinela_ds_ai_business.alerta_sentinela(\n",
    "            categoria='Alerta', \n",
    "            mensagem=summary_message,\n",
    "            job_id_descritivo='2_pardini_mama_biopsia'\n",
    "        )\n",
    "except Exception as e:\n",
    "    traceback.print_exc()        \n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
